From b2d9df7c788793a498f8aa0e0561bd1a6851da55 Mon Sep 17 00:00:00 2001
From: Drew DeVault <sir@cmpwn.com>
Date: Tue, 16 Nov 2021 08:23:10 +0100
Subject: [PATCH] all: remove support for nonfree Redis modules

---
 redis/client.py                          |    5 +-
 redis/commands/__init__.py               |    2 -
 redis/commands/json/__init__.py          |  120 --
 redis/commands/json/commands.py          |  232 ----
 redis/commands/json/decoders.py          |   59 -
 redis/commands/json/path.py              |   16 -
 redis/commands/redismodules.py           |   35 -
 redis/commands/search/__init__.py        |   96 --
 redis/commands/search/_util.py           |    7 -
 redis/commands/search/aggregation.py     |  406 -------
 redis/commands/search/commands.py        |  706 -----------
 redis/commands/search/document.py        |   13 -
 redis/commands/search/field.py           |   94 --
 redis/commands/search/indexDefinition.py |   80 --
 redis/commands/search/query.py           |  325 -----
 redis/commands/search/querystring.py     |  321 -----
 redis/commands/search/reducers.py        |  178 ---
 redis/commands/search/result.py          |   73 --
 redis/commands/search/suggestion.py      |   53 -
 redis/commands/timeseries/__init__.py    |   85 --
 redis/commands/timeseries/commands.py    |  775 ------------
 redis/commands/timeseries/info.py        |   82 --
 redis/commands/timeseries/utils.py       |   49 -
 tests/test_connection.py                 |   21 -
 tests/test_json.py                       | 1416 ----------------------
 tests/test_search.py                     | 1315 --------------------
 tests/test_timeseries.py                 |  588 ---------
 27 files changed, 2 insertions(+), 7150 deletions(-)
 delete mode 100644 redis/commands/json/__init__.py
 delete mode 100644 redis/commands/json/commands.py
 delete mode 100644 redis/commands/json/decoders.py
 delete mode 100644 redis/commands/json/path.py
 delete mode 100644 redis/commands/redismodules.py
 delete mode 100644 redis/commands/search/__init__.py
 delete mode 100644 redis/commands/search/_util.py
 delete mode 100644 redis/commands/search/aggregation.py
 delete mode 100644 redis/commands/search/commands.py
 delete mode 100644 redis/commands/search/document.py
 delete mode 100644 redis/commands/search/field.py
 delete mode 100644 redis/commands/search/indexDefinition.py
 delete mode 100644 redis/commands/search/query.py
 delete mode 100644 redis/commands/search/querystring.py
 delete mode 100644 redis/commands/search/reducers.py
 delete mode 100644 redis/commands/search/result.py
 delete mode 100644 redis/commands/search/suggestion.py
 delete mode 100644 redis/commands/timeseries/__init__.py
 delete mode 100644 redis/commands/timeseries/commands.py
 delete mode 100644 redis/commands/timeseries/info.py
 delete mode 100644 redis/commands/timeseries/utils.py
 delete mode 100644 tests/test_json.py
 delete mode 100644 tests/test_search.py
 delete mode 100644 tests/test_timeseries.py

diff --git a/redis/client.py b/redis/client.py
index dc6693d..93fc459 100755
--- a/redis/client.py
+++ b/redis/client.py
@@ -6,8 +6,7 @@ import re
 import threading
 import time
 import warnings
-from redis.commands import (CoreCommands, RedisModuleCommands,
-                            SentinelCommands, list_or_args)
+from redis.commands import CoreCommands, SentinelCommands, list_or_args
 from redis.connection import (ConnectionPool, UnixDomainSocketConnection,
                               SSLConnection)
 from redis.lock import Lock
@@ -607,7 +606,7 @@ def parse_set_result(response, **options):
     return response and str_if_bytes(response) == 'OK'
 
 
-class Redis(RedisModuleCommands, CoreCommands, SentinelCommands, object):
+class Redis(CoreCommands, SentinelCommands, object):
     """
     Implementation of the Redis protocol.
 
diff --git a/redis/commands/__init__.py b/redis/commands/__init__.py
index f1ddaaa..6e4aa0f 100644
--- a/redis/commands/__init__.py
+++ b/redis/commands/__init__.py
@@ -1,11 +1,9 @@
 from .core import CoreCommands
-from .redismodules import RedisModuleCommands
 from .helpers import list_or_args
 from .sentinel import SentinelCommands
 
 __all__ = [
     'CoreCommands',
-    'RedisModuleCommands',
     'SentinelCommands',
     'list_or_args'
 ]
diff --git a/redis/commands/json/__init__.py b/redis/commands/json/__init__.py
deleted file mode 100644
index d634dbd..0000000
--- a/redis/commands/json/__init__.py
+++ /dev/null
@@ -1,120 +0,0 @@
-from json import JSONDecoder, JSONEncoder, JSONDecodeError
-
-from .decoders import (
-    decode_list,
-    bulk_of_jsons,
-)
-from ..helpers import nativestr
-from .commands import JSONCommands
-import redis
-
-
-class JSON(JSONCommands):
-    """
-    Create a client for talking to json.
-
-    :param decoder:
-    :type json.JSONDecoder: An instance of json.JSONDecoder
-
-    :param encoder:
-    :type json.JSONEncoder: An instance of json.JSONEncoder
-    """
-
-    def __init__(
-        self,
-        client,
-        version=None,
-        decoder=JSONDecoder(),
-        encoder=JSONEncoder(),
-    ):
-        """
-        Create a client for talking to json.
-
-        :param decoder:
-        :type json.JSONDecoder: An instance of json.JSONDecoder
-
-        :param encoder:
-        :type json.JSONEncoder: An instance of json.JSONEncoder
-        """
-        # Set the module commands' callbacks
-        self.MODULE_CALLBACKS = {
-            "JSON.CLEAR": int,
-            "JSON.DEL": int,
-            "JSON.FORGET": int,
-            "JSON.GET": self._decode,
-            "JSON.MGET": bulk_of_jsons(self._decode),
-            "JSON.SET": lambda r: r and nativestr(r) == "OK",
-            "JSON.NUMINCRBY": self._decode,
-            "JSON.NUMMULTBY": self._decode,
-            "JSON.TOGGLE": self._decode,
-            "JSON.STRAPPEND": self._decode,
-            "JSON.STRLEN": self._decode,
-            "JSON.ARRAPPEND": self._decode,
-            "JSON.ARRINDEX": self._decode,
-            "JSON.ARRINSERT": self._decode,
-            "JSON.ARRLEN": self._decode,
-            "JSON.ARRPOP": self._decode,
-            "JSON.ARRTRIM": self._decode,
-            "JSON.OBJLEN": self._decode,
-            "JSON.OBJKEYS": self._decode,
-            "JSON.RESP": self._decode,
-            "JSON.DEBUG": self._decode,
-        }
-
-        self.client = client
-        self.execute_command = client.execute_command
-        self.MODULE_VERSION = version
-
-        for key, value in self.MODULE_CALLBACKS.items():
-            self.client.set_response_callback(key, value)
-
-        self.__encoder__ = encoder
-        self.__decoder__ = decoder
-
-    def _decode(self, obj):
-        """Get the decoder."""
-        if obj is None:
-            return obj
-
-        try:
-            x = self.__decoder__.decode(obj)
-            if x is None:
-                raise TypeError
-            return x
-        except TypeError:
-            try:
-                return self.__decoder__.decode(obj.decode())
-            except AttributeError:
-                return decode_list(obj)
-        except (AttributeError, JSONDecodeError):
-            return decode_list(obj)
-
-    def _encode(self, obj):
-        """Get the encoder."""
-        return self.__encoder__.encode(obj)
-
-    def pipeline(self, transaction=True, shard_hint=None):
-        """Creates a pipeline for the JSON module, that can be used for executing
-        JSON commands, as well as classic core commands.
-
-        Usage example:
-
-        r = redis.Redis()
-        pipe = r.json().pipeline()
-        pipe.jsonset('foo', '.', {'hello!': 'world'})
-        pipe.jsonget('foo')
-        pipe.jsonget('notakey')
-        """
-        p = Pipeline(
-            connection_pool=self.client.connection_pool,
-            response_callbacks=self.MODULE_CALLBACKS,
-            transaction=transaction,
-            shard_hint=shard_hint,
-        )
-        p._encode = self._encode
-        p._decode = self._decode
-        return p
-
-
-class Pipeline(JSONCommands, redis.client.Pipeline):
-    """Pipeline for the module."""
diff --git a/redis/commands/json/commands.py b/redis/commands/json/commands.py
deleted file mode 100644
index 4436f6a..0000000
--- a/redis/commands/json/commands.py
+++ /dev/null
@@ -1,232 +0,0 @@
-from .path import Path
-from .decoders import decode_dict_keys
-from deprecated import deprecated
-from redis.exceptions import DataError
-
-
-class JSONCommands:
-    """json commands."""
-
-    def arrappend(self, name, path=Path.rootPath(), *args):
-        """Append the objects ``args`` to the array under the
-        ``path` in key ``name``.
-        """
-        pieces = [name, str(path)]
-        for o in args:
-            pieces.append(self._encode(o))
-        return self.execute_command("JSON.ARRAPPEND", *pieces)
-
-    def arrindex(self, name, path, scalar, start=0, stop=-1):
-        """
-        Return the index of ``scalar`` in the JSON array under ``path`` at key
-        ``name``.
-
-        The search can be limited using the optional inclusive ``start``
-        and exclusive ``stop`` indices.
-        """
-        return self.execute_command(
-            "JSON.ARRINDEX", name, str(path), self._encode(scalar),
-            start, stop
-        )
-
-    def arrinsert(self, name, path, index, *args):
-        """Insert the objects ``args`` to the array at index ``index``
-        under the ``path` in key ``name``.
-        """
-        pieces = [name, str(path), index]
-        for o in args:
-            pieces.append(self._encode(o))
-        return self.execute_command("JSON.ARRINSERT", *pieces)
-
-    def arrlen(self, name, path=Path.rootPath()):
-        """Return the length of the array JSON value under ``path``
-        at key``name``.
-        """
-        return self.execute_command("JSON.ARRLEN", name, str(path))
-
-    def arrpop(self, name, path=Path.rootPath(), index=-1):
-        """Pop the element at ``index`` in the array JSON value under
-        ``path`` at key ``name``.
-        """
-        return self.execute_command("JSON.ARRPOP", name, str(path), index)
-
-    def arrtrim(self, name, path, start, stop):
-        """Trim the array JSON value under ``path`` at key ``name`` to the
-        inclusive range given by ``start`` and ``stop``.
-        """
-        return self.execute_command("JSON.ARRTRIM", name, str(path),
-                                    start, stop)
-
-    def type(self, name, path=Path.rootPath()):
-        """Get the type of the JSON value under ``path`` from key ``name``."""
-        return self.execute_command("JSON.TYPE", name, str(path))
-
-    def resp(self, name, path=Path.rootPath()):
-        """Return the JSON value under ``path`` at key ``name``."""
-        return self.execute_command("JSON.RESP", name, str(path))
-
-    def objkeys(self, name, path=Path.rootPath()):
-        """Return the key names in the dictionary JSON value under ``path`` at
-        key ``name``."""
-        return self.execute_command("JSON.OBJKEYS", name, str(path))
-
-    def objlen(self, name, path=Path.rootPath()):
-        """Return the length of the dictionary JSON value under ``path`` at key
-        ``name``.
-        """
-        return self.execute_command("JSON.OBJLEN", name, str(path))
-
-    def numincrby(self, name, path, number):
-        """Increment the numeric (integer or floating point) JSON value under
-        ``path`` at key ``name`` by the provided ``number``.
-        """
-        return self.execute_command(
-            "JSON.NUMINCRBY", name, str(path), self._encode(number)
-        )
-
-    @deprecated(version='4.0.0', reason='deprecated since redisjson 1.0.0')
-    def nummultby(self, name, path, number):
-        """Multiply the numeric (integer or floating point) JSON value under
-        ``path`` at key ``name`` with the provided ``number``.
-        """
-        return self.execute_command(
-            "JSON.NUMMULTBY", name, str(path), self._encode(number)
-        )
-
-    def clear(self, name, path=Path.rootPath()):
-        """
-        Empty arrays and objects (to have zero slots/keys without deleting the
-        array/object).
-
-        Return the count of cleared paths (ignoring non-array and non-objects
-        paths).
-        """
-        return self.execute_command("JSON.CLEAR", name, str(path))
-
-    def delete(self, key, path=Path.rootPath()):
-        """Delete the JSON value stored at key ``key`` under ``path``."""
-        return self.execute_command("JSON.DEL", key, str(path))
-
-    # forget is an alias for delete
-    forget = delete
-
-    def get(self, name, *args, no_escape=False):
-        """
-        Get the object stored as a JSON value at key ``name``.
-
-        ``args`` is zero or more paths, and defaults to root path
-        ```no_escape`` is a boolean flag to add no_escape option to get
-        non-ascii characters
-        """
-        pieces = [name]
-        if no_escape:
-            pieces.append("noescape")
-
-        if len(args) == 0:
-            pieces.append(Path.rootPath())
-
-        else:
-            for p in args:
-                pieces.append(str(p))
-
-        # Handle case where key doesn't exist. The JSONDecoder would raise a
-        # TypeError exception since it can't decode None
-        try:
-            return self.execute_command("JSON.GET", *pieces)
-        except TypeError:
-            return None
-
-    def mget(self, keys, path):
-        """
-        Get the objects stored as a JSON values under ``path``. ``keys``
-        is a list of one or more keys.
-        """
-        pieces = []
-        pieces += keys
-        pieces.append(str(path))
-        return self.execute_command("JSON.MGET", *pieces)
-
-    def set(self, name, path, obj, nx=False, xx=False, decode_keys=False):
-        """
-        Set the JSON value at key ``name`` under the ``path`` to ``obj``.
-
-        ``nx`` if set to True, set ``value`` only if it does not exist.
-        ``xx`` if set to True, set ``value`` only if it exists.
-        ``decode_keys`` If set to True, the keys of ``obj`` will be decoded
-        with utf-8.
-
-        For the purpose of using this within a pipeline, this command is also
-        aliased to jsonset.
-        """
-        if decode_keys:
-            obj = decode_dict_keys(obj)
-
-        pieces = [name, str(path), self._encode(obj)]
-
-        # Handle existential modifiers
-        if nx and xx:
-            raise Exception(
-                "nx and xx are mutually exclusive: use one, the "
-                "other or neither - but not both"
-            )
-        elif nx:
-            pieces.append("NX")
-        elif xx:
-            pieces.append("XX")
-        return self.execute_command("JSON.SET", *pieces)
-
-    def strlen(self, name, path=None):
-        """Return the length of the string JSON value under ``path`` at key
-        ``name``.
-        """
-        pieces = [name]
-        if path is not None:
-            pieces.append(str(path))
-        return self.execute_command("JSON.STRLEN", *pieces)
-
-    def toggle(self, name, path=Path.rootPath()):
-        """Toggle boolean value under ``path`` at key ``name``.
-        returning the new value.
-        """
-        return self.execute_command("JSON.TOGGLE", name, str(path))
-
-    def strappend(self, name, value, path=Path.rootPath()):
-        """Append to the string JSON value. If two options are specified after
-        the key name, the path is determined to be the first. If a single
-        option is passed, then the rootpath (i.e Path.rootPath()) is used.
-        """
-        pieces = [name, str(path), self._encode(value)]
-        return self.execute_command(
-            "JSON.STRAPPEND", *pieces
-        )
-
-    def debug(self, subcommand, key=None, path=Path.rootPath()):
-        """Return the memory usage in bytes of a value under ``path`` from
-        key ``name``.
-        """
-        valid_subcommands = ["MEMORY", "HELP"]
-        if subcommand not in valid_subcommands:
-            raise DataError("The only valid subcommands are ",
-                            str(valid_subcommands))
-        pieces = [subcommand]
-        if subcommand == "MEMORY":
-            if key is None:
-                raise DataError("No key specified")
-            pieces.append(key)
-            pieces.append(str(path))
-        return self.execute_command("JSON.DEBUG", *pieces)
-
-    @deprecated(version='4.0.0',
-                reason='redisjson-py supported this, call get directly.')
-    def jsonget(self, *args, **kwargs):
-        return self.get(*args, **kwargs)
-
-    @deprecated(version='4.0.0',
-                reason='redisjson-py supported this, call get directly.')
-    def jsonmget(self, *args, **kwargs):
-        return self.mget(*args, **kwargs)
-
-    @deprecated(version='4.0.0',
-                reason='redisjson-py supported this, call get directly.')
-    def jsonset(self, *args, **kwargs):
-        return self.set(*args, **kwargs)
diff --git a/redis/commands/json/decoders.py b/redis/commands/json/decoders.py
deleted file mode 100644
index b19395c..0000000
--- a/redis/commands/json/decoders.py
+++ /dev/null
@@ -1,59 +0,0 @@
-from ..helpers import nativestr
-import re
-import copy
-
-
-def bulk_of_jsons(d):
-    """Replace serialized JSON values with objects in a
-    bulk array response (list).
-    """
-
-    def _f(b):
-        for index, item in enumerate(b):
-            if item is not None:
-                b[index] = d(item)
-        return b
-
-    return _f
-
-
-def decode_dict_keys(obj):
-    """Decode the keys of the given dictionary with utf-8."""
-    newobj = copy.copy(obj)
-    for k in obj.keys():
-        if isinstance(k, bytes):
-            newobj[k.decode("utf-8")] = newobj[k]
-            newobj.pop(k)
-    return newobj
-
-
-def unstring(obj):
-    """
-    Attempt to parse string to native integer formats.
-    One can't simply call int/float in a try/catch because there is a
-    semantic difference between (for example) 15.0 and 15.
-    """
-    floatreg = '^\\d+.\\d+$'
-    match = re.findall(floatreg, obj)
-    if match != []:
-        return float(match[0])
-
-    intreg = "^\\d+$"
-    match = re.findall(intreg, obj)
-    if match != []:
-        return int(match[0])
-    return obj
-
-
-def decode_list(b):
-    """
-    Given a non-deserializable object, make a best effort to
-    return a useful set of results.
-    """
-    if isinstance(b, list):
-        return [nativestr(obj) for obj in b]
-    elif isinstance(b, bytes):
-        return unstring(nativestr(b))
-    elif isinstance(b, str):
-        return unstring(b)
-    return b
diff --git a/redis/commands/json/path.py b/redis/commands/json/path.py
deleted file mode 100644
index 6d87045..0000000
--- a/redis/commands/json/path.py
+++ /dev/null
@@ -1,16 +0,0 @@
-class Path(object):
-    """This class represents a path in a JSON value."""
-
-    strPath = ""
-
-    @staticmethod
-    def rootPath():
-        """Return the root path's string representation."""
-        return "."
-
-    def __init__(self, path):
-        """Make a new path based on the string representation in `path`."""
-        self.strPath = path
-
-    def __repr__(self):
-        return self.strPath
diff --git a/redis/commands/redismodules.py b/redis/commands/redismodules.py
deleted file mode 100644
index 5f629fb..0000000
--- a/redis/commands/redismodules.py
+++ /dev/null
@@ -1,35 +0,0 @@
-from json import JSONEncoder, JSONDecoder
-
-
-class RedisModuleCommands:
-    """This class contains the wrapper functions to bring supported redis
-    modules into the command namepsace.
-    """
-
-    def json(self, encoder=JSONEncoder(), decoder=JSONDecoder()):
-        """Access the json namespace, providing support for redis json.
-        """
-
-        from .json import JSON
-        jj = JSON(
-                client=self,
-                encoder=encoder,
-                decoder=decoder)
-        return jj
-
-    def ft(self, index_name="idx"):
-        """Access the search namespace, providing support for redis search.
-        """
-
-        from .search import Search
-        s = Search(client=self, index_name=index_name)
-        return s
-
-    def ts(self):
-        """Access the timeseries namespace, providing support for
-        redis timeseries data.
-        """
-
-        from .timeseries import TimeSeries
-        s = TimeSeries(client=self)
-        return s
diff --git a/redis/commands/search/__init__.py b/redis/commands/search/__init__.py
deleted file mode 100644
index 8320ad4..0000000
--- a/redis/commands/search/__init__.py
+++ /dev/null
@@ -1,96 +0,0 @@
-from .commands import SearchCommands
-
-
-class Search(SearchCommands):
-    """
-    Create a client for talking to search.
-    It abstracts the API of the module and lets you just use the engine.
-    """
-
-    class BatchIndexer(object):
-        """
-        A batch indexer allows you to automatically batch
-        document indexing in pipelines, flushing it every N documents.
-        """
-
-        def __init__(self, client, chunk_size=1000):
-
-            self.client = client
-            self.execute_command = client.execute_command
-            self.pipeline = client.pipeline(transaction=False, shard_hint=None)
-            self.total = 0
-            self.chunk_size = chunk_size
-            self.current_chunk = 0
-
-        def __del__(self):
-            if self.current_chunk:
-                self.commit()
-
-        def add_document(
-            self,
-            doc_id,
-            nosave=False,
-            score=1.0,
-            payload=None,
-            replace=False,
-            partial=False,
-            no_create=False,
-            **fields
-        ):
-            """
-            Add a document to the batch query
-            """
-            self.client._add_document(
-                doc_id,
-                conn=self.pipeline,
-                nosave=nosave,
-                score=score,
-                payload=payload,
-                replace=replace,
-                partial=partial,
-                no_create=no_create,
-                **fields
-            )
-            self.current_chunk += 1
-            self.total += 1
-            if self.current_chunk >= self.chunk_size:
-                self.commit()
-
-        def add_document_hash(
-            self,
-            doc_id,
-            score=1.0,
-            replace=False,
-        ):
-            """
-            Add a hash to the batch query
-            """
-            self.client._add_document_hash(
-                doc_id,
-                conn=self.pipeline,
-                score=score,
-                replace=replace,
-            )
-            self.current_chunk += 1
-            self.total += 1
-            if self.current_chunk >= self.chunk_size:
-                self.commit()
-
-        def commit(self):
-            """
-            Manually commit and flush the batch indexing query
-            """
-            self.pipeline.execute()
-            self.current_chunk = 0
-
-    def __init__(self, client, index_name="idx"):
-        """
-        Create a new Client for the given index_name.
-        The default name is `idx`
-
-        If conn is not None, we employ an already existing redis connection
-        """
-        self.client = client
-        self.index_name = index_name
-        self.execute_command = client.execute_command
-        self.pipeline = client.pipeline
diff --git a/redis/commands/search/_util.py b/redis/commands/search/_util.py
deleted file mode 100644
index dd1dff3..0000000
--- a/redis/commands/search/_util.py
+++ /dev/null
@@ -1,7 +0,0 @@
-def to_string(s):
-    if isinstance(s, str):
-        return s
-    elif isinstance(s, bytes):
-        return s.decode("utf-8", "ignore")
-    else:
-        return s  # Not a string we care about
diff --git a/redis/commands/search/aggregation.py b/redis/commands/search/aggregation.py
deleted file mode 100644
index b391d1f..0000000
--- a/redis/commands/search/aggregation.py
+++ /dev/null
@@ -1,406 +0,0 @@
-FIELDNAME = object()
-
-
-class Limit(object):
-    def __init__(self, offset=0, count=0):
-        self.offset = offset
-        self.count = count
-
-    def build_args(self):
-        if self.count:
-            return ["LIMIT", str(self.offset), str(self.count)]
-        else:
-            return []
-
-
-class Reducer(object):
-    """
-    Base reducer object for all reducers.
-
-    See the `redisearch.reducers` module for the actual reducers.
-    """
-
-    NAME = None
-
-    def __init__(self, *args):
-        self._args = args
-        self._field = None
-        self._alias = None
-
-    def alias(self, alias):
-        """
-        Set the alias for this reducer.
-
-        ### Parameters
-
-        - **alias**: The value of the alias for this reducer. If this is the
-            special value `aggregation.FIELDNAME` then this reducer will be
-            aliased using the same name as the field upon which it operates.
-            Note that using `FIELDNAME` is only possible on reducers which
-            operate on a single field value.
-
-        This method returns the `Reducer` object making it suitable for
-        chaining.
-        """
-        if alias is FIELDNAME:
-            if not self._field:
-                raise ValueError("Cannot use FIELDNAME alias with no field")
-            # Chop off initial '@'
-            alias = self._field[1:]
-        self._alias = alias
-        return self
-
-    @property
-    def args(self):
-        return self._args
-
-
-class SortDirection(object):
-    """
-    This special class is used to indicate sort direction.
-    """
-
-    DIRSTRING = None
-
-    def __init__(self, field):
-        self.field = field
-
-
-class Asc(SortDirection):
-    """
-    Indicate that the given field should be sorted in ascending order
-    """
-
-    DIRSTRING = "ASC"
-
-
-class Desc(SortDirection):
-    """
-    Indicate that the given field should be sorted in descending order
-    """
-
-    DIRSTRING = "DESC"
-
-
-class Group(object):
-    """
-    This object automatically created in the `AggregateRequest.group_by()`
-    """
-
-    def __init__(self, fields, reducers):
-        if not reducers:
-            raise ValueError("Need at least one reducer")
-
-        fields = [fields] if isinstance(fields, str) else fields
-        reducers = [reducers] if isinstance(reducers, Reducer) else reducers
-
-        self.fields = fields
-        self.reducers = reducers
-        self.limit = Limit()
-
-    def build_args(self):
-        ret = ["GROUPBY", str(len(self.fields))]
-        ret.extend(self.fields)
-        for reducer in self.reducers:
-            ret += ["REDUCE", reducer.NAME, str(len(reducer.args))]
-            ret.extend(reducer.args)
-            if reducer._alias is not None:
-                ret += ["AS", reducer._alias]
-        return ret
-
-
-class Projection(object):
-    """
-    This object automatically created in the `AggregateRequest.apply()`
-    """
-
-    def __init__(self, projector, alias=None):
-        self.alias = alias
-        self.projector = projector
-
-    def build_args(self):
-        ret = ["APPLY", self.projector]
-        if self.alias is not None:
-            ret += ["AS", self.alias]
-
-        return ret
-
-
-class SortBy(object):
-    """
-    This object automatically created in the `AggregateRequest.sort_by()`
-    """
-
-    def __init__(self, fields, max=0):
-        self.fields = fields
-        self.max = max
-
-    def build_args(self):
-        fields_args = []
-        for f in self.fields:
-            if isinstance(f, SortDirection):
-                fields_args += [f.field, f.DIRSTRING]
-            else:
-                fields_args += [f]
-
-        ret = ["SORTBY", str(len(fields_args))]
-        ret.extend(fields_args)
-        if self.max > 0:
-            ret += ["MAX", str(self.max)]
-
-        return ret
-
-
-class AggregateRequest(object):
-    """
-    Aggregation request which can be passed to `Client.aggregate`.
-    """
-
-    def __init__(self, query="*"):
-        """
-        Create an aggregation request. This request may then be passed to
-        `client.aggregate()`.
-
-        In order for the request to be usable, it must contain at least one
-        group.
-
-        - **query** Query string for filtering records.
-
-        All member methods (except `build_args()`)
-        return the object itself, making them useful for chaining.
-        """
-        self._query = query
-        self._aggregateplan = []
-        self._loadfields = []
-        self._limit = Limit()
-        self._max = 0
-        self._with_schema = False
-        self._verbatim = False
-        self._cursor = []
-
-    def load(self, *fields):
-        """
-        Indicate the fields to be returned in the response. These fields are
-        returned in addition to any others implicitly specified.
-
-        ### Parameters
-
-        - **fields**: One or more fields in the format of `@field`
-        """
-        self._loadfields.extend(fields)
-        return self
-
-    def group_by(self, fields, *reducers):
-        """
-        Specify by which fields to group the aggregation.
-
-        ### Parameters
-
-        - **fields**: Fields to group by. This can either be a single string,
-            or a list of strings. both cases, the field should be specified as
-            `@field`.
-        - **reducers**: One or more reducers. Reducers may be found in the
-            `aggregation` module.
-        """
-        group = Group(fields, reducers)
-        self._aggregateplan.extend(group.build_args())
-
-        return self
-
-    def apply(self, **kwexpr):
-        """
-        Specify one or more projection expressions to add to each result
-
-        ### Parameters
-
-        - **kwexpr**: One or more key-value pairs for a projection. The key is
-            the alias for the projection, and the value is the projection
-            expression itself, for example `apply(square_root="sqrt(@foo)")`
-        """
-        for alias, expr in kwexpr.items():
-            projection = Projection(expr, alias)
-            self._aggregateplan.extend(projection.build_args())
-
-        return self
-
-    def limit(self, offset, num):
-        """
-        Sets the limit for the most recent group or query.
-
-        If no group has been defined yet (via `group_by()`) then this sets
-        the limit for the initial pool of results from the query. Otherwise,
-        this limits the number of items operated on from the previous group.
-
-        Setting a limit on the initial search results may be useful when
-        attempting to execute an aggregation on a sample of a large data set.
-
-        ### Parameters
-
-        - **offset**: Result offset from which to begin paging
-        - **num**: Number of results to return
-
-
-        Example of sorting the initial results:
-
-        ```
-        AggregateRequest("@sale_amount:[10000, inf]")\
-            .limit(0, 10)\
-            .group_by("@state", r.count())
-        ```
-
-        Will only group by the states found in the first 10 results of the
-        query `@sale_amount:[10000, inf]`. On the other hand,
-
-        ```
-        AggregateRequest("@sale_amount:[10000, inf]")\
-            .limit(0, 1000)\
-            .group_by("@state", r.count()\
-            .limit(0, 10)
-        ```
-
-        Will group all the results matching the query, but only return the
-        first 10 groups.
-
-        If you only wish to return a *top-N* style query, consider using
-        `sort_by()` instead.
-
-        """
-        limit = Limit(offset, num)
-        self._limit = limit
-        return self
-
-    def sort_by(self, *fields, **kwargs):
-        """
-        Indicate how the results should be sorted. This can also be used for
-        *top-N* style queries
-
-        ### Parameters
-
-        - **fields**: The fields by which to sort. This can be either a single
-            field or a list of fields. If you wish to specify order, you can
-            use the `Asc` or `Desc` wrapper classes.
-        - **max**: Maximum number of results to return. This can be
-            used instead of `LIMIT` and is also faster.
-
-
-        Example of sorting by `foo` ascending and `bar` descending:
-
-        ```
-        sort_by(Asc("@foo"), Desc("@bar"))
-        ```
-
-        Return the top 10 customers:
-
-        ```
-        AggregateRequest()\
-            .group_by("@customer", r.sum("@paid").alias(FIELDNAME))\
-            .sort_by(Desc("@paid"), max=10)
-        ```
-        """
-        if isinstance(fields, (str, SortDirection)):
-            fields = [fields]
-
-        max = kwargs.get("max", 0)
-        sortby = SortBy(fields, max)
-
-        self._aggregateplan.extend(sortby.build_args())
-        return self
-
-    def filter(self, expressions):
-        """
-        Specify filter for post-query results using predicates relating to
-        values in the result set.
-
-        ### Parameters
-
-        - **fields**: Fields to group by. This can either be a single string,
-            or a list of strings.
-        """
-        if isinstance(expressions, str):
-            expressions = [expressions]
-
-        for expression in expressions:
-            self._aggregateplan.extend(["FILTER", expression])
-
-        return self
-
-    def with_schema(self):
-        """
-        If set, the `schema` property will contain a list of `[field, type]`
-        entries in the result object.
-        """
-        self._with_schema = True
-        return self
-
-    def verbatim(self):
-        self._verbatim = True
-        return self
-
-    def cursor(self, count=0, max_idle=0.0):
-        args = ["WITHCURSOR"]
-        if count:
-            args += ["COUNT", str(count)]
-        if max_idle:
-            args += ["MAXIDLE", str(max_idle * 1000)]
-        self._cursor = args
-        return self
-
-    def _limit_2_args(self, limit):
-        if limit[1]:
-            return ["LIMIT"] + [str(x) for x in limit]
-        else:
-            return []
-
-    def build_args(self):
-        # @foo:bar ...
-        ret = [self._query]
-
-        if self._with_schema:
-            ret.append("WITHSCHEMA")
-
-        if self._verbatim:
-            ret.append("VERBATIM")
-
-        if self._cursor:
-            ret += self._cursor
-
-        if self._loadfields:
-            ret.append("LOAD")
-            ret.append(str(len(self._loadfields)))
-            ret.extend(self._loadfields)
-
-        ret.extend(self._aggregateplan)
-
-        ret += self._limit.build_args()
-
-        return ret
-
-
-class Cursor(object):
-    def __init__(self, cid):
-        self.cid = cid
-        self.max_idle = 0
-        self.count = 0
-
-    def build_args(self):
-        args = [str(self.cid)]
-        if self.max_idle:
-            args += ["MAXIDLE", str(self.max_idle)]
-        if self.count:
-            args += ["COUNT", str(self.count)]
-        return args
-
-
-class AggregateResult(object):
-    def __init__(self, rows, cursor, schema):
-        self.rows = rows
-        self.cursor = cursor
-        self.schema = schema
-
-    def __repr__(self):
-        return "<{} at 0x{:x} Rows={}, Cursor={}>".format(
-            self.__class__.__name__,
-            id(self),
-            len(self.rows),
-            self.cursor.cid if self.cursor else -1,
-        )
diff --git a/redis/commands/search/commands.py b/redis/commands/search/commands.py
deleted file mode 100644
index 0cee2ad..0000000
--- a/redis/commands/search/commands.py
+++ /dev/null
@@ -1,706 +0,0 @@
-import itertools
-import time
-
-from .document import Document
-from .result import Result
-from .query import Query
-from ._util import to_string
-from .aggregation import AggregateRequest, AggregateResult, Cursor
-from .suggestion import SuggestionParser
-
-NUMERIC = "NUMERIC"
-
-CREATE_CMD = "FT.CREATE"
-ALTER_CMD = "FT.ALTER"
-SEARCH_CMD = "FT.SEARCH"
-ADD_CMD = "FT.ADD"
-ADDHASH_CMD = "FT.ADDHASH"
-DROP_CMD = "FT.DROP"
-EXPLAIN_CMD = "FT.EXPLAIN"
-EXPLAINCLI_CMD = "FT.EXPLAINCLI"
-DEL_CMD = "FT.DEL"
-AGGREGATE_CMD = "FT.AGGREGATE"
-CURSOR_CMD = "FT.CURSOR"
-SPELLCHECK_CMD = "FT.SPELLCHECK"
-DICT_ADD_CMD = "FT.DICTADD"
-DICT_DEL_CMD = "FT.DICTDEL"
-DICT_DUMP_CMD = "FT.DICTDUMP"
-GET_CMD = "FT.GET"
-MGET_CMD = "FT.MGET"
-CONFIG_CMD = "FT.CONFIG"
-TAGVALS_CMD = "FT.TAGVALS"
-ALIAS_ADD_CMD = "FT.ALIASADD"
-ALIAS_UPDATE_CMD = "FT.ALIASUPDATE"
-ALIAS_DEL_CMD = "FT.ALIASDEL"
-INFO_CMD = "FT.INFO"
-SUGADD_COMMAND = "FT.SUGADD"
-SUGDEL_COMMAND = "FT.SUGDEL"
-SUGLEN_COMMAND = "FT.SUGLEN"
-SUGGET_COMMAND = "FT.SUGGET"
-SYNUPDATE_CMD = "FT.SYNUPDATE"
-SYNDUMP_CMD = "FT.SYNDUMP"
-
-NOOFFSETS = "NOOFFSETS"
-NOFIELDS = "NOFIELDS"
-STOPWORDS = "STOPWORDS"
-WITHSCORES = "WITHSCORES"
-FUZZY = "FUZZY"
-WITHPAYLOADS = "WITHPAYLOADS"
-
-
-class SearchCommands:
-    """Search commands."""
-
-    def batch_indexer(self, chunk_size=100):
-        """
-        Create a new batch indexer from the client with a given chunk size
-        """
-        return self.BatchIndexer(self, chunk_size=chunk_size)
-
-    def create_index(
-        self,
-        fields,
-        no_term_offsets=False,
-        no_field_flags=False,
-        stopwords=None,
-        definition=None,
-    ):
-        """
-        Create the search index. The index must not already exist.
-
-        ### Parameters:
-
-        - **fields**: a list of TextField or NumericField objects
-        - **no_term_offsets**: If true, we will not save term offsets in
-        the index
-        - **no_field_flags**: If true, we will not save field flags that
-        allow searching in specific fields
-        - **stopwords**: If not None, we create the index with this custom
-        stopword list. The list can be empty
-        """
-
-        args = [CREATE_CMD, self.index_name]
-        if definition is not None:
-            args += definition.args
-        if no_term_offsets:
-            args.append(NOOFFSETS)
-        if no_field_flags:
-            args.append(NOFIELDS)
-        if stopwords is not None and isinstance(stopwords, (list, tuple, set)):
-            args += [STOPWORDS, len(stopwords)]
-            if len(stopwords) > 0:
-                args += list(stopwords)
-
-        args.append("SCHEMA")
-        try:
-            args += list(itertools.chain(*(f.redis_args() for f in fields)))
-        except TypeError:
-            args += fields.redis_args()
-
-        return self.execute_command(*args)
-
-    def alter_schema_add(self, fields):
-        """
-        Alter the existing search index by adding new fields. The index
-        must already exist.
-
-        ### Parameters:
-
-        - **fields**: a list of Field objects to add for the index
-        """
-
-        args = [ALTER_CMD, self.index_name, "SCHEMA", "ADD"]
-        try:
-            args += list(itertools.chain(*(f.redis_args() for f in fields)))
-        except TypeError:
-            args += fields.redis_args()
-
-        return self.execute_command(*args)
-
-    def drop_index(self, delete_documents=True):
-        """
-        Drop the index if it exists. Deprecated from RediSearch 2.0.
-
-        ### Parameters:
-
-        - **delete_documents**: If `True`, all documents will be deleted.
-        """
-        keep_str = "" if delete_documents else "KEEPDOCS"
-        return self.execute_command(DROP_CMD, self.index_name, keep_str)
-
-    def dropindex(self, delete_documents=False):
-        """
-        Drop the index if it exists.
-        Replaced `drop_index` in RediSearch 2.0.
-        Default behavior was changed to not delete the indexed documents.
-
-        ### Parameters:
-
-        - **delete_documents**: If `True`, all documents will be deleted.
-        """
-        keep_str = "" if delete_documents else "KEEPDOCS"
-        return self.execute_command(DROP_CMD, self.index_name, keep_str)
-
-    def _add_document(
-        self,
-        doc_id,
-        conn=None,
-        nosave=False,
-        score=1.0,
-        payload=None,
-        replace=False,
-        partial=False,
-        language=None,
-        no_create=False,
-        **fields
-    ):
-        """
-        Internal add_document used for both batch and single doc indexing
-        """
-        if conn is None:
-            conn = self.client
-
-        if partial or no_create:
-            replace = True
-
-        args = [ADD_CMD, self.index_name, doc_id, score]
-        if nosave:
-            args.append("NOSAVE")
-        if payload is not None:
-            args.append("PAYLOAD")
-            args.append(payload)
-        if replace:
-            args.append("REPLACE")
-            if partial:
-                args.append("PARTIAL")
-            if no_create:
-                args.append("NOCREATE")
-        if language:
-            args += ["LANGUAGE", language]
-        args.append("FIELDS")
-        args += list(itertools.chain(*fields.items()))
-        return conn.execute_command(*args)
-
-    def _add_document_hash(
-        self,
-        doc_id,
-        conn=None,
-        score=1.0,
-        language=None,
-        replace=False,
-    ):
-        """
-        Internal add_document_hash used for both batch and single doc indexing
-        """
-        if conn is None:
-            conn = self.client
-
-        args = [ADDHASH_CMD, self.index_name, doc_id, score]
-
-        if replace:
-            args.append("REPLACE")
-
-        if language:
-            args += ["LANGUAGE", language]
-
-        return conn.execute_command(*args)
-
-    def add_document(
-        self,
-        doc_id,
-        nosave=False,
-        score=1.0,
-        payload=None,
-        replace=False,
-        partial=False,
-        language=None,
-        no_create=False,
-        **fields
-    ):
-        """
-        Add a single document to the index.
-
-        ### Parameters
-
-        - **doc_id**: the id of the saved document.
-        - **nosave**: if set to true, we just index the document, and don't
-                      save a copy of it. This means that searches will just
-                      return ids.
-        - **score**: the document ranking, between 0.0 and 1.0
-        - **payload**: optional inner-index payload we can save for fast
-        i              access in scoring functions
-        - **replace**: if True, and the document already is in the index,
-        we perform an update and reindex the document
-        - **partial**: if True, the fields specified will be added to the
-                       existing document.
-                       This has the added benefit that any fields specified
-                       with `no_index`
-                       will not be reindexed again. Implies `replace`
-        - **language**: Specify the language used for document tokenization.
-        - **no_create**: if True, the document is only updated and reindexed
-                         if it already exists.
-                         If the document does not exist, an error will be
-                         returned. Implies `replace`
-        - **fields** kwargs dictionary of the document fields to be saved
-                         and/or indexed.
-                     NOTE: Geo points shoule be encoded as strings of "lon,lat"
-        """
-        return self._add_document(
-            doc_id,
-            conn=None,
-            nosave=nosave,
-            score=score,
-            payload=payload,
-            replace=replace,
-            partial=partial,
-            language=language,
-            no_create=no_create,
-            **fields
-        )
-
-    def add_document_hash(
-        self,
-        doc_id,
-        score=1.0,
-        language=None,
-        replace=False,
-    ):
-        """
-        Add a hash document to the index.
-
-        ### Parameters
-
-        - **doc_id**: the document's id. This has to be an existing HASH key
-                      in Redis that will hold the fields the index needs.
-        - **score**:  the document ranking, between 0.0 and 1.0
-        - **replace**: if True, and the document already is in the index, we
-                      perform an update and reindex the document
-        - **language**: Specify the language used for document tokenization.
-        """
-        return self._add_document_hash(
-            doc_id,
-            conn=None,
-            score=score,
-            language=language,
-            replace=replace,
-        )
-
-    def delete_document(self, doc_id, conn=None, delete_actual_document=False):
-        """
-        Delete a document from index
-        Returns 1 if the document was deleted, 0 if not
-
-        ### Parameters
-
-        - **delete_actual_document**: if set to True, RediSearch also delete
-                                      the actual document if it is in the index
-        """
-        args = [DEL_CMD, self.index_name, doc_id]
-        if conn is None:
-            conn = self.client
-        if delete_actual_document:
-            args.append("DD")
-
-        return conn.execute_command(*args)
-
-    def load_document(self, id):
-        """
-        Load a single document by id
-        """
-        fields = self.client.hgetall(id)
-        f2 = {to_string(k): to_string(v) for k, v in fields.items()}
-        fields = f2
-
-        try:
-            del fields["id"]
-        except KeyError:
-            pass
-
-        return Document(id=id, **fields)
-
-    def get(self, *ids):
-        """
-        Returns the full contents of multiple documents.
-
-        ### Parameters
-
-        - **ids**: the ids of the saved documents.
-        """
-
-        return self.client.execute_command(MGET_CMD, self.index_name, *ids)
-
-    def info(self):
-        """
-        Get info an stats about the the current index, including the number of
-        documents, memory consumption, etc
-        """
-
-        res = self.client.execute_command(INFO_CMD, self.index_name)
-        it = map(to_string, res)
-        return dict(zip(it, it))
-
-    def _mk_query_args(self, query):
-        args = [self.index_name]
-
-        if isinstance(query, str):
-            # convert the query from a text to a query object
-            query = Query(query)
-        if not isinstance(query, Query):
-            raise ValueError("Bad query type %s" % type(query))
-
-        args += query.get_args()
-        return args, query
-
-    def search(self, query):
-        """
-        Search the index for a given query, and return a result of documents
-
-        ### Parameters
-
-        - **query**: the search query. Either a text for simple queries with
-                     default parameters, or a Query object for complex queries.
-                     See RediSearch's documentation on query format
-        """
-        args, query = self._mk_query_args(query)
-        st = time.time()
-        res = self.execute_command(SEARCH_CMD, *args)
-
-        return Result(
-            res,
-            not query._no_content,
-            duration=(time.time() - st) * 1000.0,
-            has_payload=query._with_payloads,
-            with_scores=query._with_scores,
-        )
-
-    def explain(self, query):
-        args, query_text = self._mk_query_args(query)
-        return self.execute_command(EXPLAIN_CMD, *args)
-
-    def explain_cli(self, query):  # noqa
-        raise NotImplementedError("EXPLAINCLI will not be implemented.")
-
-    def aggregate(self, query):
-        """
-        Issue an aggregation query
-
-        ### Parameters
-
-        **query**: This can be either an `AggeregateRequest`, or a `Cursor`
-
-        An `AggregateResult` object is returned. You can access the rows from
-        its `rows` property, which will always yield the rows of the result.
-        """
-        if isinstance(query, AggregateRequest):
-            has_cursor = bool(query._cursor)
-            cmd = [AGGREGATE_CMD, self.index_name] + query.build_args()
-        elif isinstance(query, Cursor):
-            has_cursor = True
-            cmd = [CURSOR_CMD, "READ", self.index_name] + query.build_args()
-        else:
-            raise ValueError("Bad query", query)
-
-        raw = self.execute_command(*cmd)
-        if has_cursor:
-            if isinstance(query, Cursor):
-                query.cid = raw[1]
-                cursor = query
-            else:
-                cursor = Cursor(raw[1])
-            raw = raw[0]
-        else:
-            cursor = None
-
-        if isinstance(query, AggregateRequest) and query._with_schema:
-            schema = raw[0]
-            rows = raw[2:]
-        else:
-            schema = None
-            rows = raw[1:]
-
-        res = AggregateResult(rows, cursor, schema)
-        return res
-
-    def spellcheck(self, query, distance=None, include=None, exclude=None):
-        """
-        Issue a spellcheck query
-
-        ### Parameters
-
-        **query**: search query.
-        **distance***: the maximal Levenshtein distance for spelling
-                       suggestions (default: 1, max: 4).
-        **include**: specifies an inclusion custom dictionary.
-        **exclude**: specifies an exclusion custom dictionary.
-        """
-        cmd = [SPELLCHECK_CMD, self.index_name, query]
-        if distance:
-            cmd.extend(["DISTANCE", distance])
-
-        if include:
-            cmd.extend(["TERMS", "INCLUDE", include])
-
-        if exclude:
-            cmd.extend(["TERMS", "EXCLUDE", exclude])
-
-        raw = self.execute_command(*cmd)
-
-        corrections = {}
-        if raw == 0:
-            return corrections
-
-        for _correction in raw:
-            if isinstance(_correction, int) and _correction == 0:
-                continue
-
-            if len(_correction) != 3:
-                continue
-            if not _correction[2]:
-                continue
-            if not _correction[2][0]:
-                continue
-
-            # For spellcheck output
-            # 1)  1) "TERM"
-            #     2) "{term1}"
-            #     3)  1)  1)  "{score1}"
-            #             2)  "{suggestion1}"
-            #         2)  1)  "{score2}"
-            #             2)  "{suggestion2}"
-            #
-            # Following dictionary will be made
-            # corrections = {
-            #     '{term1}': [
-            #         {'score': '{score1}', 'suggestion': '{suggestion1}'},
-            #         {'score': '{score2}', 'suggestion': '{suggestion2}'}
-            #     ]
-            # }
-            corrections[_correction[1]] = [
-                {"score": _item[0], "suggestion": _item[1]}
-                for _item in _correction[2]
-            ]
-
-        return corrections
-
-    def dict_add(self, name, *terms):
-        """Adds terms to a dictionary.
-
-        ### Parameters
-
-        - **name**: Dictionary name.
-        - **terms**: List of items for adding to the dictionary.
-        """
-        cmd = [DICT_ADD_CMD, name]
-        cmd.extend(terms)
-        return self.execute_command(*cmd)
-
-    def dict_del(self, name, *terms):
-        """Deletes terms from a dictionary.
-
-        ### Parameters
-
-        - **name**: Dictionary name.
-        - **terms**: List of items for removing from the dictionary.
-        """
-        cmd = [DICT_DEL_CMD, name]
-        cmd.extend(terms)
-        return self.execute_command(*cmd)
-
-    def dict_dump(self, name):
-        """Dumps all terms in the given dictionary.
-
-        ### Parameters
-
-        - **name**: Dictionary name.
-        """
-        cmd = [DICT_DUMP_CMD, name]
-        return self.execute_command(*cmd)
-
-    def config_set(self, option, value):
-        """Set runtime configuration option.
-
-        ### Parameters
-
-        - **option**: the name of the configuration option.
-        - **value**: a value for the configuration option.
-        """
-        cmd = [CONFIG_CMD, "SET", option, value]
-        raw = self.execute_command(*cmd)
-        return raw == "OK"
-
-    def config_get(self, option):
-        """Get runtime configuration option value.
-
-        ### Parameters
-
-        - **option**: the name of the configuration option.
-        """
-        cmd = [CONFIG_CMD, "GET", option]
-        res = {}
-        raw = self.execute_command(*cmd)
-        if raw:
-            for kvs in raw:
-                res[kvs[0]] = kvs[1]
-        return res
-
-    def tagvals(self, tagfield):
-        """
-        Return a list of all possible tag values
-
-        ### Parameters
-
-        - **tagfield**: Tag field name
-        """
-
-        return self.execute_command(TAGVALS_CMD, self.index_name, tagfield)
-
-    def aliasadd(self, alias):
-        """
-        Alias a search index - will fail if alias already exists
-
-        ### Parameters
-
-        - **alias**: Name of the alias to create
-        """
-
-        return self.execute_command(ALIAS_ADD_CMD, alias, self.index_name)
-
-    def aliasupdate(self, alias):
-        """
-        Updates an alias - will fail if alias does not already exist
-
-        ### Parameters
-
-        - **alias**: Name of the alias to create
-        """
-
-        return self.execute_command(ALIAS_UPDATE_CMD, alias, self.index_name)
-
-    def aliasdel(self, alias):
-        """
-        Removes an alias to a search index
-
-        ### Parameters
-
-        - **alias**: Name of the alias to delete
-        """
-        return self.execute_command(ALIAS_DEL_CMD, alias)
-
-    def sugadd(self, key, *suggestions, **kwargs):
-        """
-        Add suggestion terms to the AutoCompleter engine. Each suggestion has
-        a score and string.
-        If kwargs["increment"] is true and the terms are already in the
-        server's dictionary, we increment their scores.
-        More information `here <https://oss.redis.com/redisearch/master/Commands/#ftsugadd>`_.  # noqa
-        """
-        # If Transaction is not False it will MULTI/EXEC which will error
-        pipe = self.pipeline(transaction=False)
-        for sug in suggestions:
-            args = [SUGADD_COMMAND, key, sug.string, sug.score]
-            if kwargs.get("increment"):
-                args.append("INCR")
-            if sug.payload:
-                args.append("PAYLOAD")
-                args.append(sug.payload)
-
-            pipe.execute_command(*args)
-
-        return pipe.execute()[-1]
-
-    def suglen(self, key):
-        """
-        Return the number of entries in the AutoCompleter index.
-        More information `here <https://oss.redis.com/redisearch/master/Commands/#ftsuglen>`_.  # noqa
-        """
-        return self.execute_command(SUGLEN_COMMAND, key)
-
-    def sugdel(self, key, string):
-        """
-        Delete a string from the AutoCompleter index.
-        Returns 1 if the string was found and deleted, 0 otherwise.
-        More information `here <https://oss.redis.com/redisearch/master/Commands/#ftsugdel>`_.  # noqa
-        """
-        return self.execute_command(SUGDEL_COMMAND, key, string)
-
-    def sugget(
-        self, key, prefix, fuzzy=False, num=10, with_scores=False,
-        with_payloads=False
-    ):
-        """
-        Get a list of suggestions from the AutoCompleter, for a given prefix.
-        More information `here <https://oss.redis.com/redisearch/master/Commands/#ftsugget>`_.  # noqa
-
-        Parameters:
-
-        prefix : str
-            The prefix we are searching. **Must be valid ascii or utf-8**
-        fuzzy : bool
-            If set to true, the prefix search is done in fuzzy mode.
-            **NOTE**: Running fuzzy searches on short (<3 letters) prefixes
-            can be very
-            slow, and even scan the entire index.
-        with_scores : bool
-            If set to true, we also return the (refactored) score of
-            each suggestion.
-            This is normally not needed, and is NOT the original score
-            inserted into the index.
-        with_payloads : bool
-            Return suggestion payloads
-        num : int
-            The maximum number of results we return. Note that we might
-            return less. The algorithm trims irrelevant suggestions.
-
-        Returns:
-
-        list:
-             A list of Suggestion objects. If with_scores was False, the
-             score of all suggestions is 1.
-        """
-        args = [SUGGET_COMMAND, key, prefix, "MAX", num]
-        if fuzzy:
-            args.append(FUZZY)
-        if with_scores:
-            args.append(WITHSCORES)
-        if with_payloads:
-            args.append(WITHPAYLOADS)
-
-        ret = self.execute_command(*args)
-        results = []
-        if not ret:
-            return results
-
-        parser = SuggestionParser(with_scores, with_payloads, ret)
-        return [s for s in parser]
-
-    def synupdate(self, groupid, skipinitial=False, *terms):
-        """
-        Updates a synonym group.
-        The command is used to create or update a synonym group with
-        additional terms.
-        Only documents which were indexed after the update will be affected.
-
-        Parameters:
-
-        groupid :
-            Synonym group id.
-        skipinitial : bool
-            If set to true, we do not scan and index.
-        terms :
-            The terms.
-        """
-        cmd = [SYNUPDATE_CMD, self.index_name, groupid]
-        if skipinitial:
-            cmd.extend(["SKIPINITIALSCAN"])
-        cmd.extend(terms)
-        return self.execute_command(*cmd)
-
-    def syndump(self):
-        """
-        Dumps the contents of a synonym group.
-
-        The command is used to dump the synonyms data structure.
-        Returns a list of synonym terms and their synonym group ids.
-        """
-        raw = self.execute_command(SYNDUMP_CMD, self.index_name)
-        return {raw[i]: raw[i + 1] for i in range(0, len(raw), 2)}
diff --git a/redis/commands/search/document.py b/redis/commands/search/document.py
deleted file mode 100644
index 0d4255d..0000000
--- a/redis/commands/search/document.py
+++ /dev/null
@@ -1,13 +0,0 @@
-class Document(object):
-    """
-    Represents a single document in a result set
-    """
-
-    def __init__(self, id, payload=None, **fields):
-        self.id = id
-        self.payload = payload
-        for k, v in fields.items():
-            setattr(self, k, v)
-
-    def __repr__(self):
-        return "Document %s" % self.__dict__
diff --git a/redis/commands/search/field.py b/redis/commands/search/field.py
deleted file mode 100644
index 45114a4..0000000
--- a/redis/commands/search/field.py
+++ /dev/null
@@ -1,94 +0,0 @@
-class Field(object):
-
-    NUMERIC = "NUMERIC"
-    TEXT = "TEXT"
-    WEIGHT = "WEIGHT"
-    GEO = "GEO"
-    TAG = "TAG"
-    SORTABLE = "SORTABLE"
-    NOINDEX = "NOINDEX"
-    AS = "AS"
-
-    def __init__(self, name, args=[], sortable=False,
-                 no_index=False, as_name=None):
-        self.name = name
-        self.args = args
-        self.args_suffix = list()
-        self.as_name = as_name
-
-        if sortable:
-            self.args_suffix.append(Field.SORTABLE)
-        if no_index:
-            self.args_suffix.append(Field.NOINDEX)
-
-        if no_index and not sortable:
-            raise ValueError("Non-Sortable non-Indexable fields are ignored")
-
-    def append_arg(self, value):
-        self.args.append(value)
-
-    def redis_args(self):
-        args = [self.name]
-        if self.as_name:
-            args += [self.AS, self.as_name]
-        args += self.args
-        args += self.args_suffix
-        return args
-
-
-class TextField(Field):
-    """
-    TextField is used to define a text field in a schema definition
-    """
-
-    NOSTEM = "NOSTEM"
-    PHONETIC = "PHONETIC"
-
-    def __init__(
-        self, name, weight=1.0, no_stem=False, phonetic_matcher=None, **kwargs
-    ):
-        Field.__init__(self, name,
-                       args=[Field.TEXT, Field.WEIGHT, weight], **kwargs)
-
-        if no_stem:
-            Field.append_arg(self, self.NOSTEM)
-        if phonetic_matcher and phonetic_matcher in [
-            "dm:en",
-            "dm:fr",
-            "dm:pt",
-            "dm:es",
-        ]:
-            Field.append_arg(self, self.PHONETIC)
-            Field.append_arg(self, phonetic_matcher)
-
-
-class NumericField(Field):
-    """
-    NumericField is used to define a numeric field in a schema definition
-    """
-
-    def __init__(self, name, **kwargs):
-        Field.__init__(self, name, args=[Field.NUMERIC], **kwargs)
-
-
-class GeoField(Field):
-    """
-    GeoField is used to define a geo-indexing field in a schema definition
-    """
-
-    def __init__(self, name, **kwargs):
-        Field.__init__(self, name, args=[Field.GEO], **kwargs)
-
-
-class TagField(Field):
-    """
-    TagField is a tag-indexing field with simpler compression and tokenization.
-    See http://redisearch.io/Tags/
-    """
-
-    SEPARATOR = "SEPARATOR"
-
-    def __init__(self, name, separator=",", **kwargs):
-        Field.__init__(
-            self, name, args=[Field.TAG, self.SEPARATOR, separator], **kwargs
-        )
diff --git a/redis/commands/search/indexDefinition.py b/redis/commands/search/indexDefinition.py
deleted file mode 100644
index 4fbc609..0000000
--- a/redis/commands/search/indexDefinition.py
+++ /dev/null
@@ -1,80 +0,0 @@
-from enum import Enum
-
-
-class IndexType(Enum):
-    """Enum of the currently supported index types."""
-
-    HASH = 1
-    JSON = 2
-
-
-class IndexDefinition(object):
-    """IndexDefinition is used to define a index definition for automatic
-    indexing on Hash or Json update."""
-
-    def __init__(
-        self,
-        prefix=[],
-        filter=None,
-        language_field=None,
-        language=None,
-        score_field=None,
-        score=1.0,
-        payload_field=None,
-        index_type=None,
-    ):
-        self.args = []
-        self._appendIndexType(index_type)
-        self._appendPrefix(prefix)
-        self._appendFilter(filter)
-        self._appendLanguage(language_field, language)
-        self._appendScore(score_field, score)
-        self._appendPayload(payload_field)
-
-    def _appendIndexType(self, index_type):
-        """Append `ON HASH` or `ON JSON` according to the enum."""
-        if index_type is IndexType.HASH:
-            self.args.extend(["ON", "HASH"])
-        elif index_type is IndexType.JSON:
-            self.args.extend(["ON", "JSON"])
-        elif index_type is not None:
-            raise RuntimeError("index_type must be one of {}".
-                               format(list(IndexType)))
-
-    def _appendPrefix(self, prefix):
-        """Append PREFIX."""
-        if len(prefix) > 0:
-            self.args.append("PREFIX")
-            self.args.append(len(prefix))
-            for p in prefix:
-                self.args.append(p)
-
-    def _appendFilter(self, filter):
-        """Append FILTER."""
-        if filter is not None:
-            self.args.append("FILTER")
-            self.args.append(filter)
-
-    def _appendLanguage(self, language_field, language):
-        """Append LANGUAGE_FIELD and LANGUAGE."""
-        if language_field is not None:
-            self.args.append("LANGUAGE_FIELD")
-            self.args.append(language_field)
-        if language is not None:
-            self.args.append("LANGUAGE")
-            self.args.append(language)
-
-    def _appendScore(self, score_field, score):
-        """Append SCORE_FIELD and SCORE."""
-        if score_field is not None:
-            self.args.append("SCORE_FIELD")
-            self.args.append(score_field)
-        if score is not None:
-            self.args.append("SCORE")
-            self.args.append(score)
-
-    def _appendPayload(self, payload_field):
-        """Append PAYLOAD_FIELD."""
-        if payload_field is not None:
-            self.args.append("PAYLOAD_FIELD")
-            self.args.append(payload_field)
diff --git a/redis/commands/search/query.py b/redis/commands/search/query.py
deleted file mode 100644
index 85a8255..0000000
--- a/redis/commands/search/query.py
+++ /dev/null
@@ -1,325 +0,0 @@
-class Query(object):
-    """
-    Query is used to build complex queries that have more parameters than just
-    the query string. The query string is set in the constructor, and other
-    options have setter functions.
-
-    The setter functions return the query object, so they can be chained,
-    i.e. `Query("foo").verbatim().filter(...)` etc.
-    """
-
-    def __init__(self, query_string):
-        """
-        Create a new query object.
-        The query string is set in the constructor, and other options have
-        setter functions.
-        """
-
-        self._query_string = query_string
-        self._offset = 0
-        self._num = 10
-        self._no_content = False
-        self._no_stopwords = False
-        self._fields = None
-        self._verbatim = False
-        self._with_payloads = False
-        self._with_scores = False
-        self._scorer = False
-        self._filters = list()
-        self._ids = None
-        self._slop = -1
-        self._in_order = False
-        self._sortby = None
-        self._return_fields = []
-        self._summarize_fields = []
-        self._highlight_fields = []
-        self._language = None
-        self._expander = None
-
-    def query_string(self):
-        """Return the query string of this query only."""
-        return self._query_string
-
-    def limit_ids(self, *ids):
-        """Limit the results to a specific set of pre-known document
-        ids of any length."""
-        self._ids = ids
-        return self
-
-    def return_fields(self, *fields):
-        """Add fields to return fields."""
-        self._return_fields += fields
-        return self
-
-    def return_field(self, field, as_field=None):
-        """Add field to return fields (Optional: add 'AS' name
-        to the field)."""
-        self._return_fields.append(field)
-        if as_field is not None:
-            self._return_fields += ("AS", as_field)
-        return self
-
-    def _mk_field_list(self, fields):
-        if not fields:
-            return []
-        return  \
-            [fields] if isinstance(fields, str) else list(fields)
-
-    def summarize(self, fields=None, context_len=None,
-                  num_frags=None, sep=None):
-        """
-        Return an abridged format of the field, containing only the segments of
-        the field which contain the matching term(s).
-
-        If `fields` is specified, then only the mentioned fields are
-        summarized; otherwise all results are summarized.
-
-        Server side defaults are used for each option (except `fields`)
-        if not specified
-
-        - **fields** List of fields to summarize. All fields are summarized
-        if not specified
-        - **context_len** Amount of context to include with each fragment
-        - **num_frags** Number of fragments per document
-        - **sep** Separator string to separate fragments
-        """
-        args = ["SUMMARIZE"]
-        fields = self._mk_field_list(fields)
-        if fields:
-            args += ["FIELDS", str(len(fields))] + fields
-
-        if context_len is not None:
-            args += ["LEN", str(context_len)]
-        if num_frags is not None:
-            args += ["FRAGS", str(num_frags)]
-        if sep is not None:
-            args += ["SEPARATOR", sep]
-
-        self._summarize_fields = args
-        return self
-
-    def highlight(self, fields=None, tags=None):
-        """
-        Apply specified markup to matched term(s) within the returned field(s).
-
-        - **fields** If specified then only those mentioned fields are
-        highlighted, otherwise all fields are highlighted
-        - **tags** A list of two strings to surround the match.
-        """
-        args = ["HIGHLIGHT"]
-        fields = self._mk_field_list(fields)
-        if fields:
-            args += ["FIELDS", str(len(fields))] + fields
-        if tags:
-            args += ["TAGS"] + list(tags)
-
-        self._highlight_fields = args
-        return self
-
-    def language(self, language):
-        """
-        Analyze the query as being in the specified language.
-
-        :param language: The language (e.g. `chinese` or `english`)
-        """
-        self._language = language
-        return self
-
-    def slop(self, slop):
-        """Allow a maximum of N intervening non matched terms between
-        phrase terms (0 means exact phrase).
-        """
-        self._slop = slop
-        return self
-
-    def in_order(self):
-        """
-        Match only documents where the query terms appear in
-        the same order in the document.
-        i.e. for the query "hello world", we do not match "world hello"
-        """
-        self._in_order = True
-        return self
-
-    def scorer(self, scorer):
-        """
-        Use a different scoring function to evaluate document relevance.
-        Default is `TFIDF`.
-
-        :param scorer: The scoring function to use
-                       (e.g. `TFIDF.DOCNORM` or `BM25`)
-        """
-        self._scorer = scorer
-        return self
-
-    def get_args(self):
-        """Format the redis arguments for this query and return them."""
-        args = [self._query_string]
-        args += self._get_args_tags()
-        args += self._summarize_fields + self._highlight_fields
-        args += ["LIMIT", self._offset, self._num]
-        return args
-
-    def _get_args_tags(self):
-        args = []
-        if self._no_content:
-            args.append("NOCONTENT")
-        if self._fields:
-            args.append("INFIELDS")
-            args.append(len(self._fields))
-            args += self._fields
-        if self._verbatim:
-            args.append("VERBATIM")
-        if self._no_stopwords:
-            args.append("NOSTOPWORDS")
-        if self._filters:
-            for flt in self._filters:
-                if not isinstance(flt, Filter):
-                    raise AttributeError("Did not receive a Filter object.")
-                args += flt.args
-        if self._with_payloads:
-            args.append("WITHPAYLOADS")
-        if self._scorer:
-            args += ["SCORER", self._scorer]
-        if self._with_scores:
-            args.append("WITHSCORES")
-        if self._ids:
-            args.append("INKEYS")
-            args.append(len(self._ids))
-            args += self._ids
-        if self._slop >= 0:
-            args += ["SLOP", self._slop]
-        if self._in_order:
-            args.append("INORDER")
-        if self._return_fields:
-            args.append("RETURN")
-            args.append(len(self._return_fields))
-            args += self._return_fields
-        if self._sortby:
-            if not isinstance(self._sortby, SortbyField):
-                raise AttributeError("Did not receive a SortByField.")
-            args.append("SORTBY")
-            args += self._sortby.args
-        if self._language:
-            args += ["LANGUAGE", self._language]
-        if self._expander:
-            args += ["EXPANDER", self._expander]
-
-        return args
-
-    def paging(self, offset, num):
-        """
-        Set the paging for the query (defaults to 0..10).
-
-        - **offset**: Paging offset for the results. Defaults to 0
-        - **num**: How many results do we want
-        """
-        self._offset = offset
-        self._num = num
-        return self
-
-    def verbatim(self):
-        """Set the query to be verbatim, i.e. use no query expansion
-        or stemming.
-        """
-        self._verbatim = True
-        return self
-
-    def no_content(self):
-        """Set the query to only return ids and not the document content."""
-        self._no_content = True
-        return self
-
-    def no_stopwords(self):
-        """
-        Prevent the query from being filtered for stopwords.
-        Only useful in very big queries that you are certain contain
-        no stopwords.
-        """
-        self._no_stopwords = True
-        return self
-
-    def with_payloads(self):
-        """Ask the engine to return document payloads."""
-        self._with_payloads = True
-        return self
-
-    def with_scores(self):
-        """Ask the engine to return document search scores."""
-        self._with_scores = True
-        return self
-
-    def limit_fields(self, *fields):
-        """
-        Limit the search to specific TEXT fields only.
-
-        - **fields**: A list of strings, case sensitive field names
-        from the defined schema.
-        """
-        self._fields = fields
-        return self
-
-    def add_filter(self, flt):
-        """
-        Add a numeric or geo filter to the query.
-        **Currently only one of each filter is supported by the engine**
-
-        - **flt**: A NumericFilter or GeoFilter object, used on a
-        corresponding field
-        """
-
-        self._filters.append(flt)
-        return self
-
-    def sort_by(self, field, asc=True):
-        """
-        Add a sortby field to the query.
-
-        - **field** - the name of the field to sort by
-        - **asc** - when `True`, sorting will be done in asceding order
-        """
-        self._sortby = SortbyField(field, asc)
-        return self
-
-    def expander(self, expander):
-        """
-        Add a expander field to the query.
-
-        - **expander** - the name of the expander
-        """
-        self._expander = expander
-        return self
-
-
-class Filter(object):
-    def __init__(self, keyword, field, *args):
-        self.args = [keyword, field] + list(args)
-
-
-class NumericFilter(Filter):
-    INF = "+inf"
-    NEG_INF = "-inf"
-
-    def __init__(self, field, minval, maxval, minExclusive=False,
-                 maxExclusive=False):
-        args = [
-            minval if not minExclusive else "({}".format(minval),
-            maxval if not maxExclusive else "({}".format(maxval),
-        ]
-
-        Filter.__init__(self, "FILTER", field, *args)
-
-
-class GeoFilter(Filter):
-    METERS = "m"
-    KILOMETERS = "km"
-    FEET = "ft"
-    MILES = "mi"
-
-    def __init__(self, field, lon, lat, radius, unit=KILOMETERS):
-        Filter.__init__(self, "GEOFILTER", field, lon, lat, radius, unit)
-
-
-class SortbyField(object):
-    def __init__(self, field, asc=True):
-        self.args = [field, "ASC" if asc else "DESC"]
diff --git a/redis/commands/search/querystring.py b/redis/commands/search/querystring.py
deleted file mode 100644
index aecd3b8..0000000
--- a/redis/commands/search/querystring.py
+++ /dev/null
@@ -1,321 +0,0 @@
-def tags(*t):
-    """
-    Indicate that the values should be matched to a tag field
-
-    ### Parameters
-
-    - **t**: Tags to search for
-    """
-    if not t:
-        raise ValueError("At least one tag must be specified")
-    return TagValue(*t)
-
-
-def between(a, b, inclusive_min=True, inclusive_max=True):
-    """
-    Indicate that value is a numeric range
-    """
-    return RangeValue(a, b, inclusive_min=inclusive_min,
-                      inclusive_max=inclusive_max)
-
-
-def equal(n):
-    """
-    Match a numeric value
-    """
-    return between(n, n)
-
-
-def lt(n):
-    """
-    Match any value less than n
-    """
-    return between(None, n, inclusive_max=False)
-
-
-def le(n):
-    """
-    Match any value less or equal to n
-    """
-    return between(None, n, inclusive_max=True)
-
-
-def gt(n):
-    """
-    Match any value greater than n
-    """
-    return between(n, None, inclusive_min=False)
-
-
-def ge(n):
-    """
-    Match any value greater or equal to n
-    """
-    return between(n, None, inclusive_min=True)
-
-
-def geo(lat, lon, radius, unit="km"):
-    """
-    Indicate that value is a geo region
-    """
-    return GeoValue(lat, lon, radius, unit)
-
-
-class Value(object):
-    @property
-    def combinable(self):
-        """
-        Whether this type of value may be combined with other values
-        for the same field. This makes the filter potentially more efficient
-        """
-        return False
-
-    @staticmethod
-    def make_value(v):
-        """
-        Convert an object to a value, if it is not a value already
-        """
-        if isinstance(v, Value):
-            return v
-        return ScalarValue(v)
-
-    def to_string(self):
-        raise NotImplementedError()
-
-    def __str__(self):
-        return self.to_string()
-
-
-class RangeValue(Value):
-    combinable = False
-
-    def __init__(self, a, b, inclusive_min=False, inclusive_max=False):
-        if a is None:
-            a = "-inf"
-        if b is None:
-            b = "inf"
-        self.range = [str(a), str(b)]
-        self.inclusive_min = inclusive_min
-        self.inclusive_max = inclusive_max
-
-    def to_string(self):
-        return "[{1}{0[0]} {2}{0[1]}]".format(
-            self.range,
-            "(" if not self.inclusive_min else "",
-            "(" if not self.inclusive_max else "",
-        )
-
-
-class ScalarValue(Value):
-    combinable = True
-
-    def __init__(self, v):
-        self.v = str(v)
-
-    def to_string(self):
-        return self.v
-
-
-class TagValue(Value):
-    combinable = False
-
-    def __init__(self, *tags):
-        self.tags = tags
-
-    def to_string(self):
-        return "{" + " | ".join(str(t) for t in self.tags) + "}"
-
-
-class GeoValue(Value):
-    def __init__(self, lon, lat, radius, unit="km"):
-        self.lon = lon
-        self.lat = lat
-        self.radius = radius
-        self.unit = unit
-
-
-class Node(object):
-    def __init__(self, *children, **kwparams):
-        """
-        Create a node
-
-        ### Parameters
-
-        - **children**: One or more sub-conditions. These can be additional
-            `intersect`, `disjunct`, `union`, `optional`, or any other `Node`
-            type.
-
-            The semantics of multiple conditions are dependent on the type of
-            query. For an `intersection` node, this amounts to a logical AND,
-            for a `union` node, this amounts to a logical `OR`.
-
-        - **kwparams**: key-value parameters. Each key is the name of a field,
-            and the value should be a field value. This can be one of the
-            following:
-
-            - Simple string (for text field matches)
-            - value returned by one of the helper functions
-            - list of either a string or a value
-
-
-        ### Examples
-
-        Field `num` should be between 1 and 10
-        ```
-        intersect(num=between(1, 10)
-        ```
-
-        Name can either be `bob` or `john`
-
-        ```
-        union(name=("bob", "john"))
-        ```
-
-        Don't select countries in Israel, Japan, or US
-
-        ```
-        disjunct_union(country=("il", "jp", "us"))
-        ```
-        """
-
-        self.params = []
-
-        kvparams = {}
-        for k, v in kwparams.items():
-            curvals = kvparams.setdefault(k, [])
-            if isinstance(v, (str, int, float)):
-                curvals.append(Value.make_value(v))
-            elif isinstance(v, Value):
-                curvals.append(v)
-            else:
-                curvals.extend(Value.make_value(subv) for subv in v)
-
-        self.params += [Node.to_node(p) for p in children]
-
-        for k, v in kvparams.items():
-            self.params.extend(self.join_fields(k, v))
-
-    def join_fields(self, key, vals):
-        if len(vals) == 1:
-            return [BaseNode("@{}:{}".format(key, vals[0].to_string()))]
-        if not vals[0].combinable:
-            return [BaseNode("@{}:{}".format(key,
-                                             v.to_string())) for v in vals]
-        s = BaseNode(
-            "@{}:({})".format(key,
-                              self.JOINSTR.join(v.to_string() for v in vals))
-        )
-        return [s]
-
-    @classmethod
-    def to_node(cls, obj):  # noqa
-        if isinstance(obj, Node):
-            return obj
-        return BaseNode(obj)
-
-    @property
-    def JOINSTR(self):
-        raise NotImplementedError()
-
-    def to_string(self, with_parens=None):
-        with_parens = self._should_use_paren(with_parens)
-        pre, post = ("(", ")") if with_parens else ("", "")
-        return "{}{}{}".format(
-            pre, self.JOINSTR.join(n.to_string() for n in self.params), post
-        )
-
-    def _should_use_paren(self, optval):
-        if optval is not None:
-            return optval
-        return len(self.params) > 1
-
-    def __str__(self):
-        return self.to_string()
-
-
-class BaseNode(Node):
-    def __init__(self, s):
-        super(BaseNode, self).__init__()
-        self.s = str(s)
-
-    def to_string(self, with_parens=None):
-        return self.s
-
-
-class IntersectNode(Node):
-    """
-    Create an intersection node. All children need to be satisfied in order for
-    this node to evaluate as true
-    """
-
-    JOINSTR = " "
-
-
-class UnionNode(Node):
-    """
-    Create a union node. Any of the children need to be satisfied in order for
-    this node to evaluate as true
-    """
-
-    JOINSTR = "|"
-
-
-class DisjunctNode(IntersectNode):
-    """
-    Create a disjunct node. In order for this node to be true, all of its
-    children must evaluate to false
-    """
-
-    def to_string(self, with_parens=None):
-        with_parens = self._should_use_paren(with_parens)
-        ret = super(DisjunctNode, self).to_string(with_parens=False)
-        if with_parens:
-            return "(-" + ret + ")"
-        else:
-            return "-" + ret
-
-
-class DistjunctUnion(DisjunctNode):
-    """
-    This node is true if *all* of its children are false. This is equivalent to
-    ```
-    disjunct(union(...))
-    ```
-    """
-
-    JOINSTR = "|"
-
-
-class OptionalNode(IntersectNode):
-    """
-    Create an optional node. If this nodes evaluates to true, then the document
-    will be rated higher in score/rank.
-    """
-
-    def to_string(self, with_parens=None):
-        with_parens = self._should_use_paren(with_parens)
-        ret = super(OptionalNode, self).to_string(with_parens=False)
-        if with_parens:
-            return "(~" + ret + ")"
-        else:
-            return "~" + ret
-
-
-def intersect(*args, **kwargs):
-    return IntersectNode(*args, **kwargs)
-
-
-def union(*args, **kwargs):
-    return UnionNode(*args, **kwargs)
-
-
-def disjunct(*args, **kwargs):
-    return DisjunctNode(*args, **kwargs)
-
-
-def disjunct_union(*args, **kwargs):
-    return DistjunctUnion(*args, **kwargs)
-
-
-def querystring(*args, **kwargs):
-    return intersect(*args, **kwargs).to_string()
diff --git a/redis/commands/search/reducers.py b/redis/commands/search/reducers.py
deleted file mode 100644
index 6cbbf2f..0000000
--- a/redis/commands/search/reducers.py
+++ /dev/null
@@ -1,178 +0,0 @@
-from .aggregation import Reducer, SortDirection
-
-
-class FieldOnlyReducer(Reducer):
-    def __init__(self, field):
-        super(FieldOnlyReducer, self).__init__(field)
-        self._field = field
-
-
-class count(Reducer):
-    """
-    Counts the number of results in the group
-    """
-
-    NAME = "COUNT"
-
-    def __init__(self):
-        super(count, self).__init__()
-
-
-class sum(FieldOnlyReducer):
-    """
-    Calculates the sum of all the values in the given fields within the group
-    """
-
-    NAME = "SUM"
-
-    def __init__(self, field):
-        super(sum, self).__init__(field)
-
-
-class min(FieldOnlyReducer):
-    """
-    Calculates the smallest value in the given field within the group
-    """
-
-    NAME = "MIN"
-
-    def __init__(self, field):
-        super(min, self).__init__(field)
-
-
-class max(FieldOnlyReducer):
-    """
-    Calculates the largest value in the given field within the group
-    """
-
-    NAME = "MAX"
-
-    def __init__(self, field):
-        super(max, self).__init__(field)
-
-
-class avg(FieldOnlyReducer):
-    """
-    Calculates the mean value in the given field within the group
-    """
-
-    NAME = "AVG"
-
-    def __init__(self, field):
-        super(avg, self).__init__(field)
-
-
-class tolist(FieldOnlyReducer):
-    """
-    Returns all the matched properties in a list
-    """
-
-    NAME = "TOLIST"
-
-    def __init__(self, field):
-        super(tolist, self).__init__(field)
-
-
-class count_distinct(FieldOnlyReducer):
-    """
-    Calculate the number of distinct values contained in all the results in
-    the group for the given field
-    """
-
-    NAME = "COUNT_DISTINCT"
-
-    def __init__(self, field):
-        super(count_distinct, self).__init__(field)
-
-
-class count_distinctish(FieldOnlyReducer):
-    """
-    Calculate the number of distinct values contained in all the results in the
-    group for the given field. This uses a faster algorithm than
-    `count_distinct` but is less accurate
-    """
-
-    NAME = "COUNT_DISTINCTISH"
-
-
-class quantile(Reducer):
-    """
-    Return the value for the nth percentile within the range of values for the
-    field within the group.
-    """
-
-    NAME = "QUANTILE"
-
-    def __init__(self, field, pct):
-        super(quantile, self).__init__(field, str(pct))
-        self._field = field
-
-
-class stddev(FieldOnlyReducer):
-    """
-    Return the standard deviation for the values within the group
-    """
-
-    NAME = "STDDEV"
-
-    def __init__(self, field):
-        super(stddev, self).__init__(field)
-
-
-class first_value(Reducer):
-    """
-    Selects the first value within the group according to sorting parameters
-    """
-
-    NAME = "FIRST_VALUE"
-
-    def __init__(self, field, *byfields):
-        """
-        Selects the first value of the given field within the group.
-
-        ### Parameter
-
-        - **field**: Source field used for the value
-        - **byfields**: How to sort the results. This can be either the
-            *class* of `aggregation.Asc` or `aggregation.Desc` in which
-            case the field `field` is also used as the sort input.
-
-            `byfields` can also be one or more *instances* of `Asc` or `Desc`
-            indicating the sort order for these fields
-        """
-
-        fieldstrs = []
-        if (
-            len(byfields) == 1
-            and isinstance(byfields[0], type)
-            and issubclass(byfields[0], SortDirection)
-        ):
-            byfields = [byfields[0](field)]
-
-        for f in byfields:
-            fieldstrs += [f.field, f.DIRSTRING]
-
-        args = [field]
-        if fieldstrs:
-            args += ["BY"] + fieldstrs
-        super(first_value, self).__init__(*args)
-        self._field = field
-
-
-class random_sample(Reducer):
-    """
-    Returns a random sample of items from the dataset, from the given property
-    """
-
-    NAME = "RANDOM_SAMPLE"
-
-    def __init__(self, field, size):
-        """
-        ### Parameter
-
-        **field**: Field to sample from
-        **size**: Return this many items (can be less)
-        """
-        args = [field, str(size)]
-        super(random_sample, self).__init__(*args)
-        self._field = field
diff --git a/redis/commands/search/result.py b/redis/commands/search/result.py
deleted file mode 100644
index 9cd922a..0000000
--- a/redis/commands/search/result.py
+++ /dev/null
@@ -1,73 +0,0 @@
-from .document import Document
-from ._util import to_string
-
-
-class Result(object):
-    """
-    Represents the result of a search query, and has an array of Document
-    objects
-    """
-
-    def __init__(
-        self, res, hascontent, duration=0, has_payload=False, with_scores=False
-    ):
-        """
-        - **snippets**: An optional dictionary of the form
-        {field: snippet_size} for snippet formatting
-        """
-
-        self.total = res[0]
-        self.duration = duration
-        self.docs = []
-
-        step = 1
-        if hascontent:
-            step = step + 1
-        if has_payload:
-            step = step + 1
-        if with_scores:
-            step = step + 1
-
-        offset = 2 if with_scores else 1
-
-        for i in range(1, len(res), step):
-            id = to_string(res[i])
-            payload = to_string(res[i + offset]) if has_payload else None
-            # fields_offset = 2 if has_payload else 1
-            fields_offset = offset + 1 if has_payload else offset
-            score = float(res[i + 1]) if with_scores else None
-
-            fields = {}
-            if hascontent:
-                fields = (
-                    dict(
-                        dict(
-                            zip(
-                                map(to_string, res[i + fields_offset][::2]),
-                                map(to_string, res[i + fields_offset][1::2]),
-                            )
-                        )
-                    )
-                    if hascontent
-                    else {}
-                )
-            try:
-                del fields["id"]
-            except KeyError:
-                pass
-
-            try:
-                fields["json"] = fields["$"]
-                del fields["$"]
-            except KeyError:
-                pass
-
-            doc = (
-                Document(id, score=score, payload=payload, **fields)
-                if with_scores
-                else Document(id, payload=payload, **fields)
-            )
-            self.docs.append(doc)
-
-    def __repr__(self):
-        return "Result{%d total, docs: %s}" % (self.total, self.docs)
diff --git a/redis/commands/search/suggestion.py b/redis/commands/search/suggestion.py
deleted file mode 100644
index 3401af9..0000000
--- a/redis/commands/search/suggestion.py
+++ /dev/null
@@ -1,53 +0,0 @@
-from ._util import to_string
-
-
-class Suggestion(object):
-    """
-    Represents a single suggestion being sent or returned from the
-    autocomplete server
-    """
-
-    def __init__(self, string, score=1.0, payload=None):
-        self.string = to_string(string)
-        self.payload = to_string(payload)
-        self.score = score
-
-    def __repr__(self):
-        return self.string
-
-
-class SuggestionParser(object):
-    """
-    Internal class used to parse results from the `SUGGET` command.
-    This needs to consume either 1, 2, or 3 values at a time from
-    the return value depending on what objects were requested
-    """
-
-    def __init__(self, with_scores, with_payloads, ret):
-        self.with_scores = with_scores
-        self.with_payloads = with_payloads
-
-        if with_scores and with_payloads:
-            self.sugsize = 3
-            self._scoreidx = 1
-            self._payloadidx = 2
-        elif with_scores:
-            self.sugsize = 2
-            self._scoreidx = 1
-        elif with_payloads:
-            self.sugsize = 2
-            self._payloadidx = 1
-        else:
-            self.sugsize = 1
-            self._scoreidx = -1
-
-        self._sugs = ret
-
-    def __iter__(self):
-        for i in range(0, len(self._sugs), self.sugsize):
-            ss = self._sugs[i]
-            score = float(self._sugs[i + self._scoreidx]) \
-                if self.with_scores else 1.0
-            payload = self._sugs[i + self._payloadidx] \
-                if self.with_payloads else None
-            yield Suggestion(ss, score, payload)
diff --git a/redis/commands/timeseries/__init__.py b/redis/commands/timeseries/__init__.py
deleted file mode 100644
index 5ce538f..0000000
--- a/redis/commands/timeseries/__init__.py
+++ /dev/null
@@ -1,85 +0,0 @@
-import redis.client
-
-from .utils import (
-    parse_range,
-    parse_get,
-    parse_m_range,
-    parse_m_get,
-)
-from .info import TSInfo
-from ..helpers import parse_to_list
-from .commands import (
-    ALTER_CMD,
-    CREATE_CMD,
-    CREATERULE_CMD,
-    DELETERULE_CMD,
-    DEL_CMD,
-    GET_CMD,
-    INFO_CMD,
-    MGET_CMD,
-    MRANGE_CMD,
-    MREVRANGE_CMD,
-    QUERYINDEX_CMD,
-    RANGE_CMD,
-    REVRANGE_CMD,
-    TimeSeriesCommands,
-)
-
-
-class TimeSeries(TimeSeriesCommands):
-    """
-    This class subclasses redis-py's `Redis` and implements RedisTimeSeries's
-    commands (prefixed with "ts").
-    The client allows to interact with RedisTimeSeries and use all of it's
-    functionality.
-    """
-
-    def __init__(self, client=None, **kwargs):
-        """Create a new RedisTimeSeries client."""
-        # Set the module commands' callbacks
-        self.MODULE_CALLBACKS = {
-            CREATE_CMD: redis.client.bool_ok,
-            ALTER_CMD: redis.client.bool_ok,
-            CREATERULE_CMD: redis.client.bool_ok,
-            DEL_CMD: int,
-            DELETERULE_CMD: redis.client.bool_ok,
-            RANGE_CMD: parse_range,
-            REVRANGE_CMD: parse_range,
-            MRANGE_CMD: parse_m_range,
-            MREVRANGE_CMD: parse_m_range,
-            GET_CMD: parse_get,
-            MGET_CMD: parse_m_get,
-            INFO_CMD: TSInfo,
-            QUERYINDEX_CMD: parse_to_list,
-        }
-
-        self.client = client
-        self.execute_command = client.execute_command
-
-        for key, value in self.MODULE_CALLBACKS.items():
-            self.client.set_response_callback(key, value)
-
-    def pipeline(self, transaction=True, shard_hint=None):
-        """Creates a pipeline for the TimeSeries module, that can be used
-        for executing only TimeSeries commands and core commands.
-
-        Usage example:
-
-        r = redis.Redis()
-        pipe = r.ts().pipeline()
-        for i in range(100):
-            pipeline.add("with_pipeline", i, 1.1 * i)
-        pipeline.execute()
-
-        """
-        p = Pipeline(
-            connection_pool=self.client.connection_pool,
-            response_callbacks=self.MODULE_CALLBACKS,
-            transaction=transaction,
-            shard_hint=shard_hint,
-        )
-        return p
-
-
-class Pipeline(TimeSeriesCommands, redis.client.Pipeline):
-    """Pipeline for the module."""
diff --git a/redis/commands/timeseries/commands.py b/redis/commands/timeseries/commands.py
deleted file mode 100644
index 3b9ee0f..0000000
--- a/redis/commands/timeseries/commands.py
+++ /dev/null
@@ -1,775 +0,0 @@
-from redis.exceptions import DataError
-
-
-ADD_CMD = "TS.ADD"
-ALTER_CMD = "TS.ALTER"
-CREATERULE_CMD = "TS.CREATERULE"
-CREATE_CMD = "TS.CREATE"
-DECRBY_CMD = "TS.DECRBY"
-DELETERULE_CMD = "TS.DELETERULE"
-DEL_CMD = "TS.DEL"
-GET_CMD = "TS.GET"
-INCRBY_CMD = "TS.INCRBY"
-INFO_CMD = "TS.INFO"
-MADD_CMD = "TS.MADD"
-MGET_CMD = "TS.MGET"
-MRANGE_CMD = "TS.MRANGE"
-MREVRANGE_CMD = "TS.MREVRANGE"
-QUERYINDEX_CMD = "TS.QUERYINDEX"
-RANGE_CMD = "TS.RANGE"
-REVRANGE_CMD = "TS.REVRANGE"
-
-
-class TimeSeriesCommands:
-    """RedisTimeSeries Commands."""
-
-    def create(self, key, **kwargs):
-        """
-        Create a new time-series.
-        For more information see
-        `TS.CREATE <https://oss.redis.com/redistimeseries/master/commands/#tscreate>`_.  # noqa
-
-        Args:
-
-        key:
-            time-series key
-        retention_msecs:
-            Maximum age for samples compared to last event time (in milliseconds).
-            If None or 0 is passed then  the series is not trimmed at all.
-        uncompressed:
-            Since RedisTimeSeries v1.2, both timestamps and values are
-            compressed by default.
-            Adding this flag will keep data in an uncompressed form.
-            Compression not only saves
-            memory but usually improve performance due to lower number
-            of memory accesses.
-        labels:
-            Set of label-value pairs that represent metadata labels of the key.
-        chunk_size:
-            Each time-serie uses chunks of memory of fixed size for
-            time series samples.
-            You can alter the default TSDB chunk size by passing the
-            chunk_size argument (in Bytes).
-        duplicate_policy:
-            Since RedisTimeSeries v1.4 you can specify the duplicate sample policy
-            ( Configure what to do on duplicate sample. )
-            Can be one of:
-            - 'block': an error will occur for any out of order sample.
-            - 'first': ignore the new value.
-            - 'last': override with latest value.
-            - 'min': only override if the value is lower than the existing value.
-            - 'max': only override if the value is higher than the existing value.
-            When this is not set, the server-wide default will be used.
-        """
-        retention_msecs = kwargs.get("retention_msecs", None)
-        uncompressed = kwargs.get("uncompressed", False)
-        labels = kwargs.get("labels", {})
-        chunk_size = kwargs.get("chunk_size", None)
-        duplicate_policy = kwargs.get("duplicate_policy", None)
-        params = [key]
-        self._appendRetention(params, retention_msecs)
-        self._appendUncompressed(params, uncompressed)
-        self._appendChunkSize(params, chunk_size)
-        self._appendDuplicatePolicy(params, CREATE_CMD, duplicate_policy)
-        self._appendLabels(params, labels)
-
-        return self.execute_command(CREATE_CMD, *params)
-
-    def alter(self, key, **kwargs):
-        """
-        Update the retention, labels of an existing key.
-        For more information see
-        `TS.ALTER <https://oss.redis.com/redistimeseries/master/commands/#tsalter>`_.  # noqa
-
-        The parameters are the same as TS.CREATE.
-        """
-        retention_msecs = kwargs.get("retention_msecs", None)
-        labels = kwargs.get("labels", {})
-        duplicate_policy = kwargs.get("duplicate_policy", None)
-        params = [key]
-        self._appendRetention(params, retention_msecs)
-        self._appendDuplicatePolicy(params, ALTER_CMD, duplicate_policy)
-        self._appendLabels(params, labels)
-
-        return self.execute_command(ALTER_CMD, *params)
-
-    def add(self, key, timestamp, value, **kwargs):
-        """
-        Append (or create and append) a new sample to the series.
-        For more information see
-        `TS.ADD <https://oss.redis.com/redistimeseries/master/commands/#tsadd>`_. # noqa
-
-        Args:
-
-        key:
-            time-series key
-        timestamp:
-            Timestamp of the sample. * can be used for automatic timestamp (using the system clock).
-        value:
-            Numeric data value of the sample
-        retention_msecs:
-            Maximum age for samples compared to last event time (in milliseconds).
-            If None or 0 is passed then  the series is not trimmed at all.
-        uncompressed:
-            Since RedisTimeSeries v1.2, both timestamps and values are compressed by default.
-            Adding this flag will keep data in an uncompressed form. Compression not only saves
-            memory but usually improve performance due to lower number of memory accesses.
-        labels:
-            Set of label-value pairs that represent metadata labels of the key.
-        chunk_size:
-            Each time-serie uses chunks of memory of fixed size for time series samples.
-            You can alter the default TSDB chunk size by passing the chunk_size argument (in Bytes).
-        duplicate_policy:
-            Since RedisTimeSeries v1.4 you can specify the duplicate sample policy
-            (Configure what to do on duplicate sample).
-            Can be one of:
-            - 'block': an error will occur for any out of order sample.
-            - 'first': ignore the new value.
-            - 'last': override with latest value.
-            - 'min': only override if the value is lower than the existing value.
-            - 'max': only override if the value is higher than the existing value.
-            When this is not set, the server-wide default will be used.
-        """
-        retention_msecs = kwargs.get("retention_msecs", None)
-        uncompressed = kwargs.get("uncompressed", False)
-        labels = kwargs.get("labels", {})
-        chunk_size = kwargs.get("chunk_size", None)
-        duplicate_policy = kwargs.get("duplicate_policy", None)
-        params = [key, timestamp, value]
-        self._appendRetention(params, retention_msecs)
-        self._appendUncompressed(params, uncompressed)
-        self._appendChunkSize(params, chunk_size)
-        self._appendDuplicatePolicy(params, ADD_CMD, duplicate_policy)
-        self._appendLabels(params, labels)
-
-        return self.execute_command(ADD_CMD, *params)
-
-    def madd(self, ktv_tuples):
-        """
-        Append (or create and append) a new `value` to series
-        `key` with `timestamp`.
-        Expects a list of `tuples` as (`key`,`timestamp`, `value`).
-        Return value is an array with timestamps of insertions.
-        For more information see
-        `TS.MADD <https://oss.redis.com/redistimeseries/master/commands/#tsmadd>`_. # noqa
-        """
-        params = []
-        for ktv in ktv_tuples:
-            for item in ktv:
-                params.append(item)
-
-        return self.execute_command(MADD_CMD, *params)
-
-    def incrby(self, key, value, **kwargs):
-        """
-        Increment (or create an time-series and increment) the latest
-        sample's of a series.
-        This command can be used as a counter or gauge that automatically gets
-        history as a time series.
-        For more information see
-        `TS.INCRBY <https://oss.redis.com/redistimeseries/master/commands/#tsincrbytsdecrby>`_. # noqa
-
-        Args:
-
-        key:
-            time-series key
-        value:
-            Numeric data value of the sample
-        timestamp:
-            Timestamp of the sample. None can be used for automatic timestamp (using the system clock).
-        retention_msecs:
-            Maximum age for samples compared to last event time (in milliseconds).
-            If None or 0 is passed then  the series is not trimmed at all.
-        uncompressed:
-            Since RedisTimeSeries v1.2, both timestamps and values are compressed by default.
-            Adding this flag will keep data in an uncompressed form. Compression not only saves
-            memory but usually improve performance due to lower number of memory accesses.
-        labels:
-            Set of label-value pairs that represent metadata labels of the key.
-        chunk_size:
-            Each time-series uses chunks of memory of fixed size for time series samples.
-            You can alter the default TSDB chunk size by passing the chunk_size argument (in Bytes).
-        """
-        timestamp = kwargs.get("timestamp", None)
-        retention_msecs = kwargs.get("retention_msecs", None)
-        uncompressed = kwargs.get("uncompressed", False)
-        labels = kwargs.get("labels", {})
-        chunk_size = kwargs.get("chunk_size", None)
-        params = [key, value]
-        self._appendTimestamp(params, timestamp)
-        self._appendRetention(params, retention_msecs)
-        self._appendUncompressed(params, uncompressed)
-        self._appendChunkSize(params, chunk_size)
-        self._appendLabels(params, labels)
-
-        return self.execute_command(INCRBY_CMD, *params)
-
-    def decrby(self, key, value, **kwargs):
-        """
-        Decrement (or create an time-series and decrement) the
-        latest sample's of a series.
-        This command can be used as a counter or gauge that
-        automatically gets history as a time series.
-        For more information see
-        `TS.DECRBY <https://oss.redis.com/redistimeseries/master/commands/#tsincrbytsdecrby>`_. # noqa
-
-        Args:
-
-        key:
-            time-series key
-        value:
-            Numeric data value of the sample
-        timestamp:
-            Timestamp of the sample. None can be used for automatic
-            timestamp (using the system clock).
-        retention_msecs:
-            Maximum age for samples compared to last event time (in milliseconds).
-            If None or 0 is passed then  the series is not trimmed at all.
-        uncompressed:
-            Since RedisTimeSeries v1.2, both timestamps and values are
-            compressed by default.
-            Adding this flag will keep data in an uncompressed form.
-            Compression not only saves
-            memory but usually improve performance due to lower number
-            of memory accesses.
-        labels:
-            Set of label-value pairs that represent metadata labels of the key.
-        chunk_size:
-            Each time-series uses chunks of memory of fixed size for time series samples.
-            You can alter the default TSDB chunk size by passing the chunk_size argument (in Bytes).
-        """
-        timestamp = kwargs.get("timestamp", None)
-        retention_msecs = kwargs.get("retention_msecs", None)
-        uncompressed = kwargs.get("uncompressed", False)
-        labels = kwargs.get("labels", {})
-        chunk_size = kwargs.get("chunk_size", None)
-        params = [key, value]
-        self._appendTimestamp(params, timestamp)
-        self._appendRetention(params, retention_msecs)
-        self._appendUncompressed(params, uncompressed)
-        self._appendChunkSize(params, chunk_size)
-        self._appendLabels(params, labels)
-
-        return self.execute_command(DECRBY_CMD, *params)
-
-    def delete(self, key, from_time, to_time):
-        """
-        Delete data points for a given timeseries and interval range
-        in the form of start and end delete timestamps.
-        The given timestamp interval is closed (inclusive), meaning start
-        and end data points will also be deleted.
-        Return the count for deleted items.
-        For more information see
-        `TS.DEL <https://oss.redis.com/redistimeseries/master/commands/#tsdel>`_. # noqa
-
-        Args:
-
-        key:
-            time-series key.
-        from_time:
-            Start timestamp for the range deletion.
-        to_time:
-            End timestamp for the range deletion.
-        """
-        return self.execute_command(DEL_CMD, key, from_time, to_time)
-
-    def createrule(
-        self,
-        source_key,
-        dest_key,
-        aggregation_type,
-        bucket_size_msec
-    ):
-        """
-        Create a compaction rule from values added to `source_key` into `dest_key`.
-        Aggregating for `bucket_size_msec` where an `aggregation_type` can be
-        [`avg`, `sum`, `min`, `max`, `range`, `count`, `first`, `last`,
-        `std.p`, `std.s`, `var.p`, `var.s`]
-        For more information see
-        `TS.CREATERULE <https://oss.redis.com/redistimeseries/master/commands/#tscreaterule>`_. # noqa
-        """
-        params = [source_key, dest_key]
-        self._appendAggregation(params, aggregation_type, bucket_size_msec)
-
-        return self.execute_command(CREATERULE_CMD, *params)
-
-    def deleterule(self, source_key, dest_key):
-        """
-        Delete a compaction rule.
-        For more information see
-        `TS.DELETERULE <https://oss.redis.com/redistimeseries/master/commands/#tsdeleterule>`_. # noqa
-        """
-        return self.execute_command(DELETERULE_CMD, source_key, dest_key)
-
-    def __range_params(
-        self,
-        key,
-        from_time,
-        to_time,
-        count,
-        aggregation_type,
-        bucket_size_msec,
-        filter_by_ts,
-        filter_by_min_value,
-        filter_by_max_value,
-        align,
-    ):
-        """Create TS.RANGE and TS.REVRANGE arguments."""
-        params = [key, from_time, to_time]
-        self._appendFilerByTs(params, filter_by_ts)
-        self._appendFilerByValue(
-            params,
-            filter_by_min_value,
-            filter_by_max_value
-        )
-        self._appendCount(params, count)
-        self._appendAlign(params, align)
-        self._appendAggregation(params, aggregation_type, bucket_size_msec)
-
-        return params
-
-    def range(
-        self,
-        key,
-        from_time,
-        to_time,
-        count=None,
-        aggregation_type=None,
-        bucket_size_msec=0,
-        filter_by_ts=None,
-        filter_by_min_value=None,
-        filter_by_max_value=None,
-        align=None,
-    ):
-        """
-        Query a range in forward direction for a specific time-serie.
-        For more information see
-        `TS.RANGE <https://oss.redis.com/redistimeseries/master/commands/#tsrangetsrevrange>`_. # noqa
-
-        Args:
-
-        key:
-            Key name for timeseries.
-        from_time:
-            Start timestamp for the range query. - can be used to express
-            the minimum possible timestamp (0).
-        to_time:
-            End timestamp for range query, + can be used to express the
-            maximum possible timestamp.
-        count:
-            Optional maximum number of returned results.
-        aggregation_type:
-            Optional aggregation type. Can be one of
-            [`avg`, `sum`, `min`, `max`, `range`, `count`,
-            `first`, `last`, `std.p`, `std.s`, `var.p`, `var.s`]
-        bucket_size_msec:
-            Time bucket for aggregation in milliseconds.
-        filter_by_ts:
-            List of timestamps to filter the result by specific timestamps.
-        filter_by_min_value:
-            Filter result by minimum value (must mention also filter
-            by_max_value).
-        filter_by_max_value:
-            Filter result by maximum value (must mention also filter
-            by_min_value).
-        align:
-            Timestamp for alignment control for aggregation.
-        """
-        params = self.__range_params(
-            key,
-            from_time,
-            to_time,
-            count,
-            aggregation_type,
-            bucket_size_msec,
-            filter_by_ts,
-            filter_by_min_value,
-            filter_by_max_value,
-            align,
-        )
-        return self.execute_command(RANGE_CMD, *params)
-
-    def revrange(
-        self,
-        key,
-        from_time,
-        to_time,
-        count=None,
-        aggregation_type=None,
-        bucket_size_msec=0,
-        filter_by_ts=None,
-        filter_by_min_value=None,
-        filter_by_max_value=None,
-        align=None,
-    ):
-        """
-        Query a range in reverse direction for a specific time-series.
-        For more information see
-        `TS.REVRANGE <https://oss.redis.com/redistimeseries/master/commands/#tsrangetsrevrange>`_.  # noqa
-
-        **Note**: This command is only available since RedisTimeSeries >= v1.4
-
-        Args:
-
-        key:
-            Key name for timeseries.
-        from_time:
-            Start timestamp for the range query. - can be used to express the minimum possible timestamp (0).
-        to_time:
-            End timestamp for range query, + can be used to express the maximum possible timestamp.
-        count:
-            Optional maximum number of returned results.
-        aggregation_type:
-            Optional aggregation type. Can be one of [`avg`, `sum`, `min`, `max`, `range`, `count`,
-            `first`, `last`, `std.p`, `std.s`, `var.p`, `var.s`]
-        bucket_size_msec:
-            Time bucket for aggregation in milliseconds.
-        filter_by_ts:
-            List of timestamps to filter the result by specific timestamps.
-        filter_by_min_value:
-            Filter result by minimum value (must mention also filter_by_max_value).
-        filter_by_max_value:
-            Filter result by maximum value (must mention also filter_by_min_value).
-        align:
-            Timestamp for alignment control for aggregation.
-        """
-        params = self.__range_params(
-            key,
-            from_time,
-            to_time,
-            count,
-            aggregation_type,
-            bucket_size_msec,
-            filter_by_ts,
-            filter_by_min_value,
-            filter_by_max_value,
-            align,
-        )
-        return self.execute_command(REVRANGE_CMD, *params)
-
-    def __mrange_params(
-        self,
-        aggregation_type,
-        bucket_size_msec,
-        count,
-        filters,
-        from_time,
-        to_time,
-        with_labels,
-        filter_by_ts,
-        filter_by_min_value,
-        filter_by_max_value,
-        groupby,
-        reduce,
-        select_labels,
-        align,
-    ):
-        """Create TS.MRANGE and TS.MREVRANGE arguments."""
-        params = [from_time, to_time]
-        self._appendFilerByTs(params, filter_by_ts)
-        self._appendFilerByValue(
-            params,
-            filter_by_min_value,
-            filter_by_max_value
-        )
-        self._appendCount(params, count)
-        self._appendAlign(params, align)
-        self._appendAggregation(params, aggregation_type, bucket_size_msec)
-        self._appendWithLabels(params, with_labels, select_labels)
-        params.extend(["FILTER"])
-        params += filters
-        self._appendGroupbyReduce(params, groupby, reduce)
-        return params
-
-    def mrange(
-        self,
-        from_time,
-        to_time,
-        filters,
-        count=None,
-        aggregation_type=None,
-        bucket_size_msec=0,
-        with_labels=False,
-        filter_by_ts=None,
-        filter_by_min_value=None,
-        filter_by_max_value=None,
-        groupby=None,
-        reduce=None,
-        select_labels=None,
-        align=None,
-    ):
-        """
-        Query a range across multiple time-series by filters in forward direction.
-        For more information see
-        `TS.MRANGE <https://oss.redis.com/redistimeseries/master/commands/#tsmrangetsmrevrange>`_.  # noqa
-
-        Args:
-
-        from_time:
-            Start timestamp for the range query. `-` can be used to
-            express the minimum possible timestamp (0).
-        to_time:
-            End timestamp for range query, `+` can be used to express
-            the maximum possible timestamp.
-        filters:
-            filter to match the time-series labels.
-        count:
-            Optional maximum number of returned results.
-        aggregation_type:
-            Optional aggregation type. Can be one of
-            [`avg`, `sum`, `min`, `max`, `range`, `count`,
-            `first`, `last`, `std.p`, `std.s`, `var.p`, `var.s`]
-        bucket_size_msec:
-            Time bucket for aggregation in milliseconds.
-        with_labels:
-            Include in the reply the label-value pairs that represent metadata
-            labels of the time-series.
-            If this argument is not set, by default, an empty Array will be
-            replied on the labels array position.
-        filter_by_ts:
-            List of timestamps to filter the result by specific timestamps.
-        filter_by_min_value:
-            Filter result by minimum value (must mention also
-            filter_by_max_value).
-        filter_by_max_value:
-            Filter result by maximum value (must mention also
-            filter_by_min_value).
-        groupby:
-            Grouping by fields the results (must mention also reduce).
-        reduce:
-            Applying reducer functions on each group. Can be one
-            of [`sum`, `min`, `max`].
-        select_labels:
-            Include in the reply only a subset of the key-value
-            pair labels of a series.
-        align:
-            Timestamp for alignment control for aggregation.
-        """
-        params = self.__mrange_params(
-            aggregation_type,
-            bucket_size_msec,
-            count,
-            filters,
-            from_time,
-            to_time,
-            with_labels,
-            filter_by_ts,
-            filter_by_min_value,
-            filter_by_max_value,
-            groupby,
-            reduce,
-            select_labels,
-            align,
-        )
-
-        return self.execute_command(MRANGE_CMD, *params)
-
-    def mrevrange(
-        self,
-        from_time,
-        to_time,
-        filters,
-        count=None,
-        aggregation_type=None,
-        bucket_size_msec=0,
-        with_labels=False,
-        filter_by_ts=None,
-        filter_by_min_value=None,
-        filter_by_max_value=None,
-        groupby=None,
-        reduce=None,
-        select_labels=None,
-        align=None,
-    ):
-        """
-        Query a range across multiple time-series by filters in reverse direction.
-        For more information see
-        `TS.MREVRANGE <https://oss.redis.com/redistimeseries/master/commands/#tsmrangetsmrevrange>`_. # noqa
-
-        Args:
-
-        from_time:
-            Start timestamp for the range query. - can be used to express
-            the minimum possible timestamp (0).
-        to_time:
-            End timestamp for range query, + can be used to express
-            the maximum possible timestamp.
-        filters:
-            Filter to match the time-series labels.
-        count:
-            Optional maximum number of returned results.
-        aggregation_type:
-            Optional aggregation type. Can be one of
-            [`avg`, `sum`, `min`, `max`, `range`, `count`,
-            `first`, `last`, `std.p`, `std.s`, `var.p`, `var.s`]
-        bucket_size_msec:
-            Time bucket for aggregation in milliseconds.
-        with_labels:
-            Include in the reply the label-value pairs that represent
-            metadata labels
-            of the time-series.
-            If this argument is not set, by default, an empty Array
-            will be replied
-            on the labels array position.
-        filter_by_ts:
-            List of timestamps to filter the result by specific timestamps.
-        filter_by_min_value:
-            Filter result by minimum value (must mention also filter
-            by_max_value).
-        filter_by_max_value:
-            Filter result by maximum value (must mention also filter
-            by_min_value).
-        groupby:
-            Grouping by fields the results (must mention also reduce).
-        reduce:
-            Applying reducer functions on each group. Can be one
-            of [`sum`, `min`, `max`].
-        select_labels:
-            Include in the reply only a subset of the key-value pair
-            labels of a series.
-        align:
-            Timestamp for alignment control for aggregation.
-        """
-        params = self.__mrange_params(
-            aggregation_type,
-            bucket_size_msec,
-            count,
-            filters,
-            from_time,
-            to_time,
-            with_labels,
-            filter_by_ts,
-            filter_by_min_value,
-            filter_by_max_value,
-            groupby,
-            reduce,
-            select_labels,
-            align,
-        )
-
-        return self.execute_command(MREVRANGE_CMD, *params)
-
-    def get(self, key):
-        """ # noqa
-        Get the last sample of `key`.
-        For more information see `TS.GET <https://oss.redis.com/redistimeseries/master/commands/#tsget>`_.
-        """
-        return self.execute_command(GET_CMD, key)
-
-    def mget(self, filters, with_labels=False):
-        """ # noqa
-        Get the last samples matching the specific `filter`.
-        For more information see `TS.MGET <https://oss.redis.com/redistimeseries/master/commands/#tsmget>`_.
-        """
-        params = []
-        self._appendWithLabels(params, with_labels)
-        params.extend(["FILTER"])
-        params += filters
-        return self.execute_command(MGET_CMD, *params)
-
-    def info(self, key):
-        """  # noqa
-        Get information of `key`.
-        For more information see `TS.INFO <https://oss.redis.com/redistimeseries/master/commands/#tsinfo>`_.
-        """
-        return self.execute_command(INFO_CMD, key)
-
-    def queryindex(self, filters):
-        """  # noqa
-        Get all the keys matching the `filter` list.
-        For more information see `TS.QUERYINDEX <https://oss.redis.com/redistimeseries/master/commands/#tsqueryindex>`_.
-        """
-        return self.execute_command(QUERYINDEX_CMD, *filters)
-
-    @staticmethod
-    def _appendUncompressed(params, uncompressed):
-        """Append UNCOMPRESSED tag to params."""
-        if uncompressed:
-            params.extend(["UNCOMPRESSED"])
-
-    @staticmethod
-    def _appendWithLabels(params, with_labels, select_labels=None):
-        """Append labels behavior to params."""
-        if with_labels and select_labels:
-            raise DataError(
-                "with_labels and select_labels cannot be provided together."
-            )
-
-        if with_labels:
-            params.extend(["WITHLABELS"])
-        if select_labels:
-            params.extend(["SELECTED_LABELS", *select_labels])
-
-    @staticmethod
-    def _appendGroupbyReduce(params, groupby, reduce):
-        """Append GROUPBY REDUCE property to params."""
-        if groupby is not None and reduce is not None:
-            params.extend(["GROUPBY", groupby, "REDUCE", reduce.upper()])
-
-    @staticmethod
-    def _appendRetention(params, retention):
-        """Append RETENTION property to params."""
-        if retention is not None:
-            params.extend(["RETENTION", retention])
-
-    @staticmethod
-    def _appendLabels(params, labels):
-        """Append LABELS property to params."""
-        if labels:
-            params.append("LABELS")
-            for k, v in labels.items():
-                params.extend([k, v])
-
-    @staticmethod
-    def _appendCount(params, count):
-        """Append COUNT property to params."""
-        if count is not None:
-            params.extend(["COUNT", count])
-
-    @staticmethod
-    def _appendTimestamp(params, timestamp):
-        """Append TIMESTAMP property to params."""
-        if timestamp is not None:
-            params.extend(["TIMESTAMP", timestamp])
-
-    @staticmethod
-    def _appendAlign(params, align):
-        """Append ALIGN property to params."""
-        if align is not None:
-            params.extend(["ALIGN", align])
-
-    @staticmethod
-    def _appendAggregation(params, aggregation_type, bucket_size_msec):
-        """Append AGGREGATION property to params."""
-        if aggregation_type is not None:
-            params.append("AGGREGATION")
-            params.extend([aggregation_type, bucket_size_msec])
-
-    @staticmethod
-    def _appendChunkSize(params, chunk_size):
-        """Append CHUNK_SIZE property to params."""
-        if chunk_size is not None:
-            params.extend(["CHUNK_SIZE", chunk_size])
-
-    @staticmethod
-    def _appendDuplicatePolicy(params, command, duplicate_policy):
-        """Append DUPLICATE_POLICY property to params on CREATE
-        and ON_DUPLICATE on ADD.
-        """
-        if duplicate_policy is not None:
-            if command == "TS.ADD":
-                params.extend(["ON_DUPLICATE", duplicate_policy])
-            else:
-                params.extend(["DUPLICATE_POLICY", duplicate_policy])
-
-    @staticmethod
-    def _appendFilerByTs(params, ts_list):
-        """Append FILTER_BY_TS property to params."""
-        if ts_list is not None:
-            params.extend(["FILTER_BY_TS", *ts_list])
-
-    @staticmethod
-    def _appendFilerByValue(params, min_value, max_value):
-        """Append FILTER_BY_VALUE property to params."""
-        if min_value is not None and max_value is not None:
-            params.extend(["FILTER_BY_VALUE", min_value, max_value])
diff --git a/redis/commands/timeseries/info.py b/redis/commands/timeseries/info.py
deleted file mode 100644
index 3b89503..0000000
--- a/redis/commands/timeseries/info.py
+++ /dev/null
@@ -1,82 +0,0 @@
-from .utils import list_to_dict
-from ..helpers import nativestr
-
-
-class TSInfo(object):
-    """
-    Hold information and statistics on the time-series.
-    Can be created using ``tsinfo`` command
-    https://oss.redis.com/redistimeseries/commands/#tsinfo.
-    """
-
-    rules = []
-    labels = []
-    sourceKey = None
-    chunk_count = None
-    memory_usage = None
-    total_samples = None
-    retention_msecs = None
-    last_time_stamp = None
-    first_time_stamp = None
-
-    max_samples_per_chunk = None
-    chunk_size = None
-    duplicate_policy = None
-
-    def __init__(self, args):
-        """
-        Hold information and statistics on the time-series.
-
-        The supported params that can be passed as args:
-
-        rules:
-            A list of compaction rules of the time series.
-        sourceKey:
-            Key name for source time series in case the current series
-            is a target of a rule.
-        chunkCount:
-            Number of Memory Chunks used for the time series.
-        memoryUsage:
-            Total number of bytes allocated for the time series.
-        totalSamples:
-            Total number of samples in the time series.
-        labels:
-            A list of label-value pairs that represent the metadata
-            labels of the time series.
-        retentionTime:
-            Retention time, in milliseconds, for the time series.
-        lastTimestamp:
-            Last timestamp present in the time series.
-        firstTimestamp:
-            First timestamp present in the time series.
-        maxSamplesPerChunk:
-            Deprecated.
-        chunkSize:
-            Amount of memory, in bytes, allocated for data.
-        duplicatePolicy:
-            Policy that will define handling of duplicate samples.
-
-        Can read more about on
-        https://oss.redis.com/redistimeseries/configuration/#duplicate_policy
-        """
-        response = dict(zip(map(nativestr, args[::2]), args[1::2]))
-        self.rules = response["rules"]
-        self.source_key = response["sourceKey"]
-        self.chunk_count = response["chunkCount"]
-        self.memory_usage = response["memoryUsage"]
-        self.total_samples = response["totalSamples"]
-        self.labels = list_to_dict(response["labels"])
-        self.retention_msecs = response["retentionTime"]
-        self.lastTimeStamp = response["lastTimestamp"]
-        self.first_time_stamp = response["firstTimestamp"]
-        if "maxSamplesPerChunk" in response:
-            self.max_samples_per_chunk = response["maxSamplesPerChunk"]
-            self.chunk_size = (
-                self.max_samples_per_chunk * 16
-            )  # backward compatible changes
-        if "chunkSize" in response:
-            self.chunk_size = response["chunkSize"]
-        if "duplicatePolicy" in response:
-            self.duplicate_policy = response["duplicatePolicy"]
-            if type(self.duplicate_policy) == bytes:
-                self.duplicate_policy = self.duplicate_policy.decode()
diff --git a/redis/commands/timeseries/utils.py b/redis/commands/timeseries/utils.py
deleted file mode 100644
index c33b7c5..0000000
--- a/redis/commands/timeseries/utils.py
+++ /dev/null
@@ -1,49 +0,0 @@
-from ..helpers import nativestr
-
-
-def list_to_dict(aList):
-    return {
-        nativestr(aList[i][0]): nativestr(aList[i][1])
-        for i in range(len(aList))}
-
-
-def parse_range(response):
-    """Parse range response. Used by TS.RANGE and TS.REVRANGE."""
-    return [tuple((r[0], float(r[1]))) for r in response]
-
-
-def parse_m_range(response):
-    """Parse multi range response. Used by TS.MRANGE and TS.MREVRANGE."""
-    res = []
-    for item in response:
-        res.append(
-            {nativestr(item[0]):
-                [list_to_dict(item[1]), parse_range(item[2])]})
-    return sorted(res, key=lambda d: list(d.keys()))
-
-
-def parse_get(response):
-    """Parse get response. Used by TS.GET."""
-    if not response:
-        return None
-    return int(response[0]), float(response[1])
-
-
-def parse_m_get(response):
-    """Parse multi get response. Used by TS.MGET."""
-    res = []
-    for item in response:
-        if not item[2]:
-            res.append(
-                {nativestr(item[0]): [list_to_dict(item[1]), None, None]})
-        else:
-            res.append(
-                {
-                    nativestr(item[0]): [
-                        list_to_dict(item[1]),
-                        int(item[2][0]),
-                        float(item[2][1]),
-                    ]
-                }
-            )
-    return sorted(res, key=lambda d: list(d.keys()))
diff --git a/tests/test_connection.py b/tests/test_connection.py
index 7c44768..0a8e9ad 100644
--- a/tests/test_connection.py
+++ b/tests/test_connection.py
@@ -15,24 +15,3 @@ def test_invalid_response(r):
         with pytest.raises(InvalidResponse) as cm:
             parser.read_response()
     assert str(cm.value) == 'Protocol Error: %r' % raw
-
-
-@skip_if_server_version_lt('4.0.0')
-@pytest.mark.redismod
-def test_loading_external_modules(modclient):
-    def inner():
-        pass
-
-    modclient.load_external_module('myfuncname', inner)
-    assert getattr(modclient, 'myfuncname') == inner
-    assert isinstance(getattr(modclient, 'myfuncname'), types.FunctionType)
-
-    # and call it
-    from redis.commands import RedisModuleCommands
-    j = RedisModuleCommands.json
-    modclient.load_external_module('sometestfuncname', j)
-
-    # d = {'hello': 'world!'}
-    # mod = j(modclient)
-    # mod.set("fookey", ".", d)
-    # assert mod.get('fookey') == d
diff --git a/tests/test_json.py b/tests/test_json.py
deleted file mode 100644
index abc5776..0000000
--- a/tests/test_json.py
+++ /dev/null
@@ -1,1416 +0,0 @@
-import pytest
-import redis
-from redis.commands.json.path import Path
-from redis import exceptions
-from redis.commands.json.decoders import unstring, decode_list
-from .conftest import skip_ifmodversion_lt
-
-
-@pytest.fixture
-def client(modclient):
-    modclient.flushdb()
-    return modclient
-
-
-@pytest.mark.redismod
-def test_json_setbinarykey(client):
-    d = {"hello": "world", b"some": "value"}
-    with pytest.raises(TypeError):
-        client.json().set("somekey", Path.rootPath(), d)
-    assert client.json().set("somekey", Path.rootPath(), d, decode_keys=True)
-
-
-@pytest.mark.redismod
-def test_json_setgetdeleteforget(client):
-    assert client.json().set("foo", Path.rootPath(), "bar")
-    assert client.json().get("foo") == "bar"
-    assert client.json().get("baz") is None
-    assert client.json().delete("foo") == 1
-    assert client.json().forget("foo") == 0  # second delete
-    assert client.exists("foo") == 0
-
-
-@pytest.mark.redismod
-def test_jsonget(client):
-    client.json().set("foo", Path.rootPath(), "bar")
-    assert client.json().get("foo") == "bar"
-
-
-@pytest.mark.redismod
-def test_json_get_jset(client):
-    assert client.json().set("foo", Path.rootPath(), "bar")
-    assert "bar" == client.json().get("foo")
-    assert client.json().get("baz") is None
-    assert 1 == client.json().delete("foo")
-    assert client.exists("foo") == 0
-
-
-@pytest.mark.redismod
-def test_nonascii_setgetdelete(client):
-    assert client.json().set("notascii", Path.rootPath(), "hyvää-élève")
-    assert "hyvää-élève" == client.json().get(
-        "notascii",
-        no_escape=True)
-    assert 1 == client.json().delete("notascii")
-    assert client.exists("notascii") == 0
-
-
-@pytest.mark.redismod
-def test_jsonsetexistentialmodifiersshouldsucceed(client):
-    obj = {"foo": "bar"}
-    assert client.json().set("obj", Path.rootPath(), obj)
-
-    # Test that flags prevent updates when conditions are unmet
-    assert client.json().set("obj", Path("foo"), "baz", nx=True) is None
-    assert client.json().set("obj", Path("qaz"), "baz", xx=True) is None
-
-    # Test that flags allow updates when conditions are met
-    assert client.json().set("obj", Path("foo"), "baz", xx=True)
-    assert client.json().set("obj", Path("qaz"), "baz", nx=True)
-
-    # Test that flags are mutually exlusive
-    with pytest.raises(Exception):
-        client.json().set("obj", Path("foo"), "baz", nx=True, xx=True)
-
-
-@pytest.mark.redismod
-def test_mgetshouldsucceed(client):
-    client.json().set("1", Path.rootPath(), 1)
-    client.json().set("2", Path.rootPath(), 2)
-    assert client.json().mget(["1"], Path.rootPath()) == [1]
-
-    assert client.json().mget([1, 2], Path.rootPath()) == [1, 2]
-
-
-@pytest.mark.redismod
-@skip_ifmodversion_lt("99.99.99", "ReJSON")  # todo: update after the release
-def test_clear(client):
-    client.json().set("arr", Path.rootPath(), [0, 1, 2, 3, 4])
-    assert 1 == client.json().clear("arr", Path.rootPath())
-    assert [] == client.json().get("arr")
-
-
-@pytest.mark.redismod
-def test_type(client):
-    client.json().set("1", Path.rootPath(), 1)
-    assert "integer" == client.json().type("1", Path.rootPath())
-    assert "integer" == client.json().type("1")
-
-
-@pytest.mark.redismod
-def test_numincrby(client):
-    client.json().set("num", Path.rootPath(), 1)
-    assert 2 == client.json().numincrby("num", Path.rootPath(), 1)
-    assert 2.5 == client.json().numincrby("num", Path.rootPath(), 0.5)
-    assert 1.25 == client.json().numincrby("num", Path.rootPath(), -1.25)
-
-
-@pytest.mark.redismod
-def test_nummultby(client):
-    client.json().set("num", Path.rootPath(), 1)
-
-    with pytest.deprecated_call():
-        assert 2 == client.json().nummultby("num", Path.rootPath(), 2)
-        assert 5 == client.json().nummultby("num", Path.rootPath(), 2.5)
-        assert 2.5 == client.json().nummultby("num", Path.rootPath(), 0.5)
-
-
-@pytest.mark.redismod
-@skip_ifmodversion_lt("99.99.99", "ReJSON")  # todo: update after the release
-def test_toggle(client):
-    client.json().set("bool", Path.rootPath(), False)
-    assert client.json().toggle("bool", Path.rootPath())
-    assert client.json().toggle("bool", Path.rootPath()) is False
-    # check non-boolean value
-    client.json().set("num", Path.rootPath(), 1)
-    with pytest.raises(redis.exceptions.ResponseError):
-        client.json().toggle("num", Path.rootPath())
-
-
-@pytest.mark.redismod
-def test_strappend(client):
-    client.json().set("jsonkey", Path.rootPath(), "foo")
-    assert 6 == client.json().strappend("jsonkey", "bar")
-    assert "foobar" == client.json().get("jsonkey", Path.rootPath())
-
-
-@pytest.mark.redismod
-def test_debug(client):
-    client.json().set("str", Path.rootPath(), "foo")
-    assert 24 == client.json().debug("MEMORY", "str", Path.rootPath())
-    assert 24 == client.json().debug("MEMORY", "str")
-
-    # technically help is valid
-    assert isinstance(client.json().debug("HELP"), list)
-
-
-@pytest.mark.redismod
-def test_strlen(client):
-    client.json().set("str", Path.rootPath(), "foo")
-    assert 3 == client.json().strlen("str", Path.rootPath())
-    client.json().strappend("str", "bar", Path.rootPath())
-    assert 6 == client.json().strlen("str", Path.rootPath())
-    assert 6 == client.json().strlen("str")
-
-
-@pytest.mark.redismod
-def test_arrappend(client):
-    client.json().set("arr", Path.rootPath(), [1])
-    assert 2 == client.json().arrappend("arr", Path.rootPath(), 2)
-    assert 4 == client.json().arrappend("arr", Path.rootPath(), 3, 4)
-    assert 7 == client.json().arrappend("arr", Path.rootPath(), *[5, 6, 7])
-
-
-@pytest.mark.redismod
-def test_arrindex(client):
-    client.json().set("arr", Path.rootPath(), [0, 1, 2, 3, 4])
-    assert 1 == client.json().arrindex("arr", Path.rootPath(), 1)
-    assert -1 == client.json().arrindex("arr", Path.rootPath(), 1, 2)
-
-
-@pytest.mark.redismod
-def test_arrinsert(client):
-    client.json().set("arr", Path.rootPath(), [0, 4])
-    assert 5 - -client.json().arrinsert(
-        "arr",
-        Path.rootPath(),
-        1,
-        *[
-            1,
-            2,
-            3,
-        ]
-    )
-    assert [0, 1, 2, 3, 4] == client.json().get("arr")
-
-    # test prepends
-    client.json().set("val2", Path.rootPath(), [5, 6, 7, 8, 9])
-    client.json().arrinsert("val2", Path.rootPath(), 0, ["some", "thing"])
-    assert client.json().get("val2") == [["some", "thing"], 5, 6, 7, 8, 9]
-
-
-@pytest.mark.redismod
-def test_arrlen(client):
-    client.json().set("arr", Path.rootPath(), [0, 1, 2, 3, 4])
-    assert 5 == client.json().arrlen("arr", Path.rootPath())
-    assert 5 == client.json().arrlen("arr")
-    assert client.json().arrlen("fakekey") is None
-
-
-@pytest.mark.redismod
-def test_arrpop(client):
-    client.json().set("arr", Path.rootPath(), [0, 1, 2, 3, 4])
-    assert 4 == client.json().arrpop("arr", Path.rootPath(), 4)
-    assert 3 == client.json().arrpop("arr", Path.rootPath(), -1)
-    assert 2 == client.json().arrpop("arr", Path.rootPath())
-    assert 0 == client.json().arrpop("arr", Path.rootPath(), 0)
-    assert [1] == client.json().get("arr")
-
-    # test out of bounds
-    client.json().set("arr", Path.rootPath(), [0, 1, 2, 3, 4])
-    assert 4 == client.json().arrpop("arr", Path.rootPath(), 99)
-
-    # none test
-    client.json().set("arr", Path.rootPath(), [])
-    assert client.json().arrpop("arr") is None
-
-
-@pytest.mark.redismod
-def test_arrtrim(client):
-    client.json().set("arr", Path.rootPath(), [0, 1, 2, 3, 4])
-    assert 3 == client.json().arrtrim("arr", Path.rootPath(), 1, 3)
-    assert [1, 2, 3] == client.json().get("arr")
-
-    # <0 test, should be 0 equivalent
-    client.json().set("arr", Path.rootPath(), [0, 1, 2, 3, 4])
-    assert 0 == client.json().arrtrim("arr", Path.rootPath(), -1, 3)
-
-    # testing stop > end
-    client.json().set("arr", Path.rootPath(), [0, 1, 2, 3, 4])
-    assert 2 == client.json().arrtrim("arr", Path.rootPath(), 3, 99)
-
-    # start > array size and stop
-    client.json().set("arr", Path.rootPath(), [0, 1, 2, 3, 4])
-    assert 0 == client.json().arrtrim("arr", Path.rootPath(), 9, 1)
-
-    # all larger
-    client.json().set("arr", Path.rootPath(), [0, 1, 2, 3, 4])
-    assert 0 == client.json().arrtrim("arr", Path.rootPath(), 9, 11)
-
-
-@pytest.mark.redismod
-def test_resp(client):
-    obj = {"foo": "bar", "baz": 1, "qaz": True}
-    client.json().set("obj", Path.rootPath(), obj)
-    assert "bar" == client.json().resp("obj", Path("foo"))
-    assert 1 == client.json().resp("obj", Path("baz"))
-    assert client.json().resp("obj", Path("qaz"))
-    assert isinstance(client.json().resp("obj"), list)
-
-
-@pytest.mark.redismod
-def test_objkeys(client):
-    obj = {"foo": "bar", "baz": "qaz"}
-    client.json().set("obj", Path.rootPath(), obj)
-    keys = client.json().objkeys("obj", Path.rootPath())
-    keys.sort()
-    exp = list(obj.keys())
-    exp.sort()
-    assert exp == keys
-
-    client.json().set("obj", Path.rootPath(), obj)
-    keys = client.json().objkeys("obj")
-    assert keys == list(obj.keys())
-
-    assert client.json().objkeys("fakekey") is None
-
-
-@pytest.mark.redismod
-def test_objlen(client):
-    obj = {"foo": "bar", "baz": "qaz"}
-    client.json().set("obj", Path.rootPath(), obj)
-    assert len(obj) == client.json().objlen("obj", Path.rootPath())
-
-    client.json().set("obj", Path.rootPath(), obj)
-    assert len(obj) == client.json().objlen("obj")
-
-
-@pytest.mark.pipeline
-@pytest.mark.redismod
-def test_json_commands_in_pipeline(client):
-    p = client.json().pipeline()
-    p.set("foo", Path.rootPath(), "bar")
-    p.get("foo")
-    p.delete("foo")
-    assert [True, "bar", 1] == p.execute()
-    assert client.keys() == []
-    assert client.get("foo") is None
-
-    # now with a true, json object
-    client.flushdb()
-    p = client.json().pipeline()
-    d = {"hello": "world", "oh": "snap"}
-    p.jsonset("foo", Path.rootPath(), d)
-    p.jsonget("foo")
-    p.exists("notarealkey")
-    p.delete("foo")
-    assert [True, d, 0, 1] == p.execute()
-    assert client.keys() == []
-    assert client.get("foo") is None
-
-
-@pytest.mark.redismod
-def test_json_delete_with_dollar(client):
-    doc1 = {"a": 1, "nested": {"a": 2, "b": 3}}
-    assert client.json().set("doc1", "$", doc1)
-    assert client.json().delete("doc1", "$..a") == 2
-    r = client.json().get("doc1", "$")
-    assert r == [{"nested": {"b": 3}}]
-
-    doc2 = {"a": {"a": 2, "b": 3}, "b": [
-        "a", "b"], "nested": {"b": [True, "a", "b"]}}
-    assert client.json().set("doc2", "$", doc2)
-    assert client.json().delete("doc2", "$..a") == 1
-    res = client.json().get("doc2", "$")
-    assert res == [{"nested": {"b": [True, "a", "b"]}, "b": ["a", "b"]}]
-
-    doc3 = [
-        {
-            "ciao": ["non ancora"],
-            "nested": [
-                {"ciao": [1, "a"]},
-                {"ciao": [2, "a"]},
-                {"ciaoc": [3, "non", "ciao"]},
-                {"ciao": [4, "a"]},
-                {"e": [5, "non", "ciao"]},
-            ],
-        }
-    ]
-    assert client.json().set("doc3", "$", doc3)
-    assert client.json().delete("doc3", '$.[0]["nested"]..ciao') == 3
-
-    doc3val = [
-        [
-            {
-                "ciao": ["non ancora"],
-                "nested": [
-                    {},
-                    {},
-                    {"ciaoc": [3, "non", "ciao"]},
-                    {},
-                    {"e": [5, "non", "ciao"]},
-                ],
-            }
-        ]
-    ]
-    res = client.json().get("doc3", "$")
-    assert res == doc3val
-
-    # Test default path
-    assert client.json().delete("doc3") == 1
-    assert client.json().get("doc3", "$") is None
-
-    client.json().delete("not_a_document", "..a")
-
-
-@pytest.mark.redismod
-def test_json_forget_with_dollar(client):
-    doc1 = {"a": 1, "nested": {"a": 2, "b": 3}}
-    assert client.json().set("doc1", "$", doc1)
-    assert client.json().forget("doc1", "$..a") == 2
-    r = client.json().get("doc1", "$")
-    assert r == [{"nested": {"b": 3}}]
-
-    doc2 = {"a": {"a": 2, "b": 3}, "b": [
-        "a", "b"], "nested": {"b": [True, "a", "b"]}}
-    assert client.json().set("doc2", "$", doc2)
-    assert client.json().forget("doc2", "$..a") == 1
-    res = client.json().get("doc2", "$")
-    assert res == [{"nested": {"b": [True, "a", "b"]}, "b": ["a", "b"]}]
-
-    doc3 = [
-        {
-            "ciao": ["non ancora"],
-            "nested": [
-                {"ciao": [1, "a"]},
-                {"ciao": [2, "a"]},
-                {"ciaoc": [3, "non", "ciao"]},
-                {"ciao": [4, "a"]},
-                {"e": [5, "non", "ciao"]},
-            ],
-        }
-    ]
-    assert client.json().set("doc3", "$", doc3)
-    assert client.json().forget("doc3", '$.[0]["nested"]..ciao') == 3
-
-    doc3val = [
-        [
-            {
-                "ciao": ["non ancora"],
-                "nested": [
-                    {},
-                    {},
-                    {"ciaoc": [3, "non", "ciao"]},
-                    {},
-                    {"e": [5, "non", "ciao"]},
-                ],
-            }
-        ]
-    ]
-    res = client.json().get("doc3", "$")
-    assert res == doc3val
-
-    # Test default path
-    assert client.json().forget("doc3") == 1
-    assert client.json().get("doc3", "$") is None
-
-    client.json().forget("not_a_document", "..a")
-
-
-@pytest.mark.redismod
-def test_json_mget_dollar(client):
-    # Test mget with multi paths
-    client.json().set(
-        "doc1",
-        "$",
-        {"a": 1,
-            "b": 2,
-            "nested": {"a": 3},
-            "c": None, "nested2": {"a": None}},
-    )
-    client.json().set(
-        "doc2",
-        "$",
-        {"a": 4, "b": 5, "nested": {"a": 6},
-            "c": None, "nested2": {"a": [None]}},
-    )
-    # Compare also to single JSON.GET
-    assert client.json().get("doc1", "$..a") == [1, 3, None]
-    assert client.json().get("doc2", "$..a") == [4, 6, [None]]
-
-    # Test mget with single path
-    client.json().mget("doc1", "$..a") == [1, 3, None]
-    # Test mget with multi path
-    client.json().mget(["doc1", "doc2"], "$..a") == [
-        [1, 3, None], [4, 6, [None]]]
-
-    # Test missing key
-    client.json().mget(["doc1", "missing_doc"], "$..a") == [[1, 3, None], None]
-    res = client.json().mget(["missing_doc1", "missing_doc2"], "$..a")
-    assert res == [None, None]
-
-
-@pytest.mark.redismod
-def test_numby_commands_dollar(client):
-
-    # Test NUMINCRBY
-    client.json().set(
-        "doc1",
-        "$", {"a": "b", "b": [{"a": 2}, {"a": 5.0}, {"a": "c"}]})
-    # Test multi
-    assert client.json().numincrby("doc1", "$..a", 2) == \
-        [None, 4, 7.0, None]
-
-    assert client.json().numincrby("doc1", "$..a", 2.5) == \
-        [None, 6.5, 9.5, None]
-    # Test single
-    assert client.json().numincrby("doc1", "$.b[1].a", 2) == [11.5]
-
-    assert client.json().numincrby("doc1", "$.b[2].a", 2) == [None]
-    assert client.json().numincrby("doc1", "$.b[1].a", 3.5) == [15.0]
-
-    # Test NUMMULTBY
-    client.json().set("doc1", "$", {"a": "b", "b": [
-        {"a": 2}, {"a": 5.0}, {"a": "c"}]})
-
-    assert client.json().nummultby("doc1", "$..a", 2) == \
-        [None, 4, 10, None]
-    assert client.json().nummultby("doc1", "$..a", 2.5) == \
-        [None, 10.0, 25.0, None]
-    # Test single
-    assert client.json().nummultby("doc1", "$.b[1].a", 2) == [50.0]
-    assert client.json().nummultby("doc1", "$.b[2].a", 2) == [None]
-    assert client.json().nummultby("doc1", "$.b[1].a", 3) == [150.0]
-
-    # test missing keys
-    with pytest.raises(exceptions.ResponseError):
-        client.json().numincrby("non_existing_doc", "$..a", 2)
-        client.json().nummultby("non_existing_doc", "$..a", 2)
-
-    # Test legacy NUMINCRBY
-    client.json().set("doc1", "$", {"a": "b", "b": [
-        {"a": 2}, {"a": 5.0}, {"a": "c"}]})
-    client.json().numincrby("doc1", ".b[0].a", 3) == 5
-
-    # Test legacy NUMMULTBY
-    client.json().set("doc1", "$", {"a": "b", "b": [
-        {"a": 2}, {"a": 5.0}, {"a": "c"}]})
-    client.json().nummultby("doc1", ".b[0].a", 3) == 6
-
-
-@pytest.mark.redismod
-def test_strappend_dollar(client):
-
-    client.json().set(
-        "doc1", "$", {"a": "foo", "nested1": {
-            "a": "hello"}, "nested2": {"a": 31}}
-    )
-    # Test multi
-    client.json().strappend("doc1", "bar", "$..a") == [6, 8, None]
-
-    client.json().get("doc1", "$") == [
-        {"a": "foobar", "nested1": {"a": "hellobar"}, "nested2": {"a": 31}}
-    ]
-    # Test single
-    client.json().strappend("doc1", "baz", "$.nested1.a") == [11]
-
-    client.json().get("doc1", "$") == [
-        {"a": "foobar", "nested1": {"a": "hellobarbaz"}, "nested2": {"a": 31}}
-    ]
-
-    # Test missing key
-    with pytest.raises(exceptions.ResponseError):
-        client.json().strappend("non_existing_doc", "$..a", "err")
-
-    # Test multi
-    client.json().strappend("doc1", "bar", ".*.a") == 8
-    client.json().get("doc1", "$") == [
-        {"a": "foo", "nested1": {"a": "hellobar"}, "nested2": {"a": 31}}
-    ]
-
-    # Test missing path
-    with pytest.raises(exceptions.ResponseError):
-        client.json().strappend("doc1", "piu")
-
-
-@pytest.mark.redismod
-def test_strlen_dollar(client):
-
-    # Test multi
-    client.json().set(
-        "doc1", "$", {"a": "foo", "nested1": {
-            "a": "hello"}, "nested2": {"a": 31}}
-    )
-    assert client.json().strlen("doc1", "$..a") == [3, 5, None]
-
-    res2 = client.json().strappend("doc1", "bar", "$..a")
-    res1 = client.json().strlen("doc1", "$..a")
-    assert res1 == res2
-
-    # Test single
-    client.json().strlen("doc1", "$.nested1.a") == [8]
-    client.json().strlen("doc1", "$.nested2.a") == [None]
-
-    # Test missing key
-    with pytest.raises(exceptions.ResponseError):
-        client.json().strlen("non_existing_doc", "$..a")
-
-
-@pytest.mark.redismod
-def test_arrappend_dollar(client):
-    client.json().set(
-        "doc1",
-        "$",
-        {
-            "a": ["foo"],
-            "nested1": {"a": ["hello", None, "world"]},
-            "nested2": {"a": 31},
-        },
-    )
-    # Test multi
-    client.json().arrappend("doc1", "$..a", "bar", "racuda") == [3, 5, None]
-    assert client.json().get("doc1", "$") == [
-        {
-            "a": ["foo", "bar", "racuda"],
-            "nested1": {"a": ["hello", None, "world", "bar", "racuda"]},
-            "nested2": {"a": 31},
-        }
-    ]
-
-    # Test single
-    assert client.json().arrappend("doc1", "$.nested1.a", "baz") == [6]
-    assert client.json().get("doc1", "$") == [
-        {
-            "a": ["foo", "bar", "racuda"],
-            "nested1": {"a": ["hello", None, "world", "bar", "racuda", "baz"]},
-            "nested2": {"a": 31},
-        }
-    ]
-
-    # Test missing key
-    with pytest.raises(exceptions.ResponseError):
-        client.json().arrappend("non_existing_doc", "$..a")
-
-    # Test legacy
-    client.json().set(
-        "doc1",
-        "$",
-        {
-            "a": ["foo"],
-            "nested1": {"a": ["hello", None, "world"]},
-            "nested2": {"a": 31},
-        },
-    )
-    # Test multi (all paths are updated, but return result of last path)
-    assert client.json().arrappend("doc1", "..a", "bar", "racuda") == 5
-
-    assert client.json().get("doc1", "$") == [
-        {
-            "a": ["foo", "bar", "racuda"],
-            "nested1": {"a": ["hello", None, "world", "bar", "racuda"]},
-            "nested2": {"a": 31},
-        }
-    ]
-    # Test single
-    assert client.json().arrappend("doc1", ".nested1.a", "baz") == 6
-    assert client.json().get("doc1", "$") == [
-        {
-            "a": ["foo", "bar", "racuda"],
-            "nested1": {"a": ["hello", None, "world", "bar", "racuda", "baz"]},
-            "nested2": {"a": 31},
-        }
-    ]
-
-    # Test missing key
-    with pytest.raises(exceptions.ResponseError):
-        client.json().arrappend("non_existing_doc", "$..a")
-
-
-@pytest.mark.redismod
-def test_arrinsert_dollar(client):
-    client.json().set(
-        "doc1",
-        "$",
-        {
-            "a": ["foo"],
-            "nested1": {"a": ["hello", None, "world"]},
-            "nested2": {"a": 31},
-        },
-    )
-    # Test multi
-    assert client.json().arrinsert("doc1", "$..a", "1",
-                                   "bar", "racuda") == [3, 5, None]
-
-    assert client.json().get("doc1", "$") == [
-        {
-            "a": ["foo", "bar", "racuda"],
-            "nested1": {"a": ["hello", "bar", "racuda", None, "world"]},
-            "nested2": {"a": 31},
-        }
-    ]
-    # Test single
-    assert client.json().arrinsert("doc1", "$.nested1.a", -2, "baz") == [6]
-    assert client.json().get("doc1", "$") == [
-        {
-            "a": ["foo", "bar", "racuda"],
-            "nested1": {"a": ["hello", "bar", "racuda", "baz", None, "world"]},
-            "nested2": {"a": 31},
-        }
-    ]
-
-    # Test missing key
-    with pytest.raises(exceptions.ResponseError):
-        client.json().arrappend("non_existing_doc", "$..a")
-
-
-@pytest.mark.redismod
-def test_arrlen_dollar(client):
-
-    client.json().set(
-        "doc1",
-        "$",
-        {
-            "a": ["foo"],
-            "nested1": {"a": ["hello", None, "world"]},
-            "nested2": {"a": 31},
-        },
-    )
-
-    # Test multi
-    assert client.json().arrlen("doc1", "$..a") == [1, 3, None]
-    assert client.json().arrappend("doc1", "$..a", "non", "abba", "stanza") \
-        == [4, 6, None]
-
-    client.json().clear("doc1", "$.a")
-    assert client.json().arrlen("doc1", "$..a") == [0, 6, None]
-    # Test single
-    assert client.json().arrlen("doc1", "$.nested1.a") == [6]
-
-    # Test missing key
-    with pytest.raises(exceptions.ResponseError):
-        client.json().arrappend("non_existing_doc", "$..a")
-
-    client.json().set(
-        "doc1",
-        "$",
-        {
-            "a": ["foo"],
-            "nested1": {"a": ["hello", None, "world"]},
-            "nested2": {"a": 31},
-        },
-    )
-    # Test multi (return result of last path)
-    assert client.json().arrlen("doc1", "$..a") == [1, 3, None]
-    assert client.json().arrappend("doc1", "..a", "non", "abba", "stanza") == 6
-
-    # Test single
-    assert client.json().arrlen("doc1", ".nested1.a") == 6
-
-    # Test missing key
-    assert client.json().arrlen("non_existing_doc", "..a") is None
-
-
-@pytest.mark.redismod
-def test_arrpop_dollar(client):
-    client.json().set(
-        "doc1",
-        "$",
-        {
-            "a": ["foo"],
-            "nested1": {"a": ["hello", None, "world"]},
-            "nested2": {"a": 31},
-        },
-    )
-
-    # # # Test multi
-    assert client.json().arrpop("doc1", "$..a", 1) == ['"foo"', None, None]
-
-    assert client.json().get("doc1", "$") == [
-        {"a": [], "nested1": {"a": ["hello", "world"]}, "nested2": {"a": 31}}
-    ]
-
-    # Test missing key
-    with pytest.raises(exceptions.ResponseError):
-        client.json().arrpop("non_existing_doc", "..a")
-
-    # # Test legacy
-    client.json().set(
-        "doc1",
-        "$",
-        {
-            "a": ["foo"],
-            "nested1": {"a": ["hello", None, "world"]},
-            "nested2": {"a": 31},
-        },
-    )
-    # Test multi (all paths are updated, but return result of last path)
-    client.json().arrpop("doc1", "..a", "1") is None
-    assert client.json().get("doc1", "$") == [
-        {"a": [], "nested1": {"a": ["hello", "world"]}, "nested2": {"a": 31}}
-    ]
-
-    # # Test missing key
-    with pytest.raises(exceptions.ResponseError):
-        client.json().arrpop("non_existing_doc", "..a")
-
-
-@pytest.mark.redismod
-def test_arrtrim_dollar(client):
-
-    client.json().set(
-        "doc1",
-        "$",
-        {
-            "a": ["foo"],
-            "nested1": {"a": ["hello", None, "world"]},
-            "nested2": {"a": 31},
-        },
-    )
-    # Test multi
-    assert client.json().arrtrim("doc1", "$..a", "1", -1) == [0, 2, None]
-    assert client.json().get("doc1", "$") == [
-        {"a": [], "nested1": {"a": [None, "world"]}, "nested2": {"a": 31}}
-    ]
-
-    assert client.json().arrtrim("doc1", "$..a", "1", "1") == [0, 1, None]
-    assert client.json().get("doc1", "$") == [
-        {"a": [], "nested1": {"a": ["world"]}, "nested2": {"a": 31}}
-    ]
-    # Test single
-    assert client.json().arrtrim("doc1", "$.nested1.a", 1, 0) == [0]
-    assert client.json().get("doc1", "$") == [
-        {"a": [], "nested1": {"a": []}, "nested2": {"a": 31}}
-    ]
-
-    # Test missing key
-    with pytest.raises(exceptions.ResponseError):
-        client.json().arrtrim("non_existing_doc", "..a", "0", 1)
-
-    # Test legacy
-    client.json().set(
-        "doc1",
-        "$",
-        {
-            "a": ["foo"],
-            "nested1": {"a": ["hello", None, "world"]},
-            "nested2": {"a": 31},
-        },
-    )
-
-    # Test multi (all paths are updated, but return result of last path)
-    assert client.json().arrtrim("doc1", "..a", "1", "-1") == 2
-
-    # Test single
-    assert client.json().arrtrim("doc1", ".nested1.a", "1", "1") == 1
-    assert client.json().get("doc1", "$") == [
-        {"a": [], "nested1": {"a": ["world"]}, "nested2": {"a": 31}}
-    ]
-
-    # Test missing key
-    with pytest.raises(exceptions.ResponseError):
-        client.json().arrtrim("non_existing_doc", "..a", 1, 1)
-
-
-@pytest.mark.redismod
-def test_objkeys_dollar(client):
-    client.json().set(
-        "doc1",
-        "$",
-        {
-            "nested1": {"a": {"foo": 10, "bar": 20}},
-            "a": ["foo"],
-            "nested2": {"a": {"baz": 50}},
-        },
-    )
-
-    # Test single
-    assert client.json().objkeys("doc1", "$.nested1.a") == [["foo", "bar"]]
-
-    # Test legacy
-    assert client.json().objkeys("doc1", ".*.a") == ["foo", "bar"]
-    # Test single
-    assert client.json().objkeys("doc1", ".nested2.a") == ["baz"]
-
-    # Test missing key
-    assert client.json().objkeys("non_existing_doc", "..a") is None
-
-    # Test missing key
-    with pytest.raises(exceptions.ResponseError):
-        client.json().objkeys("doc1", "$.nowhere")
-
-
-@pytest.mark.redismod
-def test_objlen_dollar(client):
-    client.json().set(
-        "doc1",
-        "$",
-        {
-            "nested1": {"a": {"foo": 10, "bar": 20}},
-            "a": ["foo"],
-            "nested2": {"a": {"baz": 50}},
-        },
-    )
-    # Test multi
-    assert client.json().objlen("doc1", "$..a") == [2, None, 1]
-    # Test single
-    assert client.json().objlen("doc1", "$.nested1.a") == [2]
-
-    # Test missing key
-    assert client.json().objlen("non_existing_doc", "$..a") is None
-
-    # Test missing path
-    with pytest.raises(exceptions.ResponseError):
-        client.json().objlen("doc1", "$.nowhere")
-
-    # Test legacy
-    assert client.json().objlen("doc1", ".*.a") == 2
-
-    # Test single
-    assert client.json().objlen("doc1", ".nested2.a") == 1
-
-    # Test missing key
-    assert client.json().objlen("non_existing_doc", "..a") is None
-
-    # Test missing path
-    with pytest.raises(exceptions.ResponseError):
-        client.json().objlen("doc1", ".nowhere")
-
-
-@pytest.mark.redismod
-def load_types_data(nested_key_name):
-    td = {
-        "object": {},
-        "array": [],
-        "string": "str",
-        "integer": 42,
-        "number": 1.2,
-        "boolean": False,
-        "null": None,
-    }
-    jdata = {}
-    types = []
-    for i, (k, v) in zip(range(1, len(td) + 1), iter(td.items())):
-        jdata["nested" + str(i)] = {nested_key_name: v}
-        types.append(k)
-
-    return jdata, types
-
-
-@pytest.mark.redismod
-def test_type_dollar(client):
-    jdata, jtypes = load_types_data("a")
-    client.json().set("doc1", "$", jdata)
-    # Test multi
-    assert client.json().type("doc1", "$..a") == jtypes
-
-    # Test single
-    assert client.json().type("doc1", "$.nested2.a") == [jtypes[1]]
-
-    # Test missing key
-    assert client.json().type("non_existing_doc", "..a") is None
-
-
-@pytest.mark.redismod
-def test_clear_dollar(client):
-
-    client.json().set(
-        "doc1",
-        "$",
-        {
-            "nested1": {"a": {"foo": 10, "bar": 20}},
-            "a": ["foo"],
-            "nested2": {"a": "claro"},
-            "nested3": {"a": {"baz": 50}},
-        },
-    )
-    # Test multi
-    assert client.json().clear("doc1", "$..a") == 3
-
-    assert client.json().get("doc1", "$") == [
-        {"nested1": {"a": {}}, "a": [], "nested2": {
-            "a": "claro"}, "nested3": {"a": {}}}
-    ]
-
-    # Test single
-    client.json().set(
-        "doc1",
-        "$",
-        {
-            "nested1": {"a": {"foo": 10, "bar": 20}},
-            "a": ["foo"],
-            "nested2": {"a": "claro"},
-            "nested3": {"a": {"baz": 50}},
-        },
-    )
-    assert client.json().clear("doc1", "$.nested1.a") == 1
-    assert client.json().get("doc1", "$") == [
-        {
-            "nested1": {"a": {}},
-            "a": ["foo"],
-            "nested2": {"a": "claro"},
-            "nested3": {"a": {"baz": 50}},
-        }
-    ]
-
-    # Test missing path (defaults to root)
-    assert client.json().clear("doc1") == 1
-    assert client.json().get("doc1", "$") == [{}]
-
-    # Test missing key
-    with pytest.raises(exceptions.ResponseError):
-        client.json().clear("non_existing_doc", "$..a")
-
-
-@pytest.mark.redismod
-def test_toggle_dollar(client):
-    client.json().set(
-        "doc1",
-        "$",
-        {
-            "a": ["foo"],
-            "nested1": {"a": False},
-            "nested2": {"a": 31},
-            "nested3": {"a": True},
-        },
-    )
-    # Test multi
-    assert client.json().toggle("doc1", "$..a") == [None, 1, None, 0]
-    assert client.json().get("doc1", "$") == [
-        {
-            "a": ["foo"],
-            "nested1": {"a": True},
-            "nested2": {"a": 31},
-            "nested3": {"a": False},
-        }
-    ]
-
-    # Test missing key
-    with pytest.raises(exceptions.ResponseError):
-        client.json().toggle("non_existing_doc", "$..a")
-
-
-@pytest.mark.redismod
-def test_debug_dollar(client):
-
-    jdata, jtypes = load_types_data("a")
-
-    client.json().set("doc1", "$", jdata)
-
-    # Test multi
-    assert client.json().debug("MEMORY", "doc1", "$..a") == [
-        72, 24, 24, 16, 16, 1, 0]
-
-    # Test single
-    assert client.json().debug("MEMORY", "doc1", "$.nested2.a") == [24]
-
-    # Test legacy
-    assert client.json().debug("MEMORY", "doc1", "..a") == 72
-
-    # Test missing path (defaults to root)
-    assert client.json().debug("MEMORY", "doc1") == 72
-
-    # Test missing key
-    assert client.json().debug("MEMORY", "non_existing_doc", "$..a") == []
-
-
-@pytest.mark.redismod
-def test_resp_dollar(client):
-
-    data = {
-        "L1": {
-            "a": {
-                "A1_B1": 10,
-                "A1_B2": False,
-                "A1_B3": {
-                    "A1_B3_C1": None,
-                    "A1_B3_C2": [
-                        "A1_B3_C2_D1_1",
-                        "A1_B3_C2_D1_2",
-                        -19.5,
-                        "A1_B3_C2_D1_4",
-                        "A1_B3_C2_D1_5",
-                        {"A1_B3_C2_D1_6_E1": True},
-                    ],
-                    "A1_B3_C3": [1],
-                },
-                "A1_B4": {
-                    "A1_B4_C1": "foo",
-                },
-            },
-        },
-        "L2": {
-            "a": {
-                "A2_B1": 20,
-                "A2_B2": False,
-                "A2_B3": {
-                    "A2_B3_C1": None,
-                    "A2_B3_C2": [
-                        "A2_B3_C2_D1_1",
-                        "A2_B3_C2_D1_2",
-                        -37.5,
-                        "A2_B3_C2_D1_4",
-                        "A2_B3_C2_D1_5",
-                        {"A2_B3_C2_D1_6_E1": False},
-                    ],
-                    "A2_B3_C3": [2],
-                },
-                "A2_B4": {
-                    "A2_B4_C1": "bar",
-                },
-            },
-        },
-    }
-    client.json().set("doc1", "$", data)
-    # Test multi
-    res = client.json().resp("doc1", "$..a")
-    assert res == [
-        [
-            "{",
-            "A1_B1",
-            10,
-            "A1_B2",
-            "false",
-            "A1_B3",
-            [
-                "{",
-                "A1_B3_C1",
-                None,
-                "A1_B3_C2",
-                [
-                    "[",
-                    "A1_B3_C2_D1_1",
-                    "A1_B3_C2_D1_2",
-                    "-19.5",
-                    "A1_B3_C2_D1_4",
-                    "A1_B3_C2_D1_5",
-                    ["{", "A1_B3_C2_D1_6_E1", "true"],
-                ],
-                "A1_B3_C3",
-                ["[", 1],
-            ],
-            "A1_B4",
-            ["{", "A1_B4_C1", "foo"],
-        ],
-        [
-            "{",
-            "A2_B1",
-            20,
-            "A2_B2",
-            "false",
-            "A2_B3",
-            [
-                "{",
-                "A2_B3_C1",
-                None,
-                "A2_B3_C2",
-                [
-                    "[",
-                    "A2_B3_C2_D1_1",
-                    "A2_B3_C2_D1_2",
-                    "-37.5",
-                    "A2_B3_C2_D1_4",
-                    "A2_B3_C2_D1_5",
-                    ["{", "A2_B3_C2_D1_6_E1", "false"],
-                ],
-                "A2_B3_C3",
-                ["[", 2],
-            ],
-            "A2_B4",
-            ["{", "A2_B4_C1", "bar"],
-        ],
-    ]
-
-    # Test single
-    resSingle = client.json().resp("doc1", "$.L1.a")
-    assert resSingle == [
-        [
-            "{",
-            "A1_B1",
-            10,
-            "A1_B2",
-            "false",
-            "A1_B3",
-            [
-                "{",
-                "A1_B3_C1",
-                None,
-                "A1_B3_C2",
-                [
-                    "[",
-                    "A1_B3_C2_D1_1",
-                    "A1_B3_C2_D1_2",
-                    "-19.5",
-                    "A1_B3_C2_D1_4",
-                    "A1_B3_C2_D1_5",
-                    ["{", "A1_B3_C2_D1_6_E1", "true"],
-                ],
-                "A1_B3_C3",
-                ["[", 1],
-            ],
-            "A1_B4",
-            ["{", "A1_B4_C1", "foo"],
-        ]
-    ]
-
-    # Test missing path
-    with pytest.raises(exceptions.ResponseError):
-        client.json().resp("doc1", "$.nowhere")
-
-    # Test missing key
-    assert client.json().resp("non_existing_doc", "$..a") is None
-
-
-@pytest.mark.redismod
-def test_arrindex_dollar(client):
-
-    client.json().set(
-        "store",
-        "$",
-        {
-            "store": {
-                "book": [
-                    {
-                        "category": "reference",
-                        "author": "Nigel Rees",
-                        "title": "Sayings of the Century",
-                        "price": 8.95,
-                        "size": [10, 20, 30, 40],
-                    },
-                    {
-                        "category": "fiction",
-                        "author": "Evelyn Waugh",
-                        "title": "Sword of Honour",
-                        "price": 12.99,
-                        "size": [50, 60, 70, 80],
-                    },
-                    {
-                        "category": "fiction",
-                        "author": "Herman Melville",
-                        "title": "Moby Dick",
-                        "isbn": "0-553-21311-3",
-                        "price": 8.99,
-                        "size": [5, 10, 20, 30],
-                    },
-                    {
-                        "category": "fiction",
-                        "author": "J. R. R. Tolkien",
-                        "title": "The Lord of the Rings",
-                        "isbn": "0-395-19395-8",
-                        "price": 22.99,
-                        "size": [5, 6, 7, 8],
-                    },
-                ],
-                "bicycle": {"color": "red", "price": 19.95},
-            }
-        },
-    )
-
-    assert client.json().get("store", "$.store.book[?(@.price<10)].size") == [
-        [10, 20, 30, 40],
-        [5, 10, 20, 30],
-    ]
-    assert client.json().arrindex(
-        "store", "$.store.book[?(@.price<10)].size", "20"
-    ) == [-1, -1]
-
-    # Test index of int scalar in multi values
-    client.json().set(
-        "test_num",
-        ".",
-        [
-            {"arr": [0, 1, 3.0, 3, 2, 1, 0, 3]},
-            {"nested1_found": {"arr": [5, 4, 3, 2, 1, 0, 1, 2, 3.0, 2, 4, 5]}},
-            {"nested2_not_found": {"arr": [2, 4, 6]}},
-            {"nested3_scalar": {"arr": "3"}},
-            [
-                {"nested41_not_arr": {"arr_renamed": [1, 2, 3]}},
-                {"nested42_empty_arr": {"arr": []}},
-            ],
-        ],
-    )
-
-    assert client.json().get("test_num", "$..arr") == [
-        [0, 1, 3.0, 3, 2, 1, 0, 3],
-        [5, 4, 3, 2, 1, 0, 1, 2, 3.0, 2, 4, 5],
-        [2, 4, 6],
-        "3",
-        [],
-    ]
-
-    assert client.json().arrindex("test_num", "$..arr", 3) == [
-        3, 2, -1, None, -1]
-
-    # Test index of double scalar in multi values
-    assert client.json().arrindex("test_num", "$..arr", 3.0) == [
-        2, 8, -1, None, -1]
-
-    # Test index of string scalar in multi values
-    client.json().set(
-        "test_string",
-        ".",
-        [
-            {"arr": ["bazzz", "bar", 2, "baz", 2, "ba", "baz", 3]},
-            {
-                "nested1_found": {
-                    "arr": [
-                        None,
-                        "baz2",
-                        "buzz", 2, 1, 0, 1, "2", "baz", 2, 4, 5]
-                }
-            },
-            {"nested2_not_found": {"arr": ["baz2", 4, 6]}},
-            {"nested3_scalar": {"arr": "3"}},
-            [
-                {"nested41_arr": {"arr_renamed": [1, "baz", 3]}},
-                {"nested42_empty_arr": {"arr": []}},
-            ],
-        ],
-    )
-    assert client.json().get("test_string", "$..arr") == [
-        ["bazzz", "bar", 2, "baz", 2, "ba", "baz", 3],
-        [None, "baz2", "buzz", 2, 1, 0, 1, "2", "baz", 2, 4, 5],
-        ["baz2", 4, 6],
-        "3",
-        [],
-    ]
-
-    assert client.json().arrindex("test_string", "$..arr", "baz") == [
-        3,
-        8,
-        -1,
-        None,
-        -1,
-    ]
-
-    assert client.json().arrindex("test_string", "$..arr", "baz", 2) == [
-        3,
-        8,
-        -1,
-        None,
-        -1,
-    ]
-    assert client.json().arrindex("test_string", "$..arr", "baz", 4) == [
-        6,
-        8,
-        -1,
-        None,
-        -1,
-    ]
-    assert client.json().arrindex("test_string", "$..arr", "baz", -5) == [
-        3,
-        8,
-        -1,
-        None,
-        -1,
-    ]
-    assert client.json().arrindex("test_string", "$..arr", "baz", 4, 7) == [
-        6,
-        -1,
-        -1,
-        None,
-        -1,
-    ]
-    assert client.json().arrindex("test_string", "$..arr", "baz", 4, -1) == [
-        6,
-        8,
-        -1,
-        None,
-        -1,
-    ]
-    assert client.json().arrindex("test_string", "$..arr", "baz", 4, 0) == [
-        6,
-        8,
-        -1,
-        None,
-        -1,
-    ]
-    assert client.json().arrindex("test_string", "$..arr", "5", 7, -1) == [
-        -1,
-        -1,
-        -1,
-        None,
-        -1,
-    ]
-    assert client.json().arrindex("test_string", "$..arr", "5", 7, 0) == [
-        -1,
-        -1,
-        -1,
-        None,
-        -1,
-    ]
-
-    # Test index of None scalar in multi values
-    client.json().set(
-        "test_None",
-        ".",
-        [
-            {"arr": ["bazzz", "None", 2, None, 2, "ba", "baz", 3]},
-            {
-                "nested1_found": {
-                    "arr": [
-                        "zaz",
-                        "baz2",
-                        "buzz",
-                        2, 1, 0, 1, "2", None, 2, 4, 5]
-                }
-            },
-            {"nested2_not_found": {"arr": ["None", 4, 6]}},
-            {"nested3_scalar": {"arr": None}},
-            [
-                {"nested41_arr": {"arr_renamed": [1, None, 3]}},
-                {"nested42_empty_arr": {"arr": []}},
-            ],
-        ],
-    )
-    assert client.json().get("test_None", "$..arr") == [
-        ["bazzz", "None", 2, None, 2, "ba", "baz", 3],
-        ["zaz", "baz2", "buzz", 2, 1, 0, 1, "2", None, 2, 4, 5],
-        ["None", 4, 6],
-        None,
-        [],
-    ]
-
-    # Fail with none-scalar value
-    with pytest.raises(exceptions.ResponseError):
-        client.json().arrindex(
-            "test_None", "$..nested42_empty_arr.arr", {"arr": []})
-
-    # Do not fail with none-scalar value in legacy mode
-    assert (
-        client.json().arrindex(
-            "test_None", ".[4][1].nested42_empty_arr.arr", '{"arr":[]}'
-        )
-        == -1
-    )
-
-    # Test legacy (path begins with dot)
-    # Test index of int scalar in single value
-    assert client.json().arrindex("test_num", ".[0].arr", 3) == 3
-    assert client.json().arrindex("test_num", ".[0].arr", 9) == -1
-
-    with pytest.raises(exceptions.ResponseError):
-        client.json().arrindex("test_num", ".[0].arr_not", 3)
-    # Test index of string scalar in single value
-    assert client.json().arrindex("test_string", ".[0].arr", "baz") == 3
-    assert client.json().arrindex("test_string", ".[0].arr", "faz") == -1
-    # Test index of None scalar in single value
-    assert client.json().arrindex("test_None", ".[0].arr", "None") == 1
-    assert client.json().arrindex(
-        "test_None",
-        "..nested2_not_found.arr",
-        "None") == 0
-
-
-def test_decoders_and_unstring():
-    assert unstring("4") == 4
-    assert unstring("45.55") == 45.55
-    assert unstring("hello world") == "hello world"
-
-    assert decode_list(b"45.55") == 45.55
-    assert decode_list("45.55") == 45.55
-    assert decode_list(['hello', b'world']) == ['hello', 'world']
-
-
-@pytest.mark.redismod
-def test_custom_decoder(client):
-    import ujson
-    import json
-
-    cj = client.json(encoder=ujson, decoder=ujson)
-    assert cj.set("foo", Path.rootPath(), "bar")
-    assert "bar" == cj.get("foo")
-    assert cj.get("baz") is None
-    assert 1 == cj.delete("foo")
-    assert client.exists("foo") == 0
-    assert not isinstance(cj.__encoder__, json.JSONEncoder)
-    assert not isinstance(cj.__decoder__, json.JSONDecoder)
diff --git a/tests/test_search.py b/tests/test_search.py
deleted file mode 100644
index d1fc75f..0000000
--- a/tests/test_search.py
+++ /dev/null
@@ -1,1315 +0,0 @@
-import pytest
-import redis
-import bz2
-import csv
-import time
-import os
-
-from io import TextIOWrapper
-from .conftest import skip_ifmodversion_lt, default_redismod_url
-from redis import Redis
-
-import redis.commands.search
-from redis.commands.json.path import Path
-from redis.commands.search import Search
-from redis.commands.search.field import (
-    GeoField,
-    NumericField,
-    TagField,
-    TextField
-)
-from redis.commands.search.query import (
-    GeoFilter,
-    NumericFilter,
-    Query
-)
-from redis.commands.search.result import Result
-from redis.commands.search.indexDefinition import IndexDefinition, IndexType
-from redis.commands.search.suggestion import Suggestion
-import redis.commands.search.aggregation as aggregations
-import redis.commands.search.reducers as reducers
-
-WILL_PLAY_TEXT = (
-    os.path.abspath(
-        os.path.join(
-            os.path.dirname(__file__),
-            "testdata",
-            "will_play_text.csv.bz2"
-        )
-    )
-)
-
-TITLES_CSV = (
-    os.path.abspath(
-        os.path.join(
-            os.path.dirname(__file__),
-            "testdata",
-            "titles.csv"
-        )
-    )
-)
-
-
-def waitForIndex(env, idx, timeout=None):
-    delay = 0.1
-    while True:
-        res = env.execute_command("ft.info", idx)
-        try:
-            res.index("indexing")
-        except ValueError:
-            break
-
-        if int(res[res.index("indexing") + 1]) == 0:
-            break
-
-        time.sleep(delay)
-        if timeout is not None:
-            timeout -= delay
-            if timeout <= 0:
-                break
-
-
-def getClient():
-    """
-    Gets a client client attached to an index name which is ready to be
-    created
-    """
-    rc = Redis.from_url(default_redismod_url, decode_responses=True)
-    return rc
-
-
-def createIndex(client, num_docs=100, definition=None):
-    try:
-        client.create_index(
-            (TextField("play", weight=5.0),
-                TextField("txt"),
-                NumericField("chapter")),
-            definition=definition,
-        )
-    except redis.ResponseError:
-        client.dropindex(delete_documents=True)
-        return createIndex(client, num_docs=num_docs, definition=definition)
-
-    chapters = {}
-    bzfp = TextIOWrapper(bz2.BZ2File(WILL_PLAY_TEXT), encoding="utf8")
-
-    r = csv.reader(bzfp, delimiter=";")
-    for n, line in enumerate(r):
-
-        play, chapter, _, text = \
-            line[1], line[2], line[4], line[5]
-
-        key = "{}:{}".format(play, chapter).lower()
-        d = chapters.setdefault(key, {})
-        d["play"] = play
-        d["txt"] = d.get("txt", "") + " " + text
-        d["chapter"] = int(chapter or 0)
-        if len(chapters) == num_docs:
-            break
-
-    indexer = client.batch_indexer(chunk_size=50)
-    assert isinstance(indexer, Search.BatchIndexer)
-    assert 50 == indexer.chunk_size
-
-    for key, doc in chapters.items():
-        indexer.add_document(key, **doc)
-    indexer.commit()
-
-
-# override the default module client, search requires both db=0, and text
-@pytest.fixture
-def modclient():
-    return Redis.from_url(default_redismod_url, db=0, decode_responses=True)
-
-
-@pytest.fixture
-def client(modclient):
-    modclient.flushdb()
-    return modclient
-
-
-@pytest.mark.redismod
-def test_client(client):
-    num_docs = 500
-    createIndex(client.ft(), num_docs=num_docs)
-    waitForIndex(client, "idx")
-    # verify info
-    info = client.ft().info()
-    for k in [
-        "index_name",
-        "index_options",
-        "attributes",
-        "num_docs",
-        "max_doc_id",
-        "num_terms",
-        "num_records",
-        "inverted_sz_mb",
-        "offset_vectors_sz_mb",
-        "doc_table_size_mb",
-        "key_table_size_mb",
-        "records_per_doc_avg",
-        "bytes_per_record_avg",
-        "offsets_per_term_avg",
-        "offset_bits_per_record_avg",
-    ]:
-        assert k in info
-
-    assert client.ft().index_name == info["index_name"]
-    assert num_docs == int(info["num_docs"])
-
-    res = client.ft().search("henry iv")
-    assert isinstance(res, Result)
-    assert 225 == res.total
-    assert 10 == len(res.docs)
-    assert res.duration > 0
-
-    for doc in res.docs:
-        assert doc.id
-        assert doc.play == "Henry IV"
-        assert len(doc.txt) > 0
-
-    # test no content
-    res = client.ft().search(Query("king").no_content())
-    assert 194 == res.total
-    assert 10 == len(res.docs)
-    for doc in res.docs:
-        assert "txt" not in doc.__dict__
-        assert "play" not in doc.__dict__
-
-    # test verbatim vs no verbatim
-    total = client.ft().search(Query("kings").no_content()).total
-    vtotal = client.ft().search(Query("kings").no_content().verbatim()).total
-    assert total > vtotal
-
-    # test in fields
-    txt_total = (
-        client.ft().search(
-            Query("henry").no_content().limit_fields("txt")).total
-    )
-    play_total = (
-        client.ft().search(
-            Query("henry").no_content().limit_fields("play")).total
-    )
-    both_total = (
-        client.ft()
-        .search(Query("henry").no_content().limit_fields("play", "txt"))
-        .total
-    )
-    assert 129 == txt_total
-    assert 494 == play_total
-    assert 494 == both_total
-
-    # test load_document
-    doc = client.ft().load_document("henry vi part 3:62")
-    assert doc is not None
-    assert "henry vi part 3:62" == doc.id
-    assert doc.play == "Henry VI Part 3"
-    assert len(doc.txt) > 0
-
-    # test in-keys
-    ids = [x.id for x in client.ft().search(Query("henry")).docs]
-    assert 10 == len(ids)
-    subset = ids[:5]
-    docs = client.ft().search(Query("henry").limit_ids(*subset))
-    assert len(subset) == docs.total
-    ids = [x.id for x in docs.docs]
-    assert set(ids) == set(subset)
-
-    # test slop and in order
-    assert 193 == client.ft().search(Query("henry king")).total
-    assert 3 == client.ft().search(
-        Query("henry king").slop(0).in_order()).total
-    assert 52 == client.ft().search(
-        Query("king henry").slop(0).in_order()).total
-    assert 53 == client.ft().search(Query("henry king").slop(0)).total
-    assert 167 == client.ft().search(Query("henry king").slop(100)).total
-
-    # test delete document
-    client.ft().add_document("doc-5ghs2", play="Death of a Salesman")
-    res = client.ft().search(Query("death of a salesman"))
-    assert 1 == res.total
-
-    assert 1 == client.ft().delete_document("doc-5ghs2")
-    res = client.ft().search(Query("death of a salesman"))
-    assert 0 == res.total
-    assert 0 == client.ft().delete_document("doc-5ghs2")
-
-    client.ft().add_document("doc-5ghs2", play="Death of a Salesman")
-    res = client.ft().search(Query("death of a salesman"))
-    assert 1 == res.total
-    client.ft().delete_document("doc-5ghs2")
-
-
-@pytest.mark.redismod
-@skip_ifmodversion_lt("2.2.0", "search")
-def test_payloads(client):
-    client.ft().create_index((TextField("txt"),))
-
-    client.ft().add_document("doc1", payload="foo baz", txt="foo bar")
-    client.ft().add_document("doc2", txt="foo bar")
-
-    q = Query("foo bar").with_payloads()
-    res = client.ft().search(q)
-    assert 2 == res.total
-    assert "doc1" == res.docs[0].id
-    assert "doc2" == res.docs[1].id
-    assert "foo baz" == res.docs[0].payload
-    assert res.docs[1].payload is None
-
-
-@pytest.mark.redismod
-def test_scores(client):
-    client.ft().create_index((TextField("txt"),))
-
-    client.ft().add_document("doc1", txt="foo baz")
-    client.ft().add_document("doc2", txt="foo bar")
-
-    q = Query("foo ~bar").with_scores()
-    res = client.ft().search(q)
-    assert 2 == res.total
-    assert "doc2" == res.docs[0].id
-    assert 3.0 == res.docs[0].score
-    assert "doc1" == res.docs[1].id
-    # todo: enable once new RS version is tagged
-    # self.assertEqual(0.2, res.docs[1].score)
-
-
-@pytest.mark.redismod
-def test_replace(client):
-    client.ft().create_index((TextField("txt"),))
-
-    client.ft().add_document("doc1", txt="foo bar")
-    client.ft().add_document("doc2", txt="foo bar")
-    waitForIndex(client, "idx")
-
-    res = client.ft().search("foo bar")
-    assert 2 == res.total
-    client.ft().add_document(
-        "doc1",
-        replace=True,
-        txt="this is a replaced doc"
-    )
-
-    res = client.ft().search("foo bar")
-    assert 1 == res.total
-    assert "doc2" == res.docs[0].id
-
-    res = client.ft().search("replaced doc")
-    assert 1 == res.total
-    assert "doc1" == res.docs[0].id
-
-
-@pytest.mark.redismod
-def test_stopwords(client):
-    client.ft().create_index(
-        (TextField("txt"),),
-        stopwords=["foo", "bar", "baz"]
-    )
-    client.ft().add_document("doc1", txt="foo bar")
-    client.ft().add_document("doc2", txt="hello world")
-    waitForIndex(client, "idx")
-
-    q1 = Query("foo bar").no_content()
-    q2 = Query("foo bar hello world").no_content()
-    res1, res2 = client.ft().search(q1), client.ft().search(q2)
-    assert 0 == res1.total
-    assert 1 == res2.total
-
-
-@pytest.mark.redismod
-def test_filters(client):
-    client.ft().create_index(
-        (TextField("txt"),
-            NumericField("num"),
-            GeoField("loc"))
-    )
-    client.ft().add_document(
-        "doc1",
-        txt="foo bar",
-        num=3.141,
-        loc="-0.441,51.458"
-    )
-    client.ft().add_document("doc2", txt="foo baz", num=2, loc="-0.1,51.2")
-
-    waitForIndex(client, "idx")
-    # Test numerical filter
-    q1 = Query("foo").add_filter(NumericFilter("num", 0, 2)).no_content()
-    q2 = (
-        Query("foo")
-        .add_filter(
-            NumericFilter("num", 2, NumericFilter.INF, minExclusive=True))
-        .no_content()
-    )
-    res1, res2 = client.ft().search(q1), client.ft().search(q2)
-
-    assert 1 == res1.total
-    assert 1 == res2.total
-    assert "doc2" == res1.docs[0].id
-    assert "doc1" == res2.docs[0].id
-
-    # Test geo filter
-    q1 = Query("foo").add_filter(
-        GeoFilter("loc", -0.44, 51.45, 10)).no_content()
-    q2 = Query("foo").add_filter(
-        GeoFilter("loc", -0.44, 51.45, 100)).no_content()
-    res1, res2 = client.ft().search(q1), client.ft().search(q2)
-
-    assert 1 == res1.total
-    assert 2 == res2.total
-    assert "doc1" == res1.docs[0].id
-
-    # Sort results, after RDB reload order may change
-    res = [res2.docs[0].id, res2.docs[1].id]
-    res.sort()
-    assert ["doc1", "doc2"] == res
-
-
-@pytest.mark.redismod
-def test_payloads_with_no_content(client):
-    client.ft().create_index((TextField("txt"),))
-    client.ft().add_document("doc1", payload="foo baz", txt="foo bar")
-    client.ft().add_document("doc2", payload="foo baz2", txt="foo bar")
-
-    q = Query("foo bar").with_payloads().no_content()
-    res = client.ft().search(q)
-    assert 2 == len(res.docs)
-
-
-@pytest.mark.redismod
-def test_sort_by(client):
-    client.ft().create_index(
-        (TextField("txt"),
-            NumericField("num", sortable=True))
-    )
-    client.ft().add_document("doc1", txt="foo bar", num=1)
-    client.ft().add_document("doc2", txt="foo baz", num=2)
-    client.ft().add_document("doc3", txt="foo qux", num=3)
-
-    # Test sort
-    q1 = Query("foo").sort_by("num", asc=True).no_content()
-    q2 = Query("foo").sort_by("num", asc=False).no_content()
-    res1, res2 = client.ft().search(q1), client.ft().search(q2)
-
-    assert 3 == res1.total
-    assert "doc1" == res1.docs[0].id
-    assert "doc2" == res1.docs[1].id
-    assert "doc3" == res1.docs[2].id
-    assert 3 == res2.total
-    assert "doc1" == res2.docs[2].id
-    assert "doc2" == res2.docs[1].id
-    assert "doc3" == res2.docs[0].id
-
-
-@pytest.mark.redismod
-@skip_ifmodversion_lt("2.0.0", "search")
-def test_drop_index():
-    """
-    Ensure the index gets dropped by data remains by default
-    """
-    for x in range(20):
-        for keep_docs in [[True, {}], [False, {"name": "haveit"}]]:
-            idx = "HaveIt"
-            index = getClient()
-            index.hset("index:haveit", mapping={"name": "haveit"})
-            idef = IndexDefinition(prefix=["index:"])
-            index.ft(idx).create_index((TextField("name"),), definition=idef)
-            waitForIndex(index, idx)
-            index.ft(idx).dropindex(delete_documents=keep_docs[0])
-            i = index.hgetall("index:haveit")
-            assert i == keep_docs[1]
-
-
-@pytest.mark.redismod
-def test_example(client):
-    # Creating the index definition and schema
-    client.ft().create_index(
-        (TextField("title", weight=5.0),
-            TextField("body"))
-    )
-
-    # Indexing a document
-    client.ft().add_document(
-        "doc1",
-        title="RediSearch",
-        body="Redisearch impements a search engine on top of redis",
-    )
-
-    # Searching with complex parameters:
-    q = Query("search engine").verbatim().no_content().paging(0, 5)
-
-    res = client.ft().search(q)
-    assert res is not None
-
-
-@pytest.mark.redismod
-def test_auto_complete(client):
-    n = 0
-    with open(TITLES_CSV) as f:
-        cr = csv.reader(f)
-
-        for row in cr:
-            n += 1
-            term, score = row[0], float(row[1])
-            assert n == client.ft().sugadd("ac", Suggestion(term, score=score))
-
-    assert n == client.ft().suglen("ac")
-    ret = client.ft().sugget("ac", "bad", with_scores=True)
-    assert 2 == len(ret)
-    assert "badger" == ret[0].string
-    assert isinstance(ret[0].score, float)
-    assert 1.0 != ret[0].score
-    assert "badalte rishtey" == ret[1].string
-    assert isinstance(ret[1].score, float)
-    assert 1.0 != ret[1].score
-
-    ret = client.ft().sugget("ac", "bad", fuzzy=True, num=10)
-    assert 10 == len(ret)
-    assert 1.0 == ret[0].score
-    strs = {x.string for x in ret}
-
-    for sug in strs:
-        assert 1 == client.ft().sugdel("ac", sug)
-    # make sure a second delete returns 0
-    for sug in strs:
-        assert 0 == client.ft().sugdel("ac", sug)
-
-    # make sure they were actually deleted
-    ret2 = client.ft().sugget("ac", "bad", fuzzy=True, num=10)
-    for sug in ret2:
-        assert sug.string not in strs
-
-    # Test with payload
-    client.ft().sugadd("ac", Suggestion("pay1", payload="pl1"))
-    client.ft().sugadd("ac", Suggestion("pay2", payload="pl2"))
-    client.ft().sugadd("ac", Suggestion("pay3", payload="pl3"))
-
-    sugs = client.ft().sugget(
-        "ac",
-        "pay",
-        with_payloads=True,
-        with_scores=True
-    )
-    assert 3 == len(sugs)
-    for sug in sugs:
-        assert sug.payload
-        assert sug.payload.startswith("pl")
-
-
-@pytest.mark.redismod
-def test_no_index(client):
-    client.ft().create_index(
-        (
-            TextField("field"),
-            TextField("text", no_index=True, sortable=True),
-            NumericField("numeric", no_index=True, sortable=True),
-            GeoField("geo", no_index=True, sortable=True),
-            TagField("tag", no_index=True, sortable=True),
-        )
-    )
-
-    client.ft().add_document(
-        "doc1", field="aaa", text="1", numeric="1", geo="1,1", tag="1"
-    )
-    client.ft().add_document(
-        "doc2", field="aab", text="2", numeric="2", geo="2,2", tag="2"
-    )
-    waitForIndex(client, "idx")
-
-    res = client.ft().search(Query("@text:aa*"))
-    assert 0 == res.total
-
-    res = client.ft().search(Query("@field:aa*"))
-    assert 2 == res.total
-
-    res = client.ft().search(Query("*").sort_by("text", asc=False))
-    assert 2 == res.total
-    assert "doc2" == res.docs[0].id
-
-    res = client.ft().search(Query("*").sort_by("text", asc=True))
-    assert "doc1" == res.docs[0].id
-
-    res = client.ft().search(Query("*").sort_by("numeric", asc=True))
-    assert "doc1" == res.docs[0].id
-
-    res = client.ft().search(Query("*").sort_by("geo", asc=True))
-    assert "doc1" == res.docs[0].id
-
-    res = client.ft().search(Query("*").sort_by("tag", asc=True))
-    assert "doc1" == res.docs[0].id
-
-    # Ensure exception is raised for non-indexable, non-sortable fields
-    with pytest.raises(Exception):
-        TextField("name", no_index=True, sortable=False)
-    with pytest.raises(Exception):
-        NumericField("name", no_index=True, sortable=False)
-    with pytest.raises(Exception):
-        GeoField("name", no_index=True, sortable=False)
-    with pytest.raises(Exception):
-        TagField("name", no_index=True, sortable=False)
-
-
-@pytest.mark.redismod
-def test_partial(client):
-    client.ft().create_index(
-        (TextField("f1"),
-            TextField("f2"),
-            TextField("f3"))
-    )
-    client.ft().add_document("doc1", f1="f1_val", f2="f2_val")
-    client.ft().add_document("doc2", f1="f1_val", f2="f2_val")
-    client.ft().add_document("doc1", f3="f3_val", partial=True)
-    client.ft().add_document("doc2", f3="f3_val", replace=True)
-    waitForIndex(client, "idx")
-
-    # Search for f3 value. All documents should have it
-    res = client.ft().search("@f3:f3_val")
-    assert 2 == res.total
-
-    # Only the document updated with PARTIAL should still have f1 and f2 values
-    res = client.ft().search("@f3:f3_val @f2:f2_val @f1:f1_val")
-    assert 1 == res.total
-
-
-@pytest.mark.redismod
-def test_no_create(client):
-    client.ft().create_index(
-        (TextField("f1"),
-            TextField("f2"),
-            TextField("f3"))
-    )
-    client.ft().add_document("doc1", f1="f1_val", f2="f2_val")
-    client.ft().add_document("doc2", f1="f1_val", f2="f2_val")
-    client.ft().add_document("doc1", f3="f3_val", no_create=True)
-    client.ft().add_document("doc2", f3="f3_val", no_create=True, partial=True)
-    waitForIndex(client, "idx")
-
-    # Search for f3 value. All documents should have it
-    res = client.ft().search("@f3:f3_val")
-    assert 2 == res.total
-
-    # Only the document updated with PARTIAL should still have f1 and f2 values
-    res = client.ft().search("@f3:f3_val @f2:f2_val @f1:f1_val")
-    assert 1 == res.total
-
-    with pytest.raises(redis.ResponseError):
-        client.ft().add_document(
-            "doc3",
-            f2="f2_val",
-            f3="f3_val",
-            no_create=True
-        )
-
-
-@pytest.mark.redismod
-def test_explain(client):
-    client.ft().create_index(
-        (TextField("f1"),
-            TextField("f2"),
-            TextField("f3"))
-    )
-    res = client.ft().explain("@f3:f3_val @f2:f2_val @f1:f1_val")
-    assert res
-
-
-@pytest.mark.redismod
-def test_explaincli(client):
-    with pytest.raises(NotImplementedError):
-        client.ft().explain_cli("foo")
-
-
-@pytest.mark.redismod
-def test_summarize(client):
-    createIndex(client.ft())
-    waitForIndex(client, "idx")
-
-    q = Query("king henry").paging(0, 1)
-    q.highlight(fields=("play", "txt"), tags=("<b>", "</b>"))
-    q.summarize("txt")
-
-    doc = sorted(client.ft().search(q).docs)[0]
-    assert "<b>Henry</b> IV" == doc.play
-    assert (
-        "ACT I SCENE I. London. The palace. Enter <b>KING</b> <b>HENRY</b>, LORD JOHN OF LANCASTER, the EARL of WESTMORELAND, SIR... "  # noqa
-        == doc.txt
-    )
-
-    q = Query("king henry").paging(0, 1).summarize().highlight()
-
-    doc = sorted(client.ft().search(q).docs)[0]
-    assert "<b>Henry</b> ... " == doc.play
-    assert (
-        "ACT I SCENE I. London. The palace. Enter <b>KING</b> <b>HENRY</b>, LORD JOHN OF LANCASTER, the EARL of WESTMORELAND, SIR... "  # noqa
-        == doc.txt
-    )
-
-
-@pytest.mark.redismod
-@skip_ifmodversion_lt("2.0.0", "search")
-def test_alias():
-    index1 = getClient()
-    index2 = getClient()
-
-    def1 = IndexDefinition(prefix=["index1:"])
-    def2 = IndexDefinition(prefix=["index2:"])
-
-    ftindex1 = index1.ft("testAlias")
-    ftindex2 = index2.ft("testAlias2")
-    ftindex1.create_index((TextField("name"),), definition=def1)
-    ftindex2.create_index((TextField("name"),), definition=def2)
-
-    index1.hset("index1:lonestar", mapping={"name": "lonestar"})
-    index2.hset("index2:yogurt", mapping={"name": "yogurt"})
-
-    res = ftindex1.search("*").docs[0]
-    assert "index1:lonestar" == res.id
-
-    # create alias and check for results
-    ftindex1.aliasadd("spaceballs")
-    alias_client = getClient().ft("spaceballs")
-    res = alias_client.search("*").docs[0]
-    assert "index1:lonestar" == res.id
-
-    # Throw an exception when trying to add an alias that already exists
-    with pytest.raises(Exception):
-        ftindex2.aliasadd("spaceballs")
-
-    # update alias and ensure new results
-    ftindex2.aliasupdate("spaceballs")
-    alias_client2 = getClient().ft("spaceballs")
-
-    res = alias_client2.search("*").docs[0]
-    assert "index2:yogurt" == res.id
-
-    ftindex2.aliasdel("spaceballs")
-    with pytest.raises(Exception):
-        alias_client2.search("*").docs[0]
-
-
-@pytest.mark.redismod
-def test_alias_basic():
-    # Creating a client with one index
-    getClient().flushdb()
-    index1 = getClient().ft("testAlias")
-
-    index1.create_index((TextField("txt"),))
-    index1.add_document("doc1", txt="text goes here")
-
-    index2 = getClient().ft("testAlias2")
-    index2.create_index((TextField("txt"),))
-    index2.add_document("doc2", txt="text goes here")
-
-    # add the actual alias and check
-    index1.aliasadd("myalias")
-    alias_client = getClient().ft("myalias")
-    res = sorted(alias_client.search("*").docs, key=lambda x: x.id)
-    assert "doc1" == res[0].id
-
-    # Throw an exception when trying to add an alias that already exists
-    with pytest.raises(Exception):
-        index2.aliasadd("myalias")
-
-    # update the alias and ensure we get doc2
-    index2.aliasupdate("myalias")
-    alias_client2 = getClient().ft("myalias")
-    res = sorted(alias_client2.search("*").docs, key=lambda x: x.id)
-    assert "doc1" == res[0].id
-
-    # delete the alias and expect an error if we try to query again
-    index2.aliasdel("myalias")
-    with pytest.raises(Exception):
-        _ = alias_client2.search("*").docs[0]
-
-
-@pytest.mark.redismod
-def test_tags(client):
-    client.ft().create_index((TextField("txt"), TagField("tags")))
-    tags = "foo,foo bar,hello;world"
-    tags2 = "soba,ramen"
-
-    client.ft().add_document("doc1", txt="fooz barz", tags=tags)
-    client.ft().add_document("doc2", txt="noodles", tags=tags2)
-    waitForIndex(client, "idx")
-
-    q = Query("@tags:{foo}")
-    res = client.ft().search(q)
-    assert 1 == res.total
-
-    q = Query("@tags:{foo bar}")
-    res = client.ft().search(q)
-    assert 1 == res.total
-
-    q = Query("@tags:{foo\\ bar}")
-    res = client.ft().search(q)
-    assert 1 == res.total
-
-    q = Query("@tags:{hello\\;world}")
-    res = client.ft().search(q)
-    assert 1 == res.total
-
-    q2 = client.ft().tagvals("tags")
-    assert (tags.split(",") + tags2.split(",")).sort() == q2.sort()
-
-
-@pytest.mark.redismod
-def test_textfield_sortable_nostem(client):
-    # Creating the index definition with sortable and no_stem
-    client.ft().create_index((TextField("txt", sortable=True, no_stem=True),))
-
-    # Now get the index info to confirm its contents
-    response = client.ft().info()
-    assert "SORTABLE" in response["attributes"][0]
-    assert "NOSTEM" in response["attributes"][0]
-
-
-@pytest.mark.redismod
-def test_alter_schema_add(client):
-    # Creating the index definition and schema
-    client.ft().create_index(TextField("title"))
-
-    # Using alter to add a field
-    client.ft().alter_schema_add(TextField("body"))
-
-    # Indexing a document
-    client.ft().add_document(
-        "doc1", title="MyTitle", body="Some content only in the body"
-    )
-
-    # Searching with parameter only in the body (the added field)
-    q = Query("only in the body")
-
-    # Ensure we find the result searching on the added body field
-    res = client.ft().search(q)
-    assert 1 == res.total
-
-
-@pytest.mark.redismod
-def test_spell_check(client):
-    client.ft().create_index((TextField("f1"), TextField("f2")))
-
-    client.ft().add_document(
-        "doc1",
-        f1="some valid content",
-        f2="this is sample text"
-    )
-    client.ft().add_document("doc2", f1="very important", f2="lorem ipsum")
-    waitForIndex(client, "idx")
-
-    # test spellcheck
-    res = client.ft().spellcheck("impornant")
-    assert "important" == res["impornant"][0]["suggestion"]
-
-    res = client.ft().spellcheck("contnt")
-    assert "content" == res["contnt"][0]["suggestion"]
-
-    # test spellcheck with Levenshtein distance
-    res = client.ft().spellcheck("vlis")
-    assert res == {}
-    res = client.ft().spellcheck("vlis", distance=2)
-    assert "valid" == res["vlis"][0]["suggestion"]
-
-    # test spellcheck include
-    client.ft().dict_add("dict", "lore", "lorem", "lorm")
-    res = client.ft().spellcheck("lorm", include="dict")
-    assert len(res["lorm"]) == 3
-    assert (
-        res["lorm"][0]["suggestion"],
-        res["lorm"][1]["suggestion"],
-        res["lorm"][2]["suggestion"],
-    ) == ("lorem", "lore", "lorm")
-    assert (res["lorm"][0]["score"], res["lorm"][1]["score"]) == ("0.5", "0")
-
-    # test spellcheck exclude
-    res = client.ft().spellcheck("lorm", exclude="dict")
-    assert res == {}
-
-
-@pytest.mark.redismod
-def test_dict_operations(client):
-    client.ft().create_index((TextField("f1"), TextField("f2")))
-    # Add three items
-    res = client.ft().dict_add("custom_dict", "item1", "item2", "item3")
-    assert 3 == res
-
-    # Remove one item
-    res = client.ft().dict_del("custom_dict", "item2")
-    assert 1 == res
-
-    # Dump dict and inspect content
-    res = client.ft().dict_dump("custom_dict")
-    assert ["item1", "item3"] == res
-
-    # Remove rest of the items before reload
-    client.ft().dict_del("custom_dict", *res)
-
-
-@pytest.mark.redismod
-def test_phonetic_matcher(client):
-    client.ft().create_index((TextField("name"),))
-    client.ft().add_document("doc1", name="Jon")
-    client.ft().add_document("doc2", name="John")
-
-    res = client.ft().search(Query("Jon"))
-    assert 1 == len(res.docs)
-    assert "Jon" == res.docs[0].name
-
-    # Drop and create index with phonetic matcher
-    client.flushdb()
-
-    client.ft().create_index((TextField("name", phonetic_matcher="dm:en"),))
-    client.ft().add_document("doc1", name="Jon")
-    client.ft().add_document("doc2", name="John")
-
-    res = client.ft().search(Query("Jon"))
-    assert 2 == len(res.docs)
-    assert ["John", "Jon"] == sorted([d.name for d in res.docs])
-
-
-@pytest.mark.redismod
-def test_scorer(client):
-    client.ft().create_index((TextField("description"),))
-
-    client.ft().add_document(
-        "doc1", description="The quick brown fox jumps over the lazy dog"
-    )
-    client.ft().add_document(
-        "doc2",
-        description="Quick alice was beginning to get very tired of sitting by her quick sister on the bank, and of having nothing to do.",  # noqa
-    )
-
-    # default scorer is TFIDF
-    res = client.ft().search(Query("quick").with_scores())
-    assert 1.0 == res.docs[0].score
-    res = client.ft().search(Query("quick").scorer("TFIDF").with_scores())
-    assert 1.0 == res.docs[0].score
-    res = client.ft().search(
-        Query("quick").scorer("TFIDF.DOCNORM").with_scores())
-    assert 0.1111111111111111 == res.docs[0].score
-    res = client.ft().search(Query("quick").scorer("BM25").with_scores())
-    assert 0.17699114465425977 == res.docs[0].score
-    res = client.ft().search(Query("quick").scorer("DISMAX").with_scores())
-    assert 2.0 == res.docs[0].score
-    res = client.ft().search(Query("quick").scorer("DOCSCORE").with_scores())
-    assert 1.0 == res.docs[0].score
-    res = client.ft().search(Query("quick").scorer("HAMMING").with_scores())
-    assert 0.0 == res.docs[0].score
-
-
-@pytest.mark.redismod
-def test_get(client):
-    client.ft().create_index((TextField("f1"), TextField("f2")))
-
-    assert [None] == client.ft().get("doc1")
-    assert [None, None] == client.ft().get("doc2", "doc1")
-
-    client.ft().add_document(
-        "doc1", f1="some valid content dd1", f2="this is sample text ff1"
-    )
-    client.ft().add_document(
-        "doc2", f1="some valid content dd2", f2="this is sample text ff2"
-    )
-
-    assert [
-        ["f1", "some valid content dd2", "f2", "this is sample text ff2"]
-    ] == client.ft().get("doc2")
-    assert [
-        ["f1", "some valid content dd1", "f2", "this is sample text ff1"],
-        ["f1", "some valid content dd2", "f2", "this is sample text ff2"],
-    ] == client.ft().get("doc1", "doc2")
-
-
-@pytest.mark.redismod
-@skip_ifmodversion_lt("2.2.0", "search")
-def test_config(client):
-    assert client.ft().config_set("TIMEOUT", "100")
-    with pytest.raises(redis.ResponseError):
-        client.ft().config_set("TIMEOUT", "null")
-    res = client.ft().config_get("*")
-    assert "100" == res["TIMEOUT"]
-    res = client.ft().config_get("TIMEOUT")
-    assert "100" == res["TIMEOUT"]
-
-
-@pytest.mark.redismod
-def test_aggregations(client):
-    # Creating the index definition and schema
-    client.ft().create_index(
-        (
-            NumericField("random_num"),
-            TextField("title"),
-            TextField("body"),
-            TextField("parent"),
-        )
-    )
-
-    # Indexing a document
-    client.ft().add_document(
-        "search",
-        title="RediSearch",
-        body="Redisearch impements a search engine on top of redis",
-        parent="redis",
-        random_num=10,
-    )
-    client.ft().add_document(
-        "ai",
-        title="RedisAI",
-        body="RedisAI executes Deep Learning/Machine Learning models and managing their data.",  # noqa
-        parent="redis",
-        random_num=3,
-    )
-    client.ft().add_document(
-        "json",
-        title="RedisJson",
-        body="RedisJSON implements ECMA-404 The JSON Data Interchange Standard as a native data type.",  # noqa
-        parent="redis",
-        random_num=8,
-    )
-
-    req = aggregations.AggregateRequest("redis").group_by(
-        "@parent",
-        reducers.count(),
-        reducers.count_distinct("@title"),
-        reducers.count_distinctish("@title"),
-        reducers.sum("@random_num"),
-        reducers.min("@random_num"),
-        reducers.max("@random_num"),
-        reducers.avg("@random_num"),
-        reducers.stddev("random_num"),
-        reducers.quantile("@random_num", 0.5),
-        reducers.tolist("@title"),
-        reducers.first_value("@title"),
-        reducers.random_sample("@title", 2),
-    )
-
-    res = client.ft().aggregate(req)
-
-    res = res.rows[0]
-    assert len(res) == 26
-    assert "redis" == res[1]
-    assert "3" == res[3]
-    assert "3" == res[5]
-    assert "3" == res[7]
-    assert "21" == res[9]
-    assert "3" == res[11]
-    assert "10" == res[13]
-    assert "7" == res[15]
-    assert "3.60555127546" == res[17]
-    assert "10" == res[19]
-    assert ["RediSearch", "RedisAI", "RedisJson"] == res[21]
-    assert "RediSearch" == res[23]
-    assert 2 == len(res[25])
-
-
-@pytest.mark.redismod
-@skip_ifmodversion_lt("2.0.0", "search")
-def test_index_definition(client):
-    """
-    Create definition and test its args
-    """
-    with pytest.raises(RuntimeError):
-        IndexDefinition(prefix=["hset:", "henry"], index_type="json")
-
-    definition = IndexDefinition(
-        prefix=["hset:", "henry"],
-        filter="@f1==32",
-        language="English",
-        language_field="play",
-        score_field="chapter",
-        score=0.5,
-        payload_field="txt",
-        index_type=IndexType.JSON,
-    )
-
-    assert [
-        "ON",
-        "JSON",
-        "PREFIX",
-        2,
-        "hset:",
-        "henry",
-        "FILTER",
-        "@f1==32",
-        "LANGUAGE_FIELD",
-        "play",
-        "LANGUAGE",
-        "English",
-        "SCORE_FIELD",
-        "chapter",
-        "SCORE",
-        0.5,
-        "PAYLOAD_FIELD",
-        "txt",
-    ] == definition.args
-
-    createIndex(client.ft(), num_docs=500, definition=definition)
-
-
-@pytest.mark.redismod
-@skip_ifmodversion_lt("2.0.0", "search")
-def test_create_client_definition(client):
-    """
-    Create definition with no index type provided,
-    and use hset to test the client definition (the default is HASH).
-    """
-    definition = IndexDefinition(prefix=["hset:", "henry"])
-    createIndex(client.ft(), num_docs=500, definition=definition)
-
-    info = client.ft().info()
-    assert 494 == int(info["num_docs"])
-
-    client.ft().client.hset("hset:1", "f1", "v1")
-    info = client.ft().info()
-    assert 495 == int(info["num_docs"])
-
-
-@pytest.mark.redismod
-@skip_ifmodversion_lt("2.0.0", "search")
-def test_create_client_definition_hash(client):
-    """
-    Create definition with IndexType.HASH as index type (ON HASH),
-    and use hset to test the client definition.
-    """
-    definition = IndexDefinition(
-        prefix=["hset:", "henry"],
-        index_type=IndexType.HASH
-    )
-    createIndex(client.ft(), num_docs=500, definition=definition)
-
-    info = client.ft().info()
-    assert 494 == int(info["num_docs"])
-
-    client.ft().client.hset("hset:1", "f1", "v1")
-    info = client.ft().info()
-    assert 495 == int(info["num_docs"])
-
-
-@pytest.mark.redismod
-@skip_ifmodversion_lt("2.2.0", "search")
-def test_create_client_definition_json(client):
-    """
-    Create definition with IndexType.JSON as index type (ON JSON),
-    and use json client to test it.
-    """
-    definition = IndexDefinition(prefix=["king:"], index_type=IndexType.JSON)
-    client.ft().create_index((TextField("$.name"),), definition=definition)
-
-    client.json().set("king:1", Path.rootPath(), {"name": "henry"})
-    client.json().set("king:2", Path.rootPath(), {"name": "james"})
-
-    res = client.ft().search("henry")
-    assert res.docs[0].id == "king:1"
-    assert res.docs[0].payload is None
-    assert res.docs[0].json == '{"name":"henry"}'
-    assert res.total == 1
-
-
-@pytest.mark.redismod
-@skip_ifmodversion_lt("2.2.0", "search")
-def test_fields_as_name(client):
-    # create index
-    SCHEMA = (
-        TextField("$.name", sortable=True, as_name="name"),
-        NumericField("$.age", as_name="just_a_number"),
-    )
-    definition = IndexDefinition(index_type=IndexType.JSON)
-    client.ft().create_index(SCHEMA, definition=definition)
-
-    # insert json data
-    res = client.json().set(
-        "doc:1",
-        Path.rootPath(),
-        {"name": "Jon", "age": 25}
-    )
-    assert res
-
-    total = client.ft().search(
-        Query("Jon").return_fields("name", "just_a_number")).docs
-    assert 1 == len(total)
-    assert "doc:1" == total[0].id
-    assert "Jon" == total[0].name
-    assert "25" == total[0].just_a_number
-
-
-@pytest.mark.redismod
-@skip_ifmodversion_lt("2.2.0", "search")
-def test_search_return_fields(client):
-    res = client.json().set(
-        "doc:1",
-        Path.rootPath(),
-        {"t": "riceratops", "t2": "telmatosaurus", "n": 9072, "flt": 97.2},
-    )
-    assert res
-
-    # create index on
-    definition = IndexDefinition(index_type=IndexType.JSON)
-    SCHEMA = (
-        TextField("$.t"),
-        NumericField("$.flt"),
-    )
-    client.ft().create_index(SCHEMA, definition=definition)
-    waitForIndex(client, "idx")
-
-    total = client.ft().search(
-        Query("*").return_field("$.t", as_field="txt")).docs
-    assert 1 == len(total)
-    assert "doc:1" == total[0].id
-    assert "riceratops" == total[0].txt
-
-    total = client.ft().search(
-        Query("*").return_field("$.t2", as_field="txt")).docs
-    assert 1 == len(total)
-    assert "doc:1" == total[0].id
-    assert "telmatosaurus" == total[0].txt
-
-
-@pytest.mark.redismod
-def test_synupdate(client):
-    definition = IndexDefinition(index_type=IndexType.HASH)
-    client.ft().create_index(
-        (
-            TextField("title"),
-            TextField("body"),
-        ),
-        definition=definition,
-    )
-
-    client.ft().synupdate("id1", True, "boy", "child", "offspring")
-    client.ft().add_document(
-        "doc1",
-        title="he is a baby",
-        body="this is a test")
-
-    client.ft().synupdate("id1", True, "baby")
-    client.ft().add_document(
-        "doc2",
-        title="he is another baby",
-        body="another test"
-    )
-
-    res = client.ft().search(Query("child").expander("SYNONYM"))
-    assert res.docs[0].id == "doc2"
-    assert res.docs[0].title == "he is another baby"
-    assert res.docs[0].body == "another test"
-
-
-@pytest.mark.redismod
-def test_syndump(client):
-    definition = IndexDefinition(index_type=IndexType.HASH)
-    client.ft().create_index(
-        (
-            TextField("title"),
-            TextField("body"),
-        ),
-        definition=definition,
-    )
-
-    client.ft().synupdate("id1", False, "boy", "child", "offspring")
-    client.ft().synupdate("id2", False, "baby", "child")
-    client.ft().synupdate("id3", False, "tree", "wood")
-    res = client.ft().syndump()
-    assert res == {
-        "boy": ["id1"],
-        "tree": ["id3"],
-        "wood": ["id3"],
-        "child": ["id1", "id2"],
-        "baby": ["id2"],
-        "offspring": ["id1"],
-    }
-
-
-@pytest.mark.redismod
-@skip_ifmodversion_lt("2.2.0", "search")
-def test_create_json_with_alias(client):
-    """
-    Create definition with IndexType.JSON as index type (ON JSON) with two
-    fields with aliases, and use json client to test it.
-    """
-    definition = IndexDefinition(prefix=["king:"], index_type=IndexType.JSON)
-    client.ft().create_index(
-        (TextField("$.name", as_name="name"),
-         NumericField("$.num", as_name="num")),
-        definition=definition
-    )
-
-    client.json().set("king:1", Path.rootPath(), {"name": "henry",
-                                                  "num": 42})
-    client.json().set("king:2", Path.rootPath(), {"name": "james",
-                                                  "num": 3.14})
-
-    res = client.ft().search("@name:henry")
-    assert res.docs[0].id == "king:1"
-    assert res.docs[0].json == '{"name":"henry","num":42}'
-    assert res.total == 1
-
-    res = client.ft().search("@num:[0 10]")
-    assert res.docs[0].id == "king:2"
-    assert res.docs[0].json == '{"name":"james","num":3.14}'
-    assert res.total == 1
-
-    # Tests returns an error if path contain special characters (user should
-    # use an alias)
-    with pytest.raises(Exception):
-        client.ft().search("@$.name:henry")
-
-
-@pytest.mark.redismod
-@skip_ifmodversion_lt("2.2.0", "search")
-def test_json_with_multipath(client):
-    """
-    Create definition with IndexType.JSON as index type (ON JSON),
-    and use json client to test it.
-    """
-    definition = IndexDefinition(prefix=["king:"], index_type=IndexType.JSON)
-    client.ft().create_index(
-        (TagField("$..name", as_name="name")),
-        definition=definition
-    )
-
-    client.json().set("king:1", Path.rootPath(),
-                      {"name": "henry", "country": {"name": "england"}})
-
-    res = client.ft().search("@name:{henry}")
-    assert res.docs[0].id == "king:1"
-    assert res.docs[0].json == '{"name":"henry","country":{"name":"england"}}'
-    assert res.total == 1
-
-    res = client.ft().search("@name:{england}")
-    assert res.docs[0].id == "king:1"
-    assert res.docs[0].json == '{"name":"henry","country":{"name":"england"}}'
-    assert res.total == 1
-
-
-@pytest.mark.redismod
-@skip_ifmodversion_lt("2.2.0", "search")
-def test_json_with_jsonpath(client):
-    definition = IndexDefinition(index_type=IndexType.JSON)
-    client.ft().create_index(
-        (TextField('$["prod:name"]', as_name="name"),
-         TextField('$.prod:name', as_name="name_unsupported")),
-        definition=definition
-    )
-
-    client.json().set("doc:1", Path.rootPath(), {"prod:name": "RediSearch"})
-
-    # query for a supported field succeeds
-    res = client.ft().search(Query("@name:RediSearch"))
-    assert res.total == 1
-    assert res.docs[0].id == "doc:1"
-    assert res.docs[0].json == '{"prod:name":"RediSearch"}'
-
-    # query for an unsupported field fails
-    res = client.ft().search("@name_unsupported:RediSearch")
-    assert res.total == 0
-
-    # return of a supported field succeeds
-    res = client.ft().search(Query("@name:RediSearch").return_field("name"))
-    assert res.total == 1
-    assert res.docs[0].id == "doc:1"
-    assert res.docs[0].name == 'RediSearch'
-
-    # return of an unsupported field fails
-    res = client.ft().search(Query("@name:RediSearch")
-                             .return_field("name_unsupported"))
-    assert res.total == 1
-    assert res.docs[0].id == "doc:1"
-    with pytest.raises(Exception):
-        res.docs[0].name_unsupported
diff --git a/tests/test_timeseries.py b/tests/test_timeseries.py
deleted file mode 100644
index 99c6083..0000000
--- a/tests/test_timeseries.py
+++ /dev/null
@@ -1,588 +0,0 @@
-import pytest
-import time
-from time import sleep
-from .conftest import skip_ifmodversion_lt
-
-
-@pytest.fixture
-def client(modclient):
-    modclient.flushdb()
-    return modclient
-
-
-@pytest.mark.redismod
-def test_create(client):
-    assert client.ts().create(1)
-    assert client.ts().create(2, retention_msecs=5)
-    assert client.ts().create(3, labels={"Redis": "Labs"})
-    assert client.ts().create(4, retention_msecs=20, labels={"Time": "Series"})
-    info = client.ts().info(4)
-    assert 20 == info.retention_msecs
-    assert "Series" == info.labels["Time"]
-
-    # Test for a chunk size of 128 Bytes
-    assert client.ts().create("time-serie-1", chunk_size=128)
-    info = client.ts().info("time-serie-1")
-    assert 128, info.chunk_size
-
-
-@pytest.mark.redismod
-@skip_ifmodversion_lt("1.4.0", "timeseries")
-def test_create_duplicate_policy(client):
-    # Test for duplicate policy
-    for duplicate_policy in ["block", "last", "first", "min", "max"]:
-        ts_name = "time-serie-ooo-{0}".format(duplicate_policy)
-        assert client.ts().create(ts_name, duplicate_policy=duplicate_policy)
-        info = client.ts().info(ts_name)
-        assert duplicate_policy == info.duplicate_policy
-
-
-@pytest.mark.redismod
-def test_alter(client):
-    assert client.ts().create(1)
-    assert 0 == client.ts().info(1).retention_msecs
-    assert client.ts().alter(1, retention_msecs=10)
-    assert {} == client.ts().info(1).labels
-    assert 10, client.ts().info(1).retention_msecs
-    assert client.ts().alter(1, labels={"Time": "Series"})
-    assert "Series" == client.ts().info(1).labels["Time"]
-    assert 10 == client.ts().info(1).retention_msecs
-
-
-@pytest.mark.redismod
-@skip_ifmodversion_lt("1.4.0", "timeseries")
-def test_alter_diplicate_policy(client):
-    assert client.ts().create(1)
-    info = client.ts().info(1)
-    assert info.duplicate_policy is None
-    assert client.ts().alter(1, duplicate_policy="min")
-    info = client.ts().info(1)
-    assert "min" == info.duplicate_policy
-
-
-@pytest.mark.redismod
-def test_add(client):
-    assert 1 == client.ts().add(1, 1, 1)
-    assert 2 == client.ts().add(2, 2, 3, retention_msecs=10)
-    assert 3 == client.ts().add(3, 3, 2, labels={"Redis": "Labs"})
-    assert 4 == client.ts().add(
-        4, 4, 2, retention_msecs=10, labels={"Redis": "Labs", "Time": "Series"}
-    )
-    assert round(time.time()) == \
-        round(float(client.ts().add(5, "*", 1)) / 1000)
-
-    info = client.ts().info(4)
-    assert 10 == info.retention_msecs
-    assert "Labs" == info.labels["Redis"]
-
-    # Test for a chunk size of 128 Bytes on TS.ADD
-    assert client.ts().add("time-serie-1", 1, 10.0, chunk_size=128)
-    info = client.ts().info("time-serie-1")
-    assert 128 == info.chunk_size
-
-
-@pytest.mark.redismod
-@skip_ifmodversion_lt("1.4.0", "timeseries")
-def test_add_duplicate_policy(client):
-
-    # Test for duplicate policy BLOCK
-    assert 1 == client.ts().add("time-serie-add-ooo-block", 1, 5.0)
-    with pytest.raises(Exception):
-        client.ts().add(
-            "time-serie-add-ooo-block",
-            1,
-            5.0,
-            duplicate_policy="block"
-        )
-
-    # Test for duplicate policy LAST
-    assert 1 == client.ts().add("time-serie-add-ooo-last", 1, 5.0)
-    assert 1 == client.ts().add(
-        "time-serie-add-ooo-last", 1, 10.0, duplicate_policy="last"
-    )
-    assert 10.0 == client.ts().get("time-serie-add-ooo-last")[1]
-
-    # Test for duplicate policy FIRST
-    assert 1 == client.ts().add("time-serie-add-ooo-first", 1, 5.0)
-    assert 1 == client.ts().add(
-        "time-serie-add-ooo-first", 1, 10.0, duplicate_policy="first"
-    )
-    assert 5.0 == client.ts().get("time-serie-add-ooo-first")[1]
-
-    # Test for duplicate policy MAX
-    assert 1 == client.ts().add("time-serie-add-ooo-max", 1, 5.0)
-    assert 1 == client.ts().add(
-        "time-serie-add-ooo-max", 1, 10.0, duplicate_policy="max"
-    )
-    assert 10.0 == client.ts().get("time-serie-add-ooo-max")[1]
-
-    # Test for duplicate policy MIN
-    assert 1 == client.ts().add("time-serie-add-ooo-min", 1, 5.0)
-    assert 1 == client.ts().add(
-        "time-serie-add-ooo-min", 1, 10.0, duplicate_policy="min"
-    )
-    assert 5.0 == client.ts().get("time-serie-add-ooo-min")[1]
-
-
-@pytest.mark.redismod
-def test_madd(client):
-    client.ts().create("a")
-    assert [1, 2, 3] == \
-        client.ts().madd([("a", 1, 5), ("a", 2, 10), ("a", 3, 15)])
-
-
-@pytest.mark.redismod
-def test_incrby_decrby(client):
-    for _ in range(100):
-        assert client.ts().incrby(1, 1)
-        sleep(0.001)
-    assert 100 == client.ts().get(1)[1]
-    for _ in range(100):
-        assert client.ts().decrby(1, 1)
-        sleep(0.001)
-    assert 0 == client.ts().get(1)[1]
-
-    assert client.ts().incrby(2, 1.5, timestamp=5)
-    assert (5, 1.5) == client.ts().get(2)
-    assert client.ts().incrby(2, 2.25, timestamp=7)
-    assert (7, 3.75) == client.ts().get(2)
-    assert client.ts().decrby(2, 1.5, timestamp=15)
-    assert (15, 2.25) == client.ts().get(2)
-
-    # Test for a chunk size of 128 Bytes on TS.INCRBY
-    assert client.ts().incrby("time-serie-1", 10, chunk_size=128)
-    info = client.ts().info("time-serie-1")
-    assert 128 == info.chunk_size
-
-    # Test for a chunk size of 128 Bytes on TS.DECRBY
-    assert client.ts().decrby("time-serie-2", 10, chunk_size=128)
-    info = client.ts().info("time-serie-2")
-    assert 128 == info.chunk_size
-
-
-@pytest.mark.redismod
-def test_create_and_delete_rule(client):
-    # test rule creation
-    time = 100
-    client.ts().create(1)
-    client.ts().create(2)
-    client.ts().createrule(1, 2, "avg", 100)
-    for i in range(50):
-        client.ts().add(1, time + i * 2, 1)
-        client.ts().add(1, time + i * 2 + 1, 2)
-    client.ts().add(1, time * 2, 1.5)
-    assert round(client.ts().get(2)[1], 5) == 1.5
-    info = client.ts().info(1)
-    assert info.rules[0][1] == 100
-
-    # test rule deletion
-    client.ts().deleterule(1, 2)
-    info = client.ts().info(1)
-    assert not info.rules
-
-
-@pytest.mark.redismod
-@skip_ifmodversion_lt("99.99.99", "timeseries")
-def test_del_range(client):
-    try:
-        client.ts().delete("test", 0, 100)
-    except Exception as e:
-        assert e.__str__() != ""
-
-    for i in range(100):
-        client.ts().add(1, i, i % 7)
-    assert 22 == client.ts().delete(1, 0, 21)
-    assert [] == client.ts().range(1, 0, 21)
-    assert [(22, 1.0)] == client.ts().range(1, 22, 22)
-
-
-@pytest.mark.redismod
-def test_range(client):
-    for i in range(100):
-        client.ts().add(1, i, i % 7)
-    assert 100 == len(client.ts().range(1, 0, 200))
-    for i in range(100):
-        client.ts().add(1, i + 200, i % 7)
-    assert 200 == len(client.ts().range(1, 0, 500))
-    # last sample isn't returned
-    assert 20 == len(
-        client.ts().range(
-            1,
-            0,
-            500,
-            aggregation_type="avg",
-            bucket_size_msec=10
-        )
-    )
-    assert 10 == len(client.ts().range(1, 0, 500, count=10))
-
-
-@pytest.mark.redismod
-@skip_ifmodversion_lt("99.99.99", "timeseries")
-def test_range_advanced(client):
-    for i in range(100):
-        client.ts().add(1, i, i % 7)
-        client.ts().add(1, i + 200, i % 7)
-
-    assert 2 == len(
-        client.ts().range(
-            1,
-            0,
-            500,
-            filter_by_ts=[i for i in range(10, 20)],
-            filter_by_min_value=1,
-            filter_by_max_value=2,
-        )
-    )
-    assert [(0, 10.0), (10, 1.0)] == client.ts().range(
-        1, 0, 10, aggregation_type="count", bucket_size_msec=10, align="+"
-    )
-    assert [(-5, 5.0), (5, 6.0)] == client.ts().range(
-        1, 0, 10, aggregation_type="count", bucket_size_msec=10, align=5
-    )
-
-
-@pytest.mark.redismod
-@skip_ifmodversion_lt("99.99.99", "timeseries")
-def test_rev_range(client):
-    for i in range(100):
-        client.ts().add(1, i, i % 7)
-    assert 100 == len(client.ts().range(1, 0, 200))
-    for i in range(100):
-        client.ts().add(1, i + 200, i % 7)
-    assert 200 == len(client.ts().range(1, 0, 500))
-    # first sample isn't returned
-    assert 20 == len(
-        client.ts().revrange(
-            1,
-            0,
-            500,
-            aggregation_type="avg",
-            bucket_size_msec=10
-        )
-    )
-    assert 10 == len(client.ts().revrange(1, 0, 500, count=10))
-    assert 2 == len(
-        client.ts().revrange(
-            1,
-            0,
-            500,
-            filter_by_ts=[i for i in range(10, 20)],
-            filter_by_min_value=1,
-            filter_by_max_value=2,
-        )
-    )
-    assert [(10, 1.0), (0, 10.0)] == client.ts().revrange(
-        1, 0, 10, aggregation_type="count", bucket_size_msec=10, align="+"
-    )
-    assert [(1, 10.0), (-9, 1.0)] == client.ts().revrange(
-        1, 0, 10, aggregation_type="count", bucket_size_msec=10, align=1
-    )
-
-
-@pytest.mark.redismod
-def testMultiRange(client):
-    client.ts().create(1, labels={"Test": "This", "team": "ny"})
-    client.ts().create(
-        2,
-        labels={"Test": "This", "Taste": "That", "team": "sf"}
-    )
-    for i in range(100):
-        client.ts().add(1, i, i % 7)
-        client.ts().add(2, i, i % 11)
-
-    res = client.ts().mrange(0, 200, filters=["Test=This"])
-    assert 2 == len(res)
-    assert 100 == len(res[0]["1"][1])
-
-    res = client.ts().mrange(0, 200, filters=["Test=This"], count=10)
-    assert 10 == len(res[0]["1"][1])
-
-    for i in range(100):
-        client.ts().add(1, i + 200, i % 7)
-    res = client.ts().mrange(
-        0,
-        500,
-        filters=["Test=This"],
-        aggregation_type="avg",
-        bucket_size_msec=10
-    )
-    assert 2 == len(res)
-    assert 20 == len(res[0]["1"][1])
-
-    # test withlabels
-    assert {} == res[0]["1"][0]
-    res = client.ts().mrange(0, 200, filters=["Test=This"], with_labels=True)
-    assert {"Test": "This", "team": "ny"} == res[0]["1"][0]
-
-
-@pytest.mark.redismod
-@skip_ifmodversion_lt("99.99.99", "timeseries")
-def test_multi_range_advanced(client):
-    client.ts().create(1, labels={"Test": "This", "team": "ny"})
-    client.ts().create(
-        2,
-        labels={"Test": "This", "Taste": "That", "team": "sf"}
-    )
-    for i in range(100):
-        client.ts().add(1, i, i % 7)
-        client.ts().add(2, i, i % 11)
-
-    # test with selected labels
-    res = client.ts().mrange(
-        0,
-        200,
-        filters=["Test=This"],
-        select_labels=["team"]
-    )
-    assert {"team": "ny"} == res[0]["1"][0]
-    assert {"team": "sf"} == res[1]["2"][0]
-
-    # test with filterby
-    res = client.ts().mrange(
-        0,
-        200,
-        filters=["Test=This"],
-        filter_by_ts=[i for i in range(10, 20)],
-        filter_by_min_value=1,
-        filter_by_max_value=2,
-    )
-    assert [(15, 1.0), (16, 2.0)] == res[0]["1"][1]
-
-    # test groupby
-    res = client.ts().mrange(
-        0,
-        3,
-        filters=["Test=This"],
-        groupby="Test",
-        reduce="sum"
-    )
-    assert [(0, 0.0), (1, 2.0), (2, 4.0), (3, 6.0)] == res[0]["Test=This"][1]
-    res = client.ts().mrange(
-        0,
-        3,
-        filters=["Test=This"],
-        groupby="Test",
-        reduce="max"
-    )
-    assert [(0, 0.0), (1, 1.0), (2, 2.0), (3, 3.0)] == res[0]["Test=This"][1]
-    res = client.ts().mrange(
-        0,
-        3,
-        filters=["Test=This"],
-        groupby="team",
-        reduce="min")
-    assert 2 == len(res)
-    assert [(0, 0.0), (1, 1.0), (2, 2.0), (3, 3.0)] == res[0]["team=ny"][1]
-    assert [(0, 0.0), (1, 1.0), (2, 2.0), (3, 3.0)] == res[1]["team=sf"][1]
-
-    # test align
-    res = client.ts().mrange(
-        0,
-        10,
-        filters=["team=ny"],
-        aggregation_type="count",
-        bucket_size_msec=10,
-        align="-",
-    )
-    assert [(0, 10.0), (10, 1.0)] == res[0]["1"][1]
-    res = client.ts().mrange(
-        0,
-        10,
-        filters=["team=ny"],
-        aggregation_type="count",
-        bucket_size_msec=10,
-        align=5,
-    )
-    assert [(-5, 5.0), (5, 6.0)] == res[0]["1"][1]
-
-
-@pytest.mark.redismod
-@skip_ifmodversion_lt("99.99.99", "timeseries")
-def test_multi_reverse_range(client):
-    client.ts().create(1, labels={"Test": "This", "team": "ny"})
-    client.ts().create(
-        2,
-        labels={"Test": "This", "Taste": "That", "team": "sf"}
-    )
-    for i in range(100):
-        client.ts().add(1, i, i % 7)
-        client.ts().add(2, i, i % 11)
-
-    res = client.ts().mrange(0, 200, filters=["Test=This"])
-    assert 2 == len(res)
-    assert 100 == len(res[0]["1"][1])
-
-    res = client.ts().mrange(0, 200, filters=["Test=This"], count=10)
-    assert 10 == len(res[0]["1"][1])
-
-    for i in range(100):
-        client.ts().add(1, i + 200, i % 7)
-    res = client.ts().mrevrange(
-        0,
-        500,
-        filters=["Test=This"],
-        aggregation_type="avg",
-        bucket_size_msec=10
-    )
-    assert 2 == len(res)
-    assert 20 == len(res[0]["1"][1])
-    assert {} == res[0]["1"][0]
-
-    # test withlabels
-    res = client.ts().mrevrange(
-        0,
-        200,
-        filters=["Test=This"],
-        with_labels=True
-    )
-    assert {"Test": "This", "team": "ny"} == res[0]["1"][0]
-
-    # test with selected labels
-    res = client.ts().mrevrange(
-        0,
-        200,
-        filters=["Test=This"], select_labels=["team"]
-    )
-    assert {"team": "ny"} == res[0]["1"][0]
-    assert {"team": "sf"} == res[1]["2"][0]
-
-    # test filterby
-    res = client.ts().mrevrange(
-        0,
-        200,
-        filters=["Test=This"],
-        filter_by_ts=[i for i in range(10, 20)],
-        filter_by_min_value=1,
-        filter_by_max_value=2,
-    )
-    assert [(16, 2.0), (15, 1.0)] == res[0]["1"][1]
-
-    # test groupby
-    res = client.ts().mrevrange(
-        0, 3, filters=["Test=This"], groupby="Test", reduce="sum"
-    )
-    assert [(3, 6.0), (2, 4.0), (1, 2.0), (0, 0.0)] == res[0]["Test=This"][1]
-    res = client.ts().mrevrange(
-        0, 3, filters=["Test=This"], groupby="Test", reduce="max"
-    )
-    assert [(3, 3.0), (2, 2.0), (1, 1.0), (0, 0.0)] == res[0]["Test=This"][1]
-    res = client.ts().mrevrange(
-        0, 3, filters=["Test=This"], groupby="team", reduce="min"
-    )
-    assert 2 == len(res)
-    assert [(3, 3.0), (2, 2.0), (1, 1.0), (0, 0.0)] == res[0]["team=ny"][1]
-    assert [(3, 3.0), (2, 2.0), (1, 1.0), (0, 0.0)] == res[1]["team=sf"][1]
-
-    # test align
-    res = client.ts().mrevrange(
-        0,
-        10,
-        filters=["team=ny"],
-        aggregation_type="count",
-        bucket_size_msec=10,
-        align="-",
-    )
-    assert [(10, 1.0), (0, 10.0)] == res[0]["1"][1]
-    res = client.ts().mrevrange(
-        0,
-        10,
-        filters=["team=ny"],
-        aggregation_type="count",
-        bucket_size_msec=10,
-        align=1,
-    )
-    assert [(1, 10.0), (-9, 1.0)] == res[0]["1"][1]
-
-
-@pytest.mark.redismod
-def test_get(client):
-    name = "test"
-    client.ts().create(name)
-    assert client.ts().get(name) is None
-    client.ts().add(name, 2, 3)
-    assert 2 == client.ts().get(name)[0]
-    client.ts().add(name, 3, 4)
-    assert 4 == client.ts().get(name)[1]
-
-
-@pytest.mark.redismod
-def test_mget(client):
-    client.ts().create(1, labels={"Test": "This"})
-    client.ts().create(2, labels={"Test": "This", "Taste": "That"})
-    act_res = client.ts().mget(["Test=This"])
-    exp_res = [{"1": [{}, None, None]}, {"2": [{}, None, None]}]
-    assert act_res == exp_res
-    client.ts().add(1, "*", 15)
-    client.ts().add(2, "*", 25)
-    res = client.ts().mget(["Test=This"])
-    assert 15 == res[0]["1"][2]
-    assert 25 == res[1]["2"][2]
-    res = client.ts().mget(["Taste=That"])
-    assert 25 == res[0]["2"][2]
-
-    # test with_labels
-    assert {} == res[0]["2"][0]
-    res = client.ts().mget(["Taste=That"], with_labels=True)
-    assert {"Taste": "That", "Test": "This"} == res[0]["2"][0]
-
-
-@pytest.mark.redismod
-def test_info(client):
-    client.ts().create(
-        1,
-        retention_msecs=5,
-        labels={"currentLabel": "currentData"}
-    )
-    info = client.ts().info(1)
-    assert 5 == info.retention_msecs
-    assert info.labels["currentLabel"] == "currentData"
-
-
-@pytest.mark.redismod
-@skip_ifmodversion_lt("1.4.0", "timeseries")
-def testInfoDuplicatePolicy(client):
-    client.ts().create(
-        1,
-        retention_msecs=5,
-        labels={"currentLabel": "currentData"}
-    )
-    info = client.ts().info(1)
-    assert info.duplicate_policy is None
-
-    client.ts().create("time-serie-2", duplicate_policy="min")
-    info = client.ts().info("time-serie-2")
-    assert "min" == info.duplicate_policy
-
-
-@pytest.mark.redismod
-def test_query_index(client):
-    client.ts().create(1, labels={"Test": "This"})
-    client.ts().create(2, labels={"Test": "This", "Taste": "That"})
-    assert 2 == len(client.ts().queryindex(["Test=This"]))
-    assert 1 == len(client.ts().queryindex(["Taste=That"]))
-    assert [2] == client.ts().queryindex(["Taste=That"])
-
-
-@pytest.mark.redismod
-@pytest.mark.pipeline
-def test_pipeline(client):
-    pipeline = client.ts().pipeline()
-    pipeline.create("with_pipeline")
-    for i in range(100):
-        pipeline.add("with_pipeline", i, 1.1 * i)
-    pipeline.execute()
-
-    info = client.ts().info("with_pipeline")
-    assert info.lastTimeStamp == 99
-    assert info.total_samples == 100
-    assert client.ts().get("with_pipeline")[1] == 99 * 1.1
-
-
-@pytest.mark.redismod
-def test_uncompressed(client):
-    client.ts().create("compressed")
-    client.ts().create("uncompressed", uncompressed=True)
-    compressed_info = client.ts().info("compressed")
-    uncompressed_info = client.ts().info("uncompressed")
-    assert compressed_info.memory_usage != uncompressed_info.memory_usage
-- 
2.34.0

